{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f1dc6b-78d1-47a7-8bdf-835231ba7ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/04/06 10:29:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4040\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743892185327</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743892185426</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-c0092dfc0d0840c28e91e715c5a8606c</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-4f30e59608130454</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 0: Start Spark Session  \n",
    "# CELL 0: \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Start Spark session using shared notebook 00_ghcn2_setup.ipynb \n",
    "# - Use lightweight config: 2 executors, 1 core each, 1GB memory\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "%run ./00_ghcn2_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "\n",
    "start_spark(\n",
    "    executor_instances=2,\n",
    "    executor_cores=1,\n",
    "    worker_memory=1,\n",
    "    master_memory=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d72776",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578aa0",
   "metadata": {},
   "source": [
    "**Q1** First you will investigate the `daily`, `stations`, `states`, `countries`, and `inventory` data provided  in cloud storage in:\n",
    " `wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`  \n",
    "using the `hdfs` command.\n",
    "\n",
    "**(a)** How is the data structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a3b475",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql           import functions as F \n",
    "from pyspark.sql.types     import *\n",
    "from pyspark.sql.functions import col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d6483f-a00f-4e25-83b4-4cd4af8ab743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n",
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/\n",
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/\n",
      "DEEBUG = TRUE\n"
     ]
    }
   ],
   "source": [
    "aDaily         = f'/2025.csv.gz'\n",
    "prefix         = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/'\n",
    "prefixWrite    = f'wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/'\n",
    "#prefixWrite    = \"./\"\n",
    "prefixDaily    = f'{prefix}daily/'\n",
    "dprintf(DEEBUG,prefix)\n",
    "dprintf(DEEBUG,prefixWrite)\n",
    "dprintf(DEEBUG,prefixDaily)\n",
    "dprintf(DEEBUG,\"DEEBUG = TRUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed435aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1:  Azure HDFS layout  daily folder \n",
      "--- PART 1 ---\n",
      "drwxrwxrwx   daily\n",
      "-rwxrwxrwx   ghcnd-countries.txt\n",
      "-rwxrwxrwx   ghcnd-inventory.txt\n",
      "-rwxrwxrwx   ghcnd-states.txt\n",
      "-rwxrwxrwx   ghcnd-stations.txt\n",
      "complete in 1.72 seconds\n",
      "--- PART 2 ---\n",
      "Unable                    2025-04-06 10:29:53,836 WARN util.NativeCodeLoader:\n",
      "Loaded                    2025-04-06 10:29:54,096 INFO impl.MetricsConfig:\n",
      "Scheduled                 2025-04-06 10:29:54,142 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-06 10:29:54,142 INFO impl.MetricsSystemImpl:\n",
      "ghcnd-countries.txt       3.6 K   3.6 K  \n",
      "ghcnd-states.txt          1.1 K   1.1 K  \n",
      "ghcnd-stations.txt        10.6 M  10.6 M \n",
      "daily                     13.0 G  13.0 G \n",
      "ghcnd-inventory.txt       33.6 M  33.6 M \n",
      "Stopping                  2025-04-06 10:29:54,813 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-06 10:29:54,813 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-06 10:29:54,813 INFO impl.MetricsSystemImpl:\n",
      "complete in 1.88 seconds\n",
      "--- PART 3 ---\n",
      "2025-04-06 10:29:55,894 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-04-06 10:29:56,154 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-04-06 10:29:56,199 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-04-06 10:29:56,199 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "           1          264        13975887693 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily\n",
      "2025-04-06 10:29:56,776 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n",
      "2025-04-06 10:29:56,776 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n",
      "2025-04-06 10:29:56,776 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n",
      "complete in 2.06 seconds\n",
      "--- PART 4 ---\n",
      "1750.csv.gz     1385743\n",
      "1763.csv.gz     3358\n",
      "1764.csv.gz     3327\n",
      "1765.csv.gz     3335\n",
      "1766.csv.gz     3344\n",
      "1767.csv.gz     3356\n",
      "1768.csv.gz     3325\n",
      "1769.csv.gz     3418\n",
      "1770.csv.gz     3357\n",
      "1771.csv.gz     3373\n",
      "1772.csv.gz     3419\n",
      "1773.csv.gz     3368\n",
      "1774.csv.gz     3393\n",
      "1775.csv.gz     6365\n",
      "1776.csv.gz     6425\n",
      "1777.csv.gz     6424\n",
      "1778.csv.gz     6240\n",
      "1779.csv.gz     6144\n",
      "1780.csv.gz     6245\n",
      "1781.csv.gz     7799\n",
      "1782.csv.gz     7808\n",
      "1783.csv.gz     7920\n",
      "1784.csv.gz     7946\n",
      "1785.csv.gz     7804\n",
      "1786.csv.gz     7891\n",
      "1787.csv.gz     6336\n",
      "1788.csv.gz     6394\n",
      "1789.csv.gz     7773\n",
      "1790.csv.gz     7786\n",
      "1791.csv.gz     7734\n",
      "1792.csv.gz     7806\n",
      "1793.csv.gz     6342\n",
      "1794.csv.gz     7770\n",
      "1795.csv.gz     7843\n",
      "1796.csv.gz     7821\n",
      "1797.csv.gz     9239\n",
      "1798.csv.gz     9290\n",
      "1799.csv.gz     6351\n",
      "1800.csv.gz     7783\n",
      "1801.csv.gz     7801\n",
      "1802.csv.gz     9195\n",
      "1803.csv.gz     7892\n",
      "1804.csv.gz     8727\n",
      "1805.csv.gz     8901\n",
      "1806.csv.gz     8598\n",
      "1807.csv.gz     8725\n",
      "1808.csv.gz     8928\n",
      "1809.csv.gz     8778\n",
      "1810.csv.gz     8829\n",
      "1811.csv.gz     8932\n",
      "1812.csv.gz     8993\n",
      "1813.csv.gz     9264\n",
      "1814.csv.gz     10833\n",
      "1815.csv.gz     13884\n",
      "1816.csv.gz     13865\n",
      "1817.csv.gz     13758\n",
      "1818.csv.gz     13758\n",
      "1819.csv.gz     13690\n",
      "1820.csv.gz     13997\n",
      "1821.csv.gz     13792\n",
      "1822.csv.gz     14049\n",
      "1823.csv.gz     14745\n",
      "1824.csv.gz     18035\n",
      "1825.csv.gz     18101\n",
      "1826.csv.gz     18403\n",
      "1827.csv.gz     20855\n",
      "1828.csv.gz     20981\n",
      "1829.csv.gz     21141\n",
      "1830.csv.gz     21306\n",
      "1831.csv.gz     21319\n",
      "1832.csv.gz     22435\n",
      "1833.csv.gz     27262\n",
      "1834.csv.gz     27164\n",
      "1835.csv.gz     27546\n",
      "1836.csv.gz     29795\n",
      "1837.csv.gz     29334\n",
      "1838.csv.gz     31459\n",
      "1839.csv.gz     29361\n",
      "1840.csv.gz     35913\n",
      "1841.csv.gz     36864\n",
      "1842.csv.gz     39570\n",
      "1843.csv.gz     39535\n",
      "1844.csv.gz     43325\n",
      "1845.csv.gz     52256\n",
      "1846.csv.gz     49535\n",
      "1847.csv.gz     50847\n",
      "1848.csv.gz     50033\n",
      "1849.csv.gz     51977\n",
      "1850.csv.gz     51871\n",
      "1851.csv.gz     59192\n",
      "1852.csv.gz     63805\n",
      "1853.csv.gz     64356\n",
      "1854.csv.gz     64004\n",
      "1855.csv.gz     70710\n",
      "1856.csv.gz     81212\n",
      "1857.csv.gz     88910\n",
      "1858.csv.gz     122842\n",
      "1859.csv.gz     136813\n",
      "1860.csv.gz     146995\n",
      "1861.csv.gz     150606\n",
      "1862.csv.gz     145625\n",
      "1863.csv.gz     164239\n",
      "1864.csv.gz     164146\n",
      "1865.csv.gz     163927\n",
      "1866.csv.gz     212821\n",
      "1867.csv.gz     256701\n",
      "1868.csv.gz     269545\n",
      "1869.csv.gz     314469\n",
      "1870.csv.gz     360140\n",
      "1871.csv.gz     477098\n",
      "1872.csv.gz     650129\n",
      "1873.csv.gz     728688\n",
      "1874.csv.gz     815760\n",
      "1875.csv.gz     886014\n",
      "1876.csv.gz     965160\n",
      "1877.csv.gz     1077970\n",
      "1878.csv.gz     1274702\n",
      "1879.csv.gz     1461379\n",
      "1880.csv.gz     1847043\n",
      "1881.csv.gz     2208841\n",
      "1882.csv.gz     2512758\n",
      "1883.csv.gz     2775527\n",
      "1884.csv.gz     3303464\n",
      "1885.csv.gz     3745027\n",
      "1886.csv.gz     4102788\n",
      "1887.csv.gz     4605113\n",
      "1888.csv.gz     4958378\n",
      "1889.csv.gz     5388271\n",
      "1890.csv.gz     5878742\n",
      "1891.csv.gz     6170065\n",
      "1892.csv.gz     7118004\n",
      "1893.csv.gz     13052983\n",
      "1894.csv.gz     13929624\n",
      "1895.csv.gz     15036340\n",
      "1896.csv.gz     16204018\n",
      "1897.csv.gz     17474772\n",
      "1898.csv.gz     18225580\n",
      "1899.csv.gz     18953308\n",
      "1900.csv.gz     20251196\n",
      "1901.csv.gz     25767142\n",
      "1902.csv.gz     26736821\n",
      "1903.csv.gz     27302649\n",
      "1904.csv.gz     28201320\n",
      "1905.csv.gz     29632148\n",
      "1906.csv.gz     30289765\n",
      "1907.csv.gz     31144951\n",
      "1908.csv.gz     31867868\n",
      "1909.csv.gz     33519209\n",
      "1910.csv.gz     34622865\n",
      "1911.csv.gz     35904462\n",
      "1912.csv.gz     37263226\n",
      "1913.csv.gz     38255770\n",
      "1914.csv.gz     39485528\n",
      "1915.csv.gz     40658560\n",
      "1916.csv.gz     42029487\n",
      "1917.csv.gz     42252301\n",
      "1918.csv.gz     41304535\n",
      "1919.csv.gz     40799997\n",
      "1920.csv.gz     41033429\n",
      "1921.csv.gz     41324111\n",
      "1922.csv.gz     42113624\n",
      "1923.csv.gz     42862524\n",
      "1924.csv.gz     43710214\n",
      "1925.csv.gz     44015966\n",
      "1926.csv.gz     45226774\n",
      "1927.csv.gz     46033600\n",
      "1928.csv.gz     46580018\n",
      "1929.csv.gz     47573559\n",
      "1930.csv.gz     48987101\n",
      "1931.csv.gz     50624633\n",
      "1932.csv.gz     51705273\n",
      "1933.csv.gz     52178613\n",
      "1934.csv.gz     52521149\n",
      "1935.csv.gz     53488183\n",
      "1936.csv.gz     56768695\n",
      "1937.csv.gz     58175289\n",
      "1938.csv.gz     59344873\n",
      "1939.csv.gz     61148235\n",
      "1940.csv.gz     63463421\n",
      "1941.csv.gz     65375485\n",
      "1942.csv.gz     67537465\n",
      "1943.csv.gz     68405592\n",
      "1944.csv.gz     70192973\n",
      "1945.csv.gz     72659632\n",
      "1946.csv.gz     73148444\n",
      "1947.csv.gz     74970819\n",
      "1948.csv.gz     89145605\n",
      "1949.csv.gz     101758958\n",
      "1950.csv.gz     104856670\n",
      "1951.csv.gz     108201081\n",
      "1952.csv.gz     109667528\n",
      "1953.csv.gz     111171866\n",
      "1954.csv.gz     113330494\n",
      "1955.csv.gz     115757071\n",
      "1956.csv.gz     117984088\n",
      "1957.csv.gz     120673205\n",
      "1958.csv.gz     121914900\n",
      "1959.csv.gz     124376950\n",
      "1960.csv.gz     126849252\n",
      "1961.csv.gz     130750539\n",
      "1962.csv.gz     133559230\n",
      "1963.csv.gz     136585486\n",
      "1964.csv.gz     137581099\n",
      "1965.csv.gz     142018261\n",
      "1966.csv.gz     143937982\n",
      "1967.csv.gz     145306876\n",
      "1968.csv.gz     144896387\n",
      "1969.csv.gz     146762160\n",
      "1970.csv.gz     147692050\n",
      "1971.csv.gz     142221845\n",
      "1972.csv.gz     141305619\n",
      "1973.csv.gz     148100923\n",
      "1974.csv.gz     149378829\n",
      "1975.csv.gz     148905604\n",
      "1976.csv.gz     148764292\n",
      "1977.csv.gz     148575501\n",
      "1978.csv.gz     148815187\n",
      "1979.csv.gz     149117704\n",
      "1980.csv.gz     149579207\n",
      "1981.csv.gz     152656825\n",
      "1982.csv.gz     154508476\n",
      "1983.csv.gz     155940158\n",
      "1984.csv.gz     154313147\n",
      "1985.csv.gz     152811879\n",
      "1986.csv.gz     151770851\n",
      "1987.csv.gz     151809884\n",
      "1988.csv.gz     152626966\n",
      "1989.csv.gz     153053088\n",
      "1990.csv.gz     153157679\n",
      "1991.csv.gz     153852159\n",
      "1992.csv.gz     154026142\n",
      "1993.csv.gz     152865318\n",
      "1994.csv.gz     151844762\n",
      "1995.csv.gz     151469358\n",
      "1996.csv.gz     151756945\n",
      "1997.csv.gz     150454010\n",
      "1998.csv.gz     153494337\n",
      "1999.csv.gz     156271670\n",
      "2000.csv.gz     158226488\n",
      "2001.csv.gz     160670036\n",
      "2002.csv.gz     162275809\n",
      "2003.csv.gz     165928854\n",
      "2004.csv.gz     168459055\n",
      "2005.csv.gz     165289423\n",
      "2006.csv.gz     171894892\n",
      "2007.csv.gz     174934788\n",
      "2008.csv.gz     182624806\n",
      "2009.csv.gz     185641000\n",
      "2010.csv.gz     187293865\n",
      "2011.csv.gz     177949864\n",
      "2012.csv.gz     175262367\n",
      "2013.csv.gz     170236479\n",
      "2014.csv.gz     168607573\n",
      "2015.csv.gz     171192339\n",
      "2016.csv.gz     172621542\n",
      "2017.csv.gz     172257484\n",
      "2018.csv.gz     172359764\n",
      "2019.csv.gz     171131311\n",
      "2020.csv.gz     172132828\n",
      "2021.csv.gz     175050475\n",
      "2022.csv.gz     175205910\n",
      "2023.csv.gz     174726829\n",
      "2024.csv.gz     168485088\n",
      "2025.csv.gz     17061071\n",
      "complete in 1.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# CELL 15 Q1: - a last minute attempt to accept James challenge\n",
    "# CELL 15 Q1: – Inspect Azure HDFS layout and daily folder contents\n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - PART 1: list contents under {prefix}\n",
    "# - PART 2: human-readable size info under {prefix}\n",
    "# - PART 3: HDFS block and file count for prefixDaily\n",
    "# - PART 4: list and format daily file entries\n",
    "\n",
    "#  A last minute attempt to do what James had challenged the class with.\n",
    "\n",
    "dprintf(DEEBUG,\"Q1:  Azure HDFS layout  daily folder \")\n",
    "\n",
    "# --- PART 1: directory listing ---\n",
    "dprintf(DEEBUG, \"--- PART 1 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines_ls = !hdfs dfs -ls {prefix}\n",
    "parsed_ls = []\n",
    "\n",
    "for line in lines_ls:\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(\"INFO\") or line.startswith(\"WARN\") or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 2:\n",
    "        perms     = parts[0]\n",
    "        full_path = parts[-1]\n",
    "\n",
    "        if perms.startswith(\"-\") or perms.startswith(\"d\"):\n",
    "            rel_path = full_path.replace(prefix, '')\n",
    "            parsed_ls.append((perms, rel_path))\n",
    "\n",
    "for perms, name in parsed_ls:\n",
    "    dprintf(DEEBUG, f\"{perms:<12} {name}\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "# --- PART 2: du -h (size info) ---\n",
    "dprintf(DEEBUG, \"--- PART 2 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines = !hdfs dfs -du -h {prefix}\n",
    "parsed_du = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith(\"INFO\") or line.startswith(\"WARN\"):\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 5:\n",
    "        size1    = f\"{parts[0]} {parts[1]}\"\n",
    "        size2    = f\"{parts[2]} {parts[3]}\"\n",
    "        full_path = parts[4]\n",
    "    elif len(parts) >= 3:\n",
    "        size1, size2, full_path = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    rel_path = full_path.replace(prefix, '')\n",
    "    parsed_du.append((rel_path, size1, size2))\n",
    "\n",
    "for name, size1, size2 in parsed_du:\n",
    "    dprintf(DEEBUG, f\"{name:<25} {size1:<7} {size2:<7}\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 3: HDFS file count ---\n",
    "dprintf(DEEBUG, \"--- PART 3 ---\")\n",
    "start = time.time()\n",
    "\n",
    "!hdfs dfs -count {prefixDaily}\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 4: list daily files ---\n",
    "dprintf(DEEBUG, \"--- PART 4 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines_daily = !hdfs dfs -ls {prefixDaily}\n",
    "parsed_daily = []\n",
    "\n",
    "for line in lines_daily:\n",
    "    line = line.strip()\n",
    "    if not line or \"INFO\" in line or \"WARN\" in line or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) == 6:\n",
    "        size      = parts[2]\n",
    "        full_path = parts[5]\n",
    "        file_name = full_path.rsplit('/', 1)[-1]\n",
    "        parsed_daily.append((size, file_name))\n",
    "    else:\n",
    "        dprintf(DEEBUG, f\"(wrong format): {line}\")\n",
    "\n",
    "if parsed_daily:\n",
    "    for size, name in parsed_daily:\n",
    "        dprintf(DEEBUG, f\"{name:<15} {size}\")\n",
    "else:\n",
    "    dprintf(DEEBUG, \"none found.\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21795049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 Q1(A): Define Schema for Daily  \n",
    "# CELL 2 Q1(A): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Schema for daily based on GHCN Daily README\n",
    "# - Define the schema using PySpark types for the daily data format\n",
    "\n",
    "# The daily dataset includes\n",
    "# - ID (String)\n",
    "# - DATE (String)\n",
    "# - ELEMENT (String)\n",
    "# - VALUE (Double)\n",
    "# - MFLAG (String)\n",
    "# - QFLAG (String)\n",
    "# - SFLAG (String)\n",
    "# - OBS_TIME (String)\n",
    "# using pyspark.sql.types.StructType\n",
    "# \n",
    "# The DATE field is formatted as YYYYMMDD as loaded \n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schemaDaily = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"ELEMENT\", StringType(), True),\n",
    "    StructField(\"VALUE\", DoubleType(), True),\n",
    "    StructField(\"MFLAG\", StringType(), True),\n",
    "    StructField(\"QFLAG\", StringType(), True),\n",
    "    StructField(\"SFLAG\", StringType(), True),\n",
    "    StructField(\"OBS_TIME\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9aeed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2025.csv.gz\n",
      "schema: struct<ID:string,DATE:string,ELEMENT:string,VALUE:double,MFLAG:string,QFLAG:string,SFLAG:string,OBS_TIME:string>\n",
      "hprintf start\n",
      "hprintf: show top 5 rows\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: double (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBS_TIME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MFLAG</th>\n",
       "      <th>QFLAG</th>\n",
       "      <th>SFLAG</th>\n",
       "      <th>OBS_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA00616I001</td>\n",
       "      <td>20250205</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US1MOBY0025</td>\n",
       "      <td>20250115</td>\n",
       "      <td>WESF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USW00014758</td>\n",
       "      <td>20250121</td>\n",
       "      <td>AWND</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>W</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US1MANF0085</td>\n",
       "      <td>20250115</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA1ON000717</td>\n",
       "      <td>20250121</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>86.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "      <td>0700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      DATE ELEMENT  VALUE MFLAG QFLAG SFLAG OBS_TIME\n",
       "0  CA00616I001  20250205    PRCP    0.0  None  None     S     None\n",
       "1  US1MOBY0025  20250115    WESF    0.0  None  None     N     0700\n",
       "2  USW00014758  20250121    AWND   30.0  None  None     W     None\n",
       "3  US1MANF0085  20250115    PRCP    0.0  None  None     N     0800\n",
       "4  CA1ON000717  20250121    SNWD   86.0  None  None     N     0700"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprintf end\n",
      "complete in 1.93 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result.count() = 3596588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# CELL 3 Q2(B): Load Single Year of Daily  \n",
    "# CELL 3 Q2(B): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Load this year's daily observations using schemaDaily\n",
    "\n",
    "# The source data is stored as CSV with no header and read using the schema defined in CELL 2.\n",
    "# We load only one file (2025.csv.gz) to keep execution fast.\n",
    "#  \n",
    "\n",
    "# 2023.csv.gz     174726829\n",
    "# 2024.csv.gz     168485088\n",
    "# 2025.csv.gz     17061071 \n",
    "# drwxrwxrwx   daily\n",
    "# -rwxrwxrwx   ghcnd-countries.txt\n",
    "# -rwxrwxrwx   ghcnd-inventory.txt\n",
    "# -rwxrwxrwx   ghcnd-states.txt\n",
    "# -rwxrwxrwx   ghcnd-stations.txt\n",
    "\n",
    "filePath          = f\"{prefixDaily}2025.csv.gz\"\n",
    "filePathCountries = f\"{prefixDaily}ghcnd-countries.txt\"\n",
    "filePathInventory = f\"{prefixDaily}ghcnd-inventory.txt\"\n",
    "filePathStates    = f\"{prefixDaily}ghcnd-states.txt\"\n",
    "filePathStations  = f\"{prefixDaily}ghcnd-stations.txtt\"\n",
    "try:\n",
    "    dfDailyYear = dReadCSV(\n",
    "        b = 1,\n",
    "        path = filePath,\n",
    "        bHeader = False,\n",
    "        schema = schemaDaily\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Error caught:\", type(e), e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afe2be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt\n",
      "bool = 1, read file from: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt\n",
      "hprintf start\n",
      "hprintf: show top 5 rows\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604  17.1167  -61.7833   10.1    ST JO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647  17.1333  -61.7833   19.2    ST JO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196  25.3330   55.5170   34.0    SHARJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194  25.2550   55.3640   10.4    DUBAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217  24.4330   54.6510   26.8    ABU D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               value\n",
       "0  ACW00011604  17.1167  -61.7833   10.1    ST JO...\n",
       "1  ACW00011647  17.1333  -61.7833   19.2    ST JO...\n",
       "2  AE000041196  25.3330   55.5170   34.0    SHARJ...\n",
       "3  AEM00041194  25.2550   55.3640   10.4    DUBAI...\n",
       "4  AEM00041217  24.4330   54.6510   26.8    ABU D..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprintf end\n",
      "completed in 0.05 seconds\n",
      "result.count() = 129657\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/inventory.txt.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m\n\u001b[1;32m     34\u001b[0m stationsdf \u001b[38;5;241m=\u001b[39m stationsRawdf\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     35\u001b[0m     F\u001b[38;5;241m.\u001b[39msubstring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     36\u001b[0m     F\u001b[38;5;241m.\u001b[39msubstring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLATITUDE\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     F\u001b[38;5;241m.\u001b[39msubstring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m81\u001b[39m, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWMOID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# inventory\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m inventoryRawdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/inventory.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m inventorydf \u001b[38;5;241m=\u001b[39m inventoryRawdf\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     55\u001b[0m     F\u001b[38;5;241m.\u001b[39msubstring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     56\u001b[0m     F\u001b[38;5;241m.\u001b[39msubstring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLATITUDE\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     F\u001b[38;5;241m.\u001b[39msubstring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLASTYEAR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m dprintf(DEEBUG, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minventorydf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:615\u001b[0m, in \u001b[0;36mDataFrameReader.text\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    613\u001b[0m     paths \u001b[38;5;241m=\u001b[39m [paths]\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/inventory.txt."
     ]
    }
   ],
   "source": [
    "# CELL 4 Q2(C): Load Metadata Tables  \n",
    "# CELL 4 Q2(C): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Load and parse metadata tables: stations, inventory, countries, and states\n",
    "# - Each dataset is fixed-width format and will be read as text then parsed with substring\n",
    "\n",
    "#  \n",
    "#  \n",
    "#  \n",
    "\n",
    "# 2023.csv.gz     174726829\n",
    "# 2024.csv.gz     168485088\n",
    "# 2025.csv.gz     17061071 \n",
    "# drwxrwxrwx   daily\n",
    "# -rwxrwxrwx   ghcnd-countries.txt\n",
    "# -rwxrwxrwx   ghcnd-inventory.txt\n",
    "# -rwxrwxrwx   ghcnd-states.txt\n",
    "# -rwxrwxrwx   ghcnd-stations.txt\n",
    "\n",
    "filePath          = f\"{prefixDaily}2025.csv.gz\"\n",
    "filePathCountries = f\"{prefix}ghcnd-countries.txt\"\n",
    "filePathInventory = f\"{prefix}ghcnd-inventory.txt\"\n",
    "filePathStates    = f\"{prefix}ghcnd-states.txt\"\n",
    "filePathStations  = f\"{prefix}ghcnd-stations.txt\"\n",
    "\n",
    "# stations\n",
    "\n",
    "stationsRawdf = dReadTEXT(1,filePathStations)\n",
    "\n",
    "\n",
    "\n",
    "stationsdf = stationsRawdf.select(\n",
    "    F.substring(\"value\", 1, 11).alias(\"ID\"),\n",
    "    F.substring(\"value\", 13, 8).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    F.substring(\"value\", 22, 9).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    F.substring(\"value\", 32, 6).cast(\"double\").alias(\"ELEVATION\"),\n",
    "    F.substring(\"value\", 39, 2).alias(\"STATE\"),\n",
    "    F.substring(\"value\", 42, 30).alias(\"NAME\"),\n",
    "    F.substring(\"value\", 73, 3).alias(\"GSNFLAG\"),\n",
    "    F.substring(\"value\", 77, 3).alias(\"HCNFLAG\"),\n",
    "    F.substring(\"value\", 81, 5).alias(\"WMOID\")\n",
    ")\n",
    "\n",
    "hprintf(1,stationsdf)\n",
    "\n",
    "\n",
    "# inventory\n",
    "\n",
    "inventoryRawdf = spark.read.text(f\"{prefix}/inventory.txt\")\n",
    "\n",
    "\n",
    "inventorydf = inventoryRawdf.select(\n",
    "    F.substring(\"value\", 1, 11).alias(\"ID\"),\n",
    "    F.substring(\"value\", 13, 8).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    F.substring(\"value\", 22, 9).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    F.substring(\"value\", 32, 4).alias(\"ELEMENT\"),\n",
    "    F.substring(\"value\", 37, 4).cast(\"int\").alias(\"FIRSTYEAR\"),\n",
    "    F.substring(\"value\", 42, 4).cast(\"int\").alias(\"LASTYEAR\")\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"inventorydf\")\n",
    "hprintf(DEEBUG, inventorydf.show(5))\n",
    "\n",
    "dprintf(DEEBUG, f\"inventorydf.count() = {inventorydf.count()}\")\n",
    "\n",
    "# countries\n",
    "\n",
    "countriesRawdf = spark.read.text(f\"{prefix}/countries.txt\")\n",
    "\n",
    "dprintf(DEEBUG, \"countriesRawdf\")\n",
    "dprintf(DEEBUG, f\"countriesRawdf.count() = {countriesRawdf.count()}\")\n",
    "hprintf(DEEBUG, countriesRawdf.show(5))\n",
    "\n",
    "countriesdf = countriesRawdf.select(\n",
    "    F.substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "    F.substring(\"value\", 4, 61).alias(\"NAME\")\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"countriesdf\")\n",
    "dprintf(DEEBUG, f\"countriesdf.count() = {countriesdf.count()}\")\n",
    "hprintf(DEEBUG, countriesdf.show(5))\n",
    "\n",
    "\n",
    "# states\n",
    "\n",
    "statesRawdf = spark.read.text(f\"{prefix}/states.txt\")\n",
    "\n",
    "dprintf(DEEBUG, \"statesRawdf\")\n",
    "dprintf(DEEBUG, f\"statesRawdf.count() = {statesRawdf.count()}\")\n",
    "hprintf(DEEBUG, statesRawdf.show(5))\n",
    "\n",
    "statesdf = statesRawdf.select(\n",
    "    F.substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "    F.substring(\"value\", 4, 47).alias(\"NAME\")\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"statesdf\")\n",
    "dprintf(DEEBUG, f\"statesdf.count() = {statesdf.count()}\")\n",
    "hprintf(DEEBUG, statesdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 Q2(D): Row Counts for Metadata Tables  \n",
    "# CELL 5 Q2(D): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Count number of rows in each metadata table\n",
    "# - Print each count for reference\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "dprintf(DEEBUG, f\"stationsdf.count() = {stationsdf.count()}\")\n",
    "dprintf(DEEBUG, f\"inventorydf.count() = {inventorydf.count()}\")\n",
    "dprintf(DEEBUG, f\"countriesdf.count() = {countriesdf.count()}\")\n",
    "dprintf(DEEBUG, f\"statesdf.count() = {statesdf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d88b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 Q3(A): Extract Country Code from Station ID  \n",
    "# CELL 6 Q3(A): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Extract first two characters from station ID to identify country\n",
    "# - Store as COUNTRYCODE column in stationsdf\n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "stationsWithdf = stationsdf.withColumn(\"COUNTRYCODE\", F.substring(\"ID\", 1, 2))\n",
    "\n",
    "dprintf(DEEBUG, \"stationsdf with COUNTRYCODE\")\n",
    "hprintf(DEEBUG, stationsdf.select(\"ID\", \"COUNTRYCODE\").show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ec9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 Q3(F): Build and Save Enriched Stations Table  \n",
    "# CELL 7 Q3(F): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Enrich stationsdf by joining with countriesdf and statesdf\n",
    "# - Output final stationsEnricheddf to Parquet for downstream use\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "pqtEnrichedStations = f\"{prefixWrite}/stations_enriched.parquet\"\n",
    "\n",
    "# Join 1: Add COUNTRYNAME from countriesdf\n",
    "\n",
    "dprintf(DEEBUG, \"stationsdf before join with countriesdf\")\n",
    "hprintf(DEEBUG, stationsdf.select(\"ID\", \"COUNTRYCODE\").show(5))\n",
    "dprintf(DEEBUG, f\"stationsdf.count() = {stationsdf.count()}\")\n",
    "\n",
    "stationsdf = stationsdf.join(\n",
    "    countriesdf,\n",
    "    stationsdf[\"COUNTRYCODE\"] == countriesdf[\"CODE\"],\n",
    "    \"left\"\n",
    ").withColumnRenamed(\"NAME\", \"COUNTRYNAME\")\n",
    "\n",
    "dprintf(DEEBUG, \"stationsdf after join with countriesdf\")\n",
    "hprintf(DEEBUG, stationsdf.select(\"ID\", \"COUNTRYCODE\", \"COUNTRYNAME\").show(5))\n",
    "dprintf(DEEBUG, f\"stationsdf.count() = {stationsdf.count()}\")\n",
    "\n",
    "\n",
    "# Join 2: Add STATENAME from statesdf\n",
    "\n",
    "dprintf(DEEBUG, \"stationsdf before join with statesdf\")\n",
    "hprintf(DEEBUG, stationsdf.select(\"ID\", \"STATE\").show(5))\n",
    "dprintf(DEEBUG, f\"stationsdf.count() = {stationsdf.count()}\")\n",
    "\n",
    "stationsdf = stationsdf.join(\n",
    "    statesdf,\n",
    "    stationsdf[\"STATE\"] == statesdf[\"CODE\"],\n",
    "    \"left\"\n",
    ").withColumnRenamed(\"NAME\", \"STATENAME\")\n",
    "\n",
    "dprintf(DEEBUG, \"stationsdf after join with statesdf\")\n",
    "hprintf(DEEBUG, stationsdf.select(\"ID\", \"STATE\", \"STATENAME\").show(5))\n",
    "dprintf(DEEBUG, f\"stationsdf.count() = {stationsdf.count()}\")\n",
    "\n",
    "\n",
    "stationsEnricheddf = stationsdf\n",
    "\n",
    "dprintf(DEEBUG, f\"stationsEnricheddf.count() = {stationsEnricheddf.count()}\")\n",
    "dprintf(DEEBUG, f\"write to : {pqtEnrichedStations}\")\n",
    "\n",
    "if not 0:    \n",
    "    start = time.time()\n",
    "    stationsEnricheddf.write.mode(\"overwrite\").parquet(pqtEnrichedStations)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"stationsEnricheddf.count() = {stationsEnricheddf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtEnrichedStations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 Q3(B): Extract Country Name from Enriched Metadata  \n",
    "# CELL 8 Q3(B): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - From the enriched meta-data: \n",
    "# - Display station ID, country code, and country name\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "\n",
    "dprintf(1, \"stationsEnricheddf\")\n",
    "hprintf(1, stationsEnricheddf.select(\"ID\", \"COUNTRYCODE\", \"COUNTRYNAME\").show(5))\n",
    "dprintf(1, f\"stationsEnricheddf.count() = {stationsEnricheddf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfd889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 Q3(C): Extract State Name from Enriched Metadata  \n",
    "# CELL 9 Q3(C): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Select and inspect STATE and STATENAME columns from enriched metadata\n",
    "# - Validate correctness of state-level join from earlier enrichment step\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "dprintf(1, \"stationsEnricheddf – state mapping\")\n",
    "hprintf(1, stationsEnricheddf.select(\"ID\", \"STATE\", \"STATENAME\").show(5))\n",
    "dprintf(1, f\"stationsEnricheddf.count() = {stationsEnricheddf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Display Station Count by Country (Exploratory Only)  \n",
    "# CELL 10: \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Display-only: Count number of stations per country\n",
    "# - Included for contextual exploration; not required by Q3\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "summaryByCountrydf = (\n",
    "    stationsEnricheddf\n",
    "    .groupBy(\"COUNTRYNAME\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"summaryByCountrydf\")\n",
    "hprintf(DEEBUG, summaryByCountrydf.show(5))\n",
    "dprintf(DEEBUG, f\"summaryByCountrydf.count() = {summaryByCountrydf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5db111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11 Q3(D): Count Unique Elements per Station  \n",
    "# CELL 11 Q3(D): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Join stationsEnricheddf with inventorydf using ID\n",
    "# - For each station, count number of distinct ELEMENTs observed\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "stationsWithElementsdf = stationsEnricheddf.join(\n",
    "    inventorydf,\n",
    "    on=\"ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "stationElementCountdf = (\n",
    "    stationsWithElementsdf\n",
    "    .groupBy(\"ID\")\n",
    "    .agg(F.countDistinct(\"ELEMENT\").alias(\"NUMELEMENTS\"))\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"stationElementCountdf – number of elements per station\")\n",
    "hprintf(DEEBUG, stationElementCountdf.orderBy(F.desc(\"NUMELEMENTS\")).show(5))\n",
    "dprintf(DEEBUG, f\"stationElementCountdf.count() = {stationElementCountdf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213cc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12 Q3(E): Classify Stations by Core Element Coverage  \n",
    "# CELL 12 Q3(E): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Use inventorydf to determine which core elements are observed by each station\n",
    "# - Classify each station as CORE, PARTIAL, or SPECIALISED\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "coreElements = [\"TMAX\", \"TMIN\", \"PRCP\", \"SNOW\", \"SNWD\"]\n",
    "\n",
    "# For each station, collect set of all elements\n",
    "stationElementsdf = (\n",
    "    inventorydf\n",
    "    .groupBy(\"ID\")\n",
    "    .agg(F.collect_set(\"ELEMENT\").alias(\"ELEMENTS\"))\n",
    ")\n",
    "\n",
    "# Classify based on coverage of core elements\n",
    "classifiedStationsdf = (\n",
    "    stationElementsdf\n",
    "    .withColumn(\n",
    "        \"CORE_COUNT\",\n",
    "        F.size(F.array_intersect(F.array(*[F.lit(e) for e in coreElements]), F.col(\"ELEMENTS\")))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"CLASSIFICATION\",\n",
    "        F.when(F.col(\"CORE_COUNT\") == len(coreElements), \"CORE\")\n",
    "         .when((F.col(\"CORE_COUNT\") > 0) & (F.col(\"CORE_COUNT\") < len(coreElements)), \"PARTIAL\")\n",
    "         .otherwise(\"SPECIALISED\")\n",
    "    )\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"classifiedStationsdf – core classification per station\")\n",
    "hprintf(DEEBUG, classifiedStationsdf.select(\"ID\", \"CORE_COUNT\", \"CLASSIFICATION\").show(5))\n",
    "dprintf(DEEBUG, f\"classifiedStationsdf.count() = {classifiedStationsdf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13 Q3(E): Join Classification Back to Enriched Metadata  \n",
    "# CELL 13 Q3(E): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Join classification labels (CORE / PARTIAL / SPECIALISED) back to enriched metadata\n",
    "# - Resulting dataframe can be exported or visualised\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "stationsClassifieddf = stationsEnricheddf.join(\n",
    "    classifiedStationsdf.select(\"ID\", \"CLASSIFICATION\"),\n",
    "    on=\"ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"stationsClassifieddf – enriched metadata with classification\")\n",
    "hprintf(DEEBUG, stationsClassifieddf.select(\"ID\", \"COUNTRYNAME\", \"CLASSIFICATION\").show(5))\n",
    "dprintf(DEEBUG, f\"stationsClassifieddf.count() = {stationsClassifieddf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: Write Classified Metadata for Analysis  \n",
    "# CELL 14: \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Write only classifiedStationsdf for later analysis steps\n",
    "# - stationsEnricheddf was already written in CELL 7\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "pqtStationsClassified = f\"{prefixWrite}/stations_classified.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"classifiedStationsdf.count() = {classifiedStationsdf.count()}\")\n",
    "dprintf(DEEBUG, f\"write to : {pqtStationsClassified}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    classifiedStationsdf.write.mode(\"overwrite\").parquet(pqtStationsClassified)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"classifiedStationsdf.count() = {classifiedStationsdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtStationsClassified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b5282",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CELL 15 Q1: – Inspect Azure HDFS layout and daily folder contents\n",
    "# CELL 15 Q1: \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - PART 1: list contents under {prefix}\n",
    "# - PART 2: human-readable size info under {prefix}\n",
    "# - PART 3: HDFS block and file count for prefixDaily\n",
    "# - PART 4: list and format daily file entries\n",
    "\n",
    "#  A last minute attempt to do what James had challenged the class with.\n",
    "\n",
    "hprintf(\"Q1:  Azure HDFS layout  daily folder \")\n",
    "\n",
    "# --- PART 1: directory listing ---\n",
    "dprintf(DEEBUG, \"--- PART 1 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines_ls = !hdfs dfs -ls {prefix}\n",
    "parsed_ls = []\n",
    "\n",
    "for line in lines_ls:\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(\"INFO\") or line.startswith(\"WARN\") or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 2:\n",
    "        perms     = parts[0]\n",
    "        full_path = parts[-1]\n",
    "\n",
    "        if perms.startswith(\"-\") or perms.startswith(\"d\"):\n",
    "            rel_path = full_path.replace(prefix, '')\n",
    "            parsed_ls.append((perms, rel_path))\n",
    "\n",
    "for perms, name in parsed_ls:\n",
    "    dprintf(DEEBUG, f\"{perms:<12} {name}\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 2: du -h (size info) ---\n",
    "dprintf(DEEBUG, \"--- PART 2 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines = !hdfs dfs -du -h {prefix}\n",
    "parsed_du = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith(\"INFO\") or line.startswith(\"WARN\"):\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 5:\n",
    "        size1    = f\"{parts[0]} {parts[1]}\"\n",
    "        size2    = f\"{parts[2]} {parts[3]}\"\n",
    "        full_path = parts[4]\n",
    "    elif len(parts) >= 3:\n",
    "        size1, size2, full_path = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    rel_path = full_path.replace(prefix, '')\n",
    "    parsed_du.append((rel_path, size1, size2))\n",
    "\n",
    "for name, size1, size2 in parsed_du:\n",
    "    dprintf(DEEBUG, f\"{name:<25} {size1:<7} {size2:<7}\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 3: HDFS file count ---\n",
    "dprintf(DEEBUG, \"--- PART 3 ---\")\n",
    "start = time.time()\n",
    "\n",
    "!hdfs dfs -count {prefixDaily}\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 4: list daily files ---\n",
    "dprintf(DEEBUG, \"--- PART 4 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines_daily = !hdfs dfs -ls {prefixDaily}\n",
    "parsed_daily = []\n",
    "\n",
    "for line in lines_daily:\n",
    "    line = line.strip()\n",
    "    if not line or \"INFO\" in line or \"WARN\" in line or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) == 6:\n",
    "        size      = parts[2]\n",
    "        full_path = parts[5]\n",
    "        file_name = full_path.rsplit('/', 1)[-1]\n",
    "        parsed_daily.append((size, file_name))\n",
    "    else:\n",
    "        dprintf(DEEBUG, f\"(wrong format): {line}\")\n",
    "\n",
    "if parsed_daily:\n",
    "    for size, name in parsed_daily:\n",
    "        dprintf(DEEBUG, f\"{name:<15} {size}\")\n",
    "else:\n",
    "    dprintf(DEEBUG, \"none found.\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6807c3-951e-4ad6-bbc2-d67c6d34c4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
