{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1dc6b-78d1-47a7-8bdf-835231ba7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 0: Start Spark Session  \n",
    "# CELL 0: \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Start Spark session using shared notebook 00_ghcn2_setup.ipynb \n",
    "# - Use config: 2 executors, 1 core each, 1GB memory\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "%run ./00_ghcn2_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "\n",
    "start_spark(\n",
    "    executor_instances=2,\n",
    "    executor_cores=1,\n",
    "    worker_memory=1,\n",
    "    master_memory=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d72776",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578aa0",
   "metadata": {},
   "source": [
    "**Q1** First you will investigate the `daily`, `stations`, `states`, `countries`, and `inventory` data provided  in cloud storage in:\n",
    " `wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`  \n",
    "using the `hdfs` command.\n",
    "\n",
    "**(a)** How is the data structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql           import functions as F \n",
    "from pyspark.sql.types     import *\n",
    "from pyspark.sql.functions import col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6483f-a00f-4e25-83b4-4cd4af8ab743",
   "metadata": {},
   "outputs": [],
   "source": [
    "aDaily         = f'/2025.csv.gz'\n",
    "prefix         = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/'\n",
    "prefixWrite    = f'wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/'\n",
    "#prefixWrite    = \"./\"\n",
    "prefixDaily    = f'{prefix}daily/'\n",
    "dprintf(DEEBUG,prefix)\n",
    "dprintf(DEEBUG,prefixWrite)\n",
    "dprintf(DEEBUG,prefixDaily)\n",
    "dprintf(DEEBUG,\"DEEBUG = TRUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed435aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL 15 Q1: - a last minute attempt to accept James challenge\n",
    "# CELL 15 Q1: – Inspect Azure HDFS layout and daily folder contents\n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - PART 1: list contents under {prefix}\n",
    "# - PART 2: human-readable size info under {prefix}\n",
    "# - PART 3: HDFS block and file count for prefixDaily\n",
    "# - PART 4: list and format daily file entries\n",
    "\n",
    "#  A last minute attempt to do what James had challenged the class with.\n",
    "\n",
    "dprintf(DEEBUG,\"Q1:  Azure HDFS layout  daily folder \")\n",
    "\n",
    "# --- PART 1: directory listing ---\n",
    "dprintf(DEEBUG, \"--- PART 1 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines_ls = !hdfs dfs -ls {prefix}\n",
    "parsed_ls = []\n",
    "\n",
    "for line in lines_ls:\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(\"INFO\") or line.startswith(\"WARN\") or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 2:\n",
    "        perms     = parts[0]\n",
    "        full_path = parts[-1]\n",
    "\n",
    "        if perms.startswith(\"-\") or perms.startswith(\"d\"):\n",
    "            rel_path = full_path.replace(prefix, '')\n",
    "            parsed_ls.append((perms, rel_path))\n",
    "\n",
    "for perms, name in parsed_ls:\n",
    "    dprintf(DEEBUG, f\"{perms:<12} {name}\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "# --- PART 2: du -h (size info) ---\n",
    "dprintf(DEEBUG, \"--- PART 2 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines = !hdfs dfs -du -h {prefix}\n",
    "parsed_du = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith(\"INFO\") or line.startswith(\"WARN\"):\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 5:\n",
    "        size1    = f\"{parts[0]} {parts[1]}\"\n",
    "        size2    = f\"{parts[2]} {parts[3]}\"\n",
    "        full_path = parts[4]\n",
    "    elif len(parts) >= 3:\n",
    "        size1, size2, full_path = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    rel_path = full_path.replace(prefix, '')\n",
    "    parsed_du.append((rel_path, size1, size2))\n",
    "\n",
    "for name, size1, size2 in parsed_du:\n",
    "    dprintf(DEEBUG, f\"{name:<25} {size1:<7} {size2:<7}\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 3: HDFS file count ---\n",
    "dprintf(DEEBUG, \"--- PART 3 ---\")\n",
    "start = time.time()\n",
    "\n",
    "!hdfs dfs -count {prefixDaily}\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 4: list daily files ---\n",
    "dprintf(DEEBUG, \"--- PART 4 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines_daily = !hdfs dfs -ls {prefixDaily}\n",
    "parsed_daily = []\n",
    "\n",
    "for line in lines_daily:\n",
    "    line = line.strip()\n",
    "    if not line or \"INFO\" in line or \"WARN\" in line or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) == 6:\n",
    "        size      = parts[2]\n",
    "        full_path = parts[5]\n",
    "        file_name = full_path.rsplit('/', 1)[-1]\n",
    "        parsed_daily.append((size, file_name))\n",
    "    else:\n",
    "        dprintf(DEEBUG, f\"(wrong format): {line}\")\n",
    "\n",
    "if parsed_daily:\n",
    "    for size, name in parsed_daily:\n",
    "        dprintf(DEEBUG, f\"{name:<15} {size}\")\n",
    "else:\n",
    "    dprintf(DEEBUG, \"none found.\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21795049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 Q1(A): Define Schema for Daily  \n",
    "# CELL 2 Q1(A): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Schema for daily based on GHCN Daily README\n",
    "# - Define the schema using PySpark types for the daily data format\n",
    "\n",
    "# The daily dataset includes\n",
    "# - ID (String)\n",
    "# - DATE (String)\n",
    "# - ELEMENT (String)\n",
    "# - VALUE (Double)\n",
    "# - MFLAG (String)\n",
    "# - QFLAG (String)\n",
    "# - SFLAG (String)\n",
    "# - OBS_TIME (String)\n",
    "# using pyspark.sql.types.StructType\n",
    "# \n",
    "# The DATE field is formatted as YYYYMMDD as loaded \n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schemaDaily = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"ELEMENT\", StringType(), True),\n",
    "    StructField(\"VALUE\", DoubleType(), True),\n",
    "    StructField(\"MFLAG\", StringType(), True),\n",
    "    StructField(\"QFLAG\", StringType(), True),\n",
    "    StructField(\"SFLAG\", StringType(), True),\n",
    "    StructField(\"OBS_TIME\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 Q2(B): Load Single Year of Daily  \n",
    "# CELL 3 Q2(B): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Load this year's daily observations using schemaDaily\n",
    "\n",
    "# The source data is stored as CSV with no header and read using the schema defined in CELL 2.\n",
    "# We load only one file (2025.csv.gz) to keep execution fast.\n",
    "#  \n",
    "\n",
    "# 2023.csv.gz     174726829\n",
    "# 2024.csv.gz     168485088\n",
    "# 2025.csv.gz     17061071 \n",
    "# drwxrwxrwx   daily\n",
    "# -rwxrwxrwx   ghcnd-countries.txt\n",
    "# -rwxrwxrwx   ghcnd-inventory.txt\n",
    "# -rwxrwxrwx   ghcnd-states.txt\n",
    "# -rwxrwxrwx   ghcnd-stations.txt\n",
    "\n",
    "filePath          = f\"{prefixDaily}2025.csv.gz\"\n",
    "filePathCountries = f\"{prefixDaily}ghcnd-countries.txt\"\n",
    "filePathInventory = f\"{prefixDaily}ghcnd-inventory.txt\"\n",
    "filePathStates    = f\"{prefixDaily}ghcnd-states.txt\"\n",
    "filePathStations  = f\"{prefixDaily}ghcnd-stations.txt\"\n",
    "pqtEnrichedStations = f\"{prefixWrite}enhanced-STATION.parquet\"\n",
    "try:\n",
    "    dfDailyYear = dReadCSV(\n",
    "        b = 1,\n",
    "        path = filePath,\n",
    "        bHeader = False,\n",
    "        schema = schemaDaily\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Error caught:\", type(e), e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db32b82-ca66-49d2-8d8d-2a710540202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 Q2(C): Load Metadata Tables  \n",
    "# CELL 4 Q2(C): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Load and parse metadata tables: stations, inventory, countries, and states\n",
    "# - Each dataset is fixed-width format and will be read as text then parsed with substring\n",
    "\n",
    "#  \n",
    "#  \n",
    "#  \n",
    "\n",
    "# 2023.csv.gz     174726829\n",
    "# 2024.csv.gz     168485088\n",
    "# 2025.csv.gz     17061071 \n",
    "# drwxrwxrwx   daily\n",
    "# -rwxrwxrwx   ghcnd-countries.txt\n",
    "# -rwxrwxrwx   ghcnd-inventory.txt\n",
    "# -rwxrwxrwx   ghcnd-states.txt\n",
    "# -rwxrwxrwx   ghcnd-stations.txt\n",
    "\n",
    "filePath          = f\"{prefixDaily}2025.csv.gz\"\n",
    "filePathCountries = f\"{prefix}ghcnd-countries.txt\"\n",
    "filePathInventory = f\"{prefix}ghcnd-inventory.txt\"\n",
    "filePathStates    = f\"{prefix}ghcnd-states.txt\"\n",
    "filePathStations  = f\"{prefix}ghcnd-stations.txt\"\n",
    "\n",
    "# stations\n",
    "\n",
    "dprintf(1,\"stationsRawdf\")\n",
    "stationsRawdf = dReadTEXT(1,filePathStations)\n",
    "\n",
    "stationsdf = stationsRawdf.select(\n",
    "    F.substring(\"value\", 1, 11).alias(\"ID\"),\n",
    "    F.substring(\"value\", 13, 8).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    F.substring(\"value\", 22, 9).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    F.substring(\"value\", 32, 6).cast(\"double\").alias(\"ELEVATION\"),\n",
    "    F.substring(\"value\", 39, 2).alias(\"STATE\"),\n",
    "    F.substring(\"value\", 42, 30).alias(\"STATIONNAME\"),\n",
    "    F.substring(\"value\", 73, 3).alias(\"GSNFLAG\"),\n",
    "    F.substring(\"value\", 77, 3).alias(\"HCNFLAG\"),\n",
    "    F.substring(\"value\", 81, 5).alias(\"WMOID\")\n",
    ")\n",
    "stationsdf = stationsdf.withColumn(\"COUNTRYCODE\", F.substring(\"ID\", 1, 2)).drop(\"CODE\")\n",
    "\n",
    "\n",
    "dprintf(1,\"stationsdf\")\n",
    "hprintf(1,stationsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c4a0a-97e4-4d2d-9816-c23b91a0bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inventory\n",
    "\n",
    "dprintf(1,\"inventoryRawdf\")\n",
    "inventoryRawdf = dReadTEXT(1, filePathInventory)\n",
    "\n",
    "inventorydf = inventoryRawdf.select(\n",
    "    F.substring(\"value\", 1, 11).alias(\"ID\"),\n",
    "    F.substring(\"value\", 13, 8).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    F.substring(\"value\", 22, 9).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    F.substring(\"value\", 32, 4).alias(\"ELEMENT\"),\n",
    "    F.substring(\"value\", 37, 4).cast(\"int\").alias(\"FIRSTYEAR\"),\n",
    "    F.substring(\"value\", 42, 4).cast(\"int\").alias(\"LASTYEAR\")\n",
    ")\n",
    "\n",
    "dprintf(1,\"inventorydf\")\n",
    "hprintf(1, inventorydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19881125-7bc2-4645-b5a1-21d5957381b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries\n",
    "\n",
    "dprintf(1,\"countriesdf\")\n",
    "countriesRawdf = dReadTEXT(1, filePathCountries)\n",
    "\n",
    "countriesdf = countriesRawdf.select(\n",
    "    F.substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "    F.substring(\"value\", 4, 61).alias(\"NAME\")\n",
    ") \n",
    "dprintf(1,\"countriesdf\")\n",
    "hprintf(1, countriesdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae5eb0-2932-49ed-9d51-704b40f00d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states\n",
    "\n",
    "dprintf(1,\"statesRawdf\")\n",
    "statesRawdf = dReadTEXT(1, filePathStates)\n",
    "\n",
    "statesdf = statesRawdf.select(\n",
    "    F.substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "    F.substring(\"value\", 4, 47).alias(\"NAME\")\n",
    ")\n",
    "\n",
    "dprintf(1,\"statesdf\")\n",
    "hprintf(1, statesdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 Q2(D): Row Counts for Metadata Tables  \n",
    "# CELL 5 Q2(D): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Count number of rows in each metadata table\n",
    "# - Print each count for reference\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "dprintf(DEEBUG, f\"stationsdf.count() = {stationsdf.count()}\")\n",
    "dprintf(DEEBUG, f\"inventorydf.count() = {inventorydf.count()}\")\n",
    "dprintf(DEEBUG, f\"countriesdf.count() = {countriesdf.count()}\")\n",
    "dprintf(DEEBUG, f\"statesdf.count() = {statesdf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d88b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 Q3(A): Extract Country Code from Station ID  \n",
    "# CELL 6 Q3(A): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Extract first two characters from station ID to identify country\n",
    "# - Store as COUNTRYCODE column in stationsdf\n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "dprintf(DEEBUG, \"stationsdf\")\n",
    "hprintf(DEEBUG, stationsdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8a157-0b7c-4bbc-a727-7fe997d4bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join 1: Add COUNTRYNAME from countriesdf\n",
    "\n",
    "dprintf(1, \"stationsdf (before join with countriesdf)\")\n",
    "hprintf(DEEBUG, stationsdf)\n",
    "\n",
    "dprintf(1, \"countriesdf (used for join)\")\n",
    "hprintf(DEEBUG, countriesdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92f61a-8247-4d80-97d7-851c19e1e10d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp1MD  = stationsdf.join(\n",
    "    countriesdf,\n",
    "    stationsdf[\"COUNTRYCODE\"] == countriesdf[\"CODE\"],\n",
    "    \"left\"\n",
    ").drop(\"CODE\").withColumnRenamed(\"NAME\",\"COUNTRYNAME\")\n",
    "\n",
    "dprintf(1, \"temp1MD (after join with countriesdf)\")\n",
    "hprintf(DEEBUG, temp1MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0eb24-7d11-4dab-9c49-a32659fc0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join 2: Add STATENAME from statesdf\n",
    "\n",
    "dprintf(1, \"temp1MD (before join with statesdf)\")\n",
    "hprintf(DEEBUG, temp1MD)\n",
    "\n",
    "dprintf(1, \"statesdf (used for join)\")\n",
    "hprintf(DEEBUG, statesdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd6e51-1886-428d-838c-fb9ffc9f18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dprintf(1, \"temp1MD (before join with statesdf)\")\n",
    "hprintf(DEEBUG, temp1MD)\n",
    "\n",
    "stationMetadata = temp1MD.join(\n",
    "    statesdf,\n",
    "    temp1MD[\"STATE\"] == statesdf[\"CODE\"],\n",
    "    \"left\"\n",
    ").drop(\"CODE\").withColumnRenamed(\"NAME\",\"STATENAME\")\n",
    "\n",
    "dprintf(1, \"stationMetadata (after join with statesdf)\")\n",
    "hprintf(DEEBUG, stationMetadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3e5f2-b2a1-419c-b50d-2e3a591b05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dprintf(1, \"stationMetadata (after join with statesdf)\")\n",
    "hprintf(DEEBUG, stationMetadata)\n",
    "\n",
    "\n",
    "cols = [\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"STATE\", \"STATIONNAME\",\n",
    "        \"GSNFLAG\", \"HCNFLAG\", \"WMOID\", \"COUNTRYCODE\", \"COUNTRYNAME\", \"STATENAME\"]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through and collect counts\n",
    "for colname in cols:\n",
    "    count = countNonEmpty(stationMetadata, colname)\n",
    "    results.append(Row(COLUMN=colname, COUNT=count))\n",
    "\n",
    "# Convert list of Rows to DataFrame\n",
    "summaryCounts = spark.createDataFrame(results)\n",
    "\n",
    "# Show result\n",
    "hprintf(1,summaryCounts,20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dWritePQ(1, pqtEnrichedStations, stationMetadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305202fc-8773-415a-ad58-7ef39431aed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 Q3(B): Extract Country Name from Enriched Metadata  \n",
    "# CELL 8 Q3(B): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - From the enriched meta-data: \n",
    "# - Display station ID, country code, and country name\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "displaydf = stationMetadata.select(\"ID\", \"COUNTRYCODE\", \"COUNTRYNAME\")\\\n",
    "            .dropDuplicates([\"COUNTRYCODE\"])\n",
    "dprintf(1, \"Display of station ID, country code, and country name\")\n",
    "hprintf(1, displaydf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e884f3-960f-433f-a83d-fbbc5b9bf11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 Q3(C): Extract State Name from Enriched Metadata  \n",
    "# CELL 9 Q3(C): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Select and inspect STATE and STATENAME columns from enriched metadata\n",
    "# - Validate correctness of state-level join from earlier enrichment step\n",
    "#  \n",
    "\n",
    "fiftyStates = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "]\n",
    "\n",
    "# Select distinct STATE-level rows\n",
    "usStationsdf = stationMetadata \\\n",
    "    .filter(F.col(\"COUNTRYCODE\") == \"US\") \\\n",
    "    .select(\"ID\", \"STATE\", \"STATENAME\") \\\n",
    "    .dropDuplicates([\"STATE\"])\n",
    "\n",
    "hprintf(1,usStationsdf,100)  # find out if there are more than 50 states coded.\n",
    "    \n",
    "# Split into recognised and unrecognised state codes\n",
    "df_50states = usStationsdf.filter(F.col(\"STATE\").isin(fiftyStates))\n",
    "df_nonStates = usStationsdf.filter(~F.col(\"STATE\").isin(fiftyStates))\n",
    "\n",
    "dprintf(1, \"✅ 50 Recognised U.S. States\")\n",
    "hprintf(1, df_50states,100)\n",
    "\n",
    "dprintf(1, \"⚠️ Extra STATE Codes\")\n",
    "hprintf(1, df_nonStates,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Display Station Count by Country (Exploratory Only)  \n",
    "# CELL 10: \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Display-only: Count number of stations per country\n",
    "# - Included for contextual exploration; not required by Q3\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "summaryByCountrydf = (\n",
    "    stationsEnricheddf\n",
    "    .groupBy(\"COUNTRYNAME\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"summaryByCountrydf\")\n",
    "hprintf(DEEBUG, summaryByCountrydf.show(5))\n",
    "dprintf(DEEBUG, f\"summaryByCountrydf.count() = {summaryByCountrydf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5db111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11 Q3(D): Count Unique Elements per Station  \n",
    "# CELL 11 Q3(D): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Join stationMetadata : inventorydf by ID\n",
    "# - For each station ID, count number of ELEMENTs \n",
    "# - Make human readable\n",
    "#  \n",
    "\n",
    "# Join and count unique ELEMENTs\n",
    "stationElementsdf = stationMetadata.join(\n",
    "    inventorydf,\n",
    "    on=\"ID\",\n",
    "    how=\"inner\"  # keep only rows that exist in both dfs\n",
    ") \\\n",
    ".groupBy(\"ID\", \"STATIONNAME\",\"COUNTRYNAME\", \"STATE\") \\\n",
    ".agg(\n",
    "    F.countDistinct(\"ELEMENT\").alias(\"nElements\")  # repurpose column: count of unique ELEMENTs\n",
    ")\n",
    "\n",
    "dprintf(1, \"✅ Unique ELEMENT counts per station (with COUNTRYNAME and STATE)\")\n",
    "hprintf(1, stationElementsdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213cc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12 Q3(E): Classify Stations by Core Element Coverage  \n",
    "# CELL 12 Q3(E): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Join stationMetadata with inventorydf on ID to retain station names\n",
    "# - For each station ID, collect all distinct ELEMENT values observed\n",
    "# - Intersect with five core elements [PRCP, SNOW, SNWD, TMAX, TMIN]\n",
    "# - Count number of matching core elements → CORE_COUNT\n",
    "# - Classify each station:\n",
    "#     CORE        = all 5 core elements\n",
    "#     PARTIAL     = some but not all core elements\n",
    "#     SPECIALISED = none of the core elements\n",
    "# - Output ID, STATIONNAME, CORE_COUNT, CLASSIFICATION\n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "coreElements = [\"TMAX\", \"TMIN\", \"PRCP\", \"SNOW\", \"SNWD\"]\n",
    "\n",
    "# For each station, get the name of the element \n",
    "stationElementsdf = (\n",
    "    inventorydf\n",
    "    .groupBy(\"ID\")                   # stationID\n",
    "    .agg(F.collect_set(\"ELEMENT\")    # get element names\n",
    "    .alias(\"ELEMENTS\"))              # repurpose \n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"stationElementsdf - Joining stationMetadata with inventorydf to create stationElementsdf\")\n",
    "hprintf(DEEBUG,stationElementsdf)\n",
    "\n",
    "coreIntersectdf = stationElementsdf.withColumn(\n",
    "    \"CORE_MATCH\",\n",
    "    F.array_intersect(\n",
    "        F.array(\n",
    "            F.lit(\"PRCP\"),\n",
    "            F.lit(\"SNOW\"),\n",
    "            F.lit(\"SNWD\"),\n",
    "            F.lit(\"TMAX\"),\n",
    "            F.lit(\"TMIN\")\n",
    "        ),\n",
    "        F.col(\"ELEMENTS\")\n",
    "    )\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"coreIntersectdf – matched core elements per station\")\n",
    "hprintf(DEEBUG,coreIntersectdf)\n",
    "\n",
    "coreCountdf = coreIntersectdf.withColumn(\n",
    "    \"CORE_COUNT\",\n",
    "    F.size(F.col(\"CORE_MATCH\"))\n",
    ")\n",
    "\n",
    "dprintf(1, \"coreCountdf – number of core elements matched\")\n",
    "\n",
    "hprintf(DEEBUG,coreCountdf)\n",
    "\n",
    "clasifydf = coreCountdf.withColumn(\n",
    "    \"CLASSIFICATION\",\n",
    "    F.when(F.col(\"CORE_COUNT\") == 5, \"CORE\")\n",
    "     .when((F.col(\"CORE_COUNT\") > 0) & (F.col(\"CORE_COUNT\") < 5), \"PARTIAL\")\n",
    "     .otherwise(\"SPECIALISED\")\n",
    ")\n",
    "\n",
    "\n",
    "resultdf = clasifydf.join(\n",
    "    stationMetadata.select(\"ID\", \"STATIONNAME\"),\n",
    "    on=\"ID\",\n",
    "    how=\"left\"\n",
    ").select(\"ID\", \"STATIONNAME\", \"CORE_COUNT\", \"CLASSIFICATION\")\n",
    "\n",
    "\n",
    "hprintf(1, clasifydf)\n",
    "\n",
    "dprintf(1, \"resultdf – final output: ID, STATIONNAME, CORE_COUNT, CLASSIFICATION\")\n",
    "hprintf(1, resultdf)\n",
    "\n",
    "# Step 1:  summary\n",
    "groupSummarydf = resultdf.groupBy(\"CLASSIFICATION\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"nStations\") \\\n",
    "    .orderBy(\"CLASSIFICATION\")\n",
    "\n",
    "\n",
    "dprintf(1, \"groupSummarydf – number of stations per CLASSIFICATION category\")\n",
    "hprintf(1, groupSummarydf)\n",
    "\n",
    "# Step 2:  total row\n",
    "totalCount = groupSummarydf.agg(F.sum(\"nStations\")).first()[0]\n",
    "totalRow = spark.createDataFrame([(\"TOTAL\", totalCount)], [\"CLASSIFICATION\", \"nStations\"])\n",
    "\n",
    "# Step 3: Union the total row with the summary\n",
    "summarydf = groupSummarydf.unionByName(totalRow)\n",
    "\n",
    "dprintf(1, \"summarydf\"  )\n",
    "hprintf(DEEBUG,summarydf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13 Q3(E): Join Classification Back to Enriched Metadata  \n",
    "# CELL 13 Q3(E): \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Join classification labels (CORE / PARTIAL / SPECIALISED) back to enriched metadata\n",
    "# - Resulting dataframe can be exported or visualised\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "stationsClassifieddf = stationsEnricheddf.join(\n",
    "    classifiedStationsdf.select(\"ID\", \"CLASSIFICATION\"),\n",
    "    on=\"ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"stationsClassifieddf – enriched metadata with classification\")\n",
    "hprintf(DEEBUG, stationsClassifieddf.select(\"ID\", \"COUNTRYNAME\", \"CLASSIFICATION\").show(5))\n",
    "dprintf(DEEBUG, f\"stationsClassifieddf.count() = {stationsClassifieddf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: Write Classified Metadata for Analysis  \n",
    "# CELL 14: \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Write only classifiedStationsdf for later analysis steps\n",
    "# - stationsEnricheddf was already written in CELL 7\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "\n",
    "pqtStationsClassified = f\"{prefixWrite}/stations_classified.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"classifiedStationsdf.count() = {classifiedStationsdf.count()}\")\n",
    "dprintf(DEEBUG, f\"write to : {pqtStationsClassified}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    classifiedStationsdf.write.mode(\"overwrite\").parquet(pqtStationsClassified)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"classifiedStationsdf.count() = {classifiedStationsdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtStationsClassified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 15 Q1: – Inspect Azure HDFS layout and daily folder contents\n",
    "# CELL 15 Q1: \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - PART 1: list contents under {prefix}\n",
    "# - PART 2: human-readable size info under {prefix}\n",
    "# - PART 3: HDFS block and file count for prefixDaily\n",
    "# - PART 4: list and format daily file entries\n",
    "\n",
    "#  A last minute attempt to do what James had challenged the class with.\n",
    "\n",
    "hprintf(\"Q1:  Azure HDFS layout  daily folder \")\n",
    "\n",
    "# --- PART 1: directory listing ---\n",
    "dprintf(DEEBUG, \"--- PART 1 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines_ls = !hdfs dfs -ls {prefix}\n",
    "parsed_ls = []\n",
    "\n",
    "for line in lines_ls:\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(\"INFO\") or line.startswith(\"WARN\") or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 2:\n",
    "        perms     = parts[0]\n",
    "        full_path = parts[-1]\n",
    "\n",
    "        if perms.startswith(\"-\") or perms.startswith(\"d\"):\n",
    "            rel_path = full_path.replace(prefix, '')\n",
    "            parsed_ls.append((perms, rel_path))\n",
    "\n",
    "for perms, name in parsed_ls:\n",
    "    dprintf(DEEBUG, f\"{perms:<12} {name}\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 2: du -h (size info) ---\n",
    "dprintf(DEEBUG, \"--- PART 2 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines = !hdfs dfs -du -h {prefix}\n",
    "parsed_du = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith(\"INFO\") or line.startswith(\"WARN\"):\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 5:\n",
    "        size1    = f\"{parts[0]} {parts[1]}\"\n",
    "        size2    = f\"{parts[2]} {parts[3]}\"\n",
    "        full_path = parts[4]\n",
    "    elif len(parts) >= 3:\n",
    "        size1, size2, full_path = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    rel_path = full_path.replace(prefix, '')\n",
    "    parsed_du.append((rel_path, size1, size2))\n",
    "\n",
    "for name, size1, size2 in parsed_du:\n",
    "    dprintf(DEEBUG, f\"{name:<25} {size1:<7} {size2:<7}\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 3: HDFS file count ---\n",
    "dprintf(DEEBUG, \"--- PART 3 ---\")\n",
    "start = time.time()\n",
    "\n",
    "!hdfs dfs -count {prefixDaily}\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# --- PART 4: list daily files ---\n",
    "dprintf(DEEBUG, \"--- PART 4 ---\")\n",
    "start = time.time()\n",
    "\n",
    "lines_daily = !hdfs dfs -ls {prefixDaily}\n",
    "parsed_daily = []\n",
    "\n",
    "for line in lines_daily:\n",
    "    line = line.strip()\n",
    "    if not line or \"INFO\" in line or \"WARN\" in line or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) == 6:\n",
    "        size      = parts[2]\n",
    "        full_path = parts[5]\n",
    "        file_name = full_path.rsplit('/', 1)[-1]\n",
    "        parsed_daily.append((size, file_name))\n",
    "    else:\n",
    "        dprintf(DEEBUG, f\"(wrong format): {line}\")\n",
    "\n",
    "if parsed_daily:\n",
    "    for size, name in parsed_daily:\n",
    "        dprintf(DEEBUG, f\"{name:<15} {size}\")\n",
    "else:\n",
    "    dprintf(DEEBUG, \"none found.\")\n",
    "\n",
    "stop = time.time()\n",
    "dprintf(DEEBUG, f\"complete in {stop - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6807c3-951e-4ad6-bbc2-d67c6d34c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef3bfe-d25b-4b40-a0da-e930396c0284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
