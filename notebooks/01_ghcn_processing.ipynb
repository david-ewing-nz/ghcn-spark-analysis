{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba7d9a1e-2ef1-44b6-9898-3307952e5933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4046\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/home/dew59/ghcn-spark-analysis/notebooks/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7079</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743504109676</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-79269295f0f1741e</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743504109810</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-357b57aecaab47d19cbfd1bfac10a00f</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4046\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/home/dew59/ghcn-spark-analysis/notebooks/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7079</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743504109676</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-79269295f0f1741e</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743504109810</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-357b57aecaab47d19cbfd1bfac10a00f</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./00_ghcn_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d72776",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578aa0",
   "metadata": {},
   "source": [
    "**Q1** First you will investigate the `daily`, `stations`, `states`, `countries`, and `inventory` data provided  in cloud storage in:\n",
    " `wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`  \n",
    "using the `hdfs` command.\n",
    "\n",
    "**(a)** How is the data structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8af9cd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4046\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/home/dew59/ghcn-spark-analysis/notebooks/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7079</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743504109676</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-79269295f0f1741e</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743504109810</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-357b57aecaab47d19cbfd1bfac10a00f</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4046\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/home/dew59/ghcn-spark-analysis/notebooks/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7079</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743504109676</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-79269295f0f1741e</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743504109810</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-357b57aecaab47d19cbfd1bfac10a00f</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./00_ghcn_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a3b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51d6483f-a00f-4e25-83b4-4cd4af8ab743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n"
     ]
    }
   ],
   "source": [
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "aDaily         = f'/2025.csv.gz'\n",
    "prefix         = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/'\n",
    "prefixDaily    = f'{prefix}/daily/'\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d988e31f-04e9-4ab2-9e8d-018996389e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PART 1\n",
      "drwxrwxrwx   daily\n",
      "-rwxrwxrwx   ghcnd-countries.txt\n",
      "-rwxrwxrwx   ghcnd-inventory.txt\n",
      "-rwxrwxrwx   ghcnd-states.txt\n",
      "-rwxrwxrwx   ghcnd-stations.txt\n",
      "--- PART 2\n",
      "Unable                    2025-04-02 00:52:57,985 WARN util.NativeCodeLoader:\n",
      "Loaded                    2025-04-02 00:52:58,263 INFO impl.MetricsConfig:\n",
      "Scheduled                 2025-04-02 00:52:58,311 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-02 00:52:58,311 INFO impl.MetricsSystemImpl:\n",
      "ghcnd-countries.txt       3.6 K   3.6 K  \n",
      "ghcnd-states.txt          1.1 K   1.1 K  \n",
      "ghcnd-stations.txt        10.6 M  10.6 M \n",
      "daily                     13.0 G  13.0 G \n",
      "ghcnd-inventory.txt       33.6 M  33.6 M \n",
      "Stopping                  2025-04-02 00:52:58,994 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-02 00:52:58,994 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-02 00:52:58,994 INFO impl.MetricsSystemImpl:\n",
      "--- PART 3\n",
      "2025-04-02 00:53:00,088 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-04-02 00:53:00,359 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-04-02 00:53:00,411 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-04-02 00:53:00,411 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "           1          264        13975887693 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily\n",
      "2025-04-02 00:53:01,025 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n",
      "2025-04-02 00:53:01,025 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n",
      "2025-04-02 00:53:01,025 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n",
      "--- PART 4\n",
      "1750.csv.gz     1385743\n",
      "1763.csv.gz     3358\n",
      "1764.csv.gz     3327\n",
      "1765.csv.gz     3335\n",
      "1766.csv.gz     3344\n",
      "1767.csv.gz     3356\n",
      "1768.csv.gz     3325\n",
      "1769.csv.gz     3418\n",
      "1770.csv.gz     3357\n",
      "1771.csv.gz     3373\n",
      "1772.csv.gz     3419\n",
      "1773.csv.gz     3368\n",
      "1774.csv.gz     3393\n",
      "1775.csv.gz     6365\n",
      "1776.csv.gz     6425\n",
      "1777.csv.gz     6424\n",
      "1778.csv.gz     6240\n",
      "1779.csv.gz     6144\n",
      "1780.csv.gz     6245\n",
      "1781.csv.gz     7799\n",
      "1782.csv.gz     7808\n",
      "1783.csv.gz     7920\n",
      "1784.csv.gz     7946\n",
      "1785.csv.gz     7804\n",
      "1786.csv.gz     7891\n",
      "1787.csv.gz     6336\n",
      "1788.csv.gz     6394\n",
      "1789.csv.gz     7773\n",
      "1790.csv.gz     7786\n",
      "1791.csv.gz     7734\n",
      "1792.csv.gz     7806\n",
      "1793.csv.gz     6342\n",
      "1794.csv.gz     7770\n",
      "1795.csv.gz     7843\n",
      "1796.csv.gz     7821\n",
      "1797.csv.gz     9239\n",
      "1798.csv.gz     9290\n",
      "1799.csv.gz     6351\n",
      "1800.csv.gz     7783\n",
      "1801.csv.gz     7801\n",
      "1802.csv.gz     9195\n",
      "1803.csv.gz     7892\n",
      "1804.csv.gz     8727\n",
      "1805.csv.gz     8901\n",
      "1806.csv.gz     8598\n",
      "1807.csv.gz     8725\n",
      "1808.csv.gz     8928\n",
      "1809.csv.gz     8778\n",
      "1810.csv.gz     8829\n",
      "1811.csv.gz     8932\n",
      "1812.csv.gz     8993\n",
      "1813.csv.gz     9264\n",
      "1814.csv.gz     10833\n",
      "1815.csv.gz     13884\n",
      "1816.csv.gz     13865\n",
      "1817.csv.gz     13758\n",
      "1818.csv.gz     13758\n",
      "1819.csv.gz     13690\n",
      "1820.csv.gz     13997\n",
      "1821.csv.gz     13792\n",
      "1822.csv.gz     14049\n",
      "1823.csv.gz     14745\n",
      "1824.csv.gz     18035\n",
      "1825.csv.gz     18101\n",
      "1826.csv.gz     18403\n",
      "1827.csv.gz     20855\n",
      "1828.csv.gz     20981\n",
      "1829.csv.gz     21141\n",
      "1830.csv.gz     21306\n",
      "1831.csv.gz     21319\n",
      "1832.csv.gz     22435\n",
      "1833.csv.gz     27262\n",
      "1834.csv.gz     27164\n",
      "1835.csv.gz     27546\n",
      "1836.csv.gz     29795\n",
      "1837.csv.gz     29334\n",
      "1838.csv.gz     31459\n",
      "1839.csv.gz     29361\n",
      "1840.csv.gz     35913\n",
      "1841.csv.gz     36864\n",
      "1842.csv.gz     39570\n",
      "1843.csv.gz     39535\n",
      "1844.csv.gz     43325\n",
      "1845.csv.gz     52256\n",
      "1846.csv.gz     49535\n",
      "1847.csv.gz     50847\n",
      "1848.csv.gz     50033\n",
      "1849.csv.gz     51977\n",
      "1850.csv.gz     51871\n",
      "1851.csv.gz     59192\n",
      "1852.csv.gz     63805\n",
      "1853.csv.gz     64356\n",
      "1854.csv.gz     64004\n",
      "1855.csv.gz     70710\n",
      "1856.csv.gz     81212\n",
      "1857.csv.gz     88910\n",
      "1858.csv.gz     122842\n",
      "1859.csv.gz     136813\n",
      "1860.csv.gz     146995\n",
      "1861.csv.gz     150606\n",
      "1862.csv.gz     145625\n",
      "1863.csv.gz     164239\n",
      "1864.csv.gz     164146\n",
      "1865.csv.gz     163927\n",
      "1866.csv.gz     212821\n",
      "1867.csv.gz     256701\n",
      "1868.csv.gz     269545\n",
      "1869.csv.gz     314469\n",
      "1870.csv.gz     360140\n",
      "1871.csv.gz     477098\n",
      "1872.csv.gz     650129\n",
      "1873.csv.gz     728688\n",
      "1874.csv.gz     815760\n",
      "1875.csv.gz     886014\n",
      "1876.csv.gz     965160\n",
      "1877.csv.gz     1077970\n",
      "1878.csv.gz     1274702\n",
      "1879.csv.gz     1461379\n",
      "1880.csv.gz     1847043\n",
      "1881.csv.gz     2208841\n",
      "1882.csv.gz     2512758\n",
      "1883.csv.gz     2775527\n",
      "1884.csv.gz     3303464\n",
      "1885.csv.gz     3745027\n",
      "1886.csv.gz     4102788\n",
      "1887.csv.gz     4605113\n",
      "1888.csv.gz     4958378\n",
      "1889.csv.gz     5388271\n",
      "1890.csv.gz     5878742\n",
      "1891.csv.gz     6170065\n",
      "1892.csv.gz     7118004\n",
      "1893.csv.gz     13052983\n",
      "1894.csv.gz     13929624\n",
      "1895.csv.gz     15036340\n",
      "1896.csv.gz     16204018\n",
      "1897.csv.gz     17474772\n",
      "1898.csv.gz     18225580\n",
      "1899.csv.gz     18953308\n",
      "1900.csv.gz     20251196\n",
      "1901.csv.gz     25767142\n",
      "1902.csv.gz     26736821\n",
      "1903.csv.gz     27302649\n",
      "1904.csv.gz     28201320\n",
      "1905.csv.gz     29632148\n",
      "1906.csv.gz     30289765\n",
      "1907.csv.gz     31144951\n",
      "1908.csv.gz     31867868\n",
      "1909.csv.gz     33519209\n",
      "1910.csv.gz     34622865\n",
      "1911.csv.gz     35904462\n",
      "1912.csv.gz     37263226\n",
      "1913.csv.gz     38255770\n",
      "1914.csv.gz     39485528\n",
      "1915.csv.gz     40658560\n",
      "1916.csv.gz     42029487\n",
      "1917.csv.gz     42252301\n",
      "1918.csv.gz     41304535\n",
      "1919.csv.gz     40799997\n",
      "1920.csv.gz     41033429\n",
      "1921.csv.gz     41324111\n",
      "1922.csv.gz     42113624\n",
      "1923.csv.gz     42862524\n",
      "1924.csv.gz     43710214\n",
      "1925.csv.gz     44015966\n",
      "1926.csv.gz     45226774\n",
      "1927.csv.gz     46033600\n",
      "1928.csv.gz     46580018\n",
      "1929.csv.gz     47573559\n",
      "1930.csv.gz     48987101\n",
      "1931.csv.gz     50624633\n",
      "1932.csv.gz     51705273\n",
      "1933.csv.gz     52178613\n",
      "1934.csv.gz     52521149\n",
      "1935.csv.gz     53488183\n",
      "1936.csv.gz     56768695\n",
      "1937.csv.gz     58175289\n",
      "1938.csv.gz     59344873\n",
      "1939.csv.gz     61148235\n",
      "1940.csv.gz     63463421\n",
      "1941.csv.gz     65375485\n",
      "1942.csv.gz     67537465\n",
      "1943.csv.gz     68405592\n",
      "1944.csv.gz     70192973\n",
      "1945.csv.gz     72659632\n",
      "1946.csv.gz     73148444\n",
      "1947.csv.gz     74970819\n",
      "1948.csv.gz     89145605\n",
      "1949.csv.gz     101758958\n",
      "1950.csv.gz     104856670\n",
      "1951.csv.gz     108201081\n",
      "1952.csv.gz     109667528\n",
      "1953.csv.gz     111171866\n",
      "1954.csv.gz     113330494\n",
      "1955.csv.gz     115757071\n",
      "1956.csv.gz     117984088\n",
      "1957.csv.gz     120673205\n",
      "1958.csv.gz     121914900\n",
      "1959.csv.gz     124376950\n",
      "1960.csv.gz     126849252\n",
      "1961.csv.gz     130750539\n",
      "1962.csv.gz     133559230\n",
      "1963.csv.gz     136585486\n",
      "1964.csv.gz     137581099\n",
      "1965.csv.gz     142018261\n",
      "1966.csv.gz     143937982\n",
      "1967.csv.gz     145306876\n",
      "1968.csv.gz     144896387\n",
      "1969.csv.gz     146762160\n",
      "1970.csv.gz     147692050\n",
      "1971.csv.gz     142221845\n",
      "1972.csv.gz     141305619\n",
      "1973.csv.gz     148100923\n",
      "1974.csv.gz     149378829\n",
      "1975.csv.gz     148905604\n",
      "1976.csv.gz     148764292\n",
      "1977.csv.gz     148575501\n",
      "1978.csv.gz     148815187\n",
      "1979.csv.gz     149117704\n",
      "1980.csv.gz     149579207\n",
      "1981.csv.gz     152656825\n",
      "1982.csv.gz     154508476\n",
      "1983.csv.gz     155940158\n",
      "1984.csv.gz     154313147\n",
      "1985.csv.gz     152811879\n",
      "1986.csv.gz     151770851\n",
      "1987.csv.gz     151809884\n",
      "1988.csv.gz     152626966\n",
      "1989.csv.gz     153053088\n",
      "1990.csv.gz     153157679\n",
      "1991.csv.gz     153852159\n",
      "1992.csv.gz     154026142\n",
      "1993.csv.gz     152865318\n",
      "1994.csv.gz     151844762\n",
      "1995.csv.gz     151469358\n",
      "1996.csv.gz     151756945\n",
      "1997.csv.gz     150454010\n",
      "1998.csv.gz     153494337\n",
      "1999.csv.gz     156271670\n",
      "2000.csv.gz     158226488\n",
      "2001.csv.gz     160670036\n",
      "2002.csv.gz     162275809\n",
      "2003.csv.gz     165928854\n",
      "2004.csv.gz     168459055\n",
      "2005.csv.gz     165289423\n",
      "2006.csv.gz     171894892\n",
      "2007.csv.gz     174934788\n",
      "2008.csv.gz     182624806\n",
      "2009.csv.gz     185641000\n",
      "2010.csv.gz     187293865\n",
      "2011.csv.gz     177949864\n",
      "2012.csv.gz     175262367\n",
      "2013.csv.gz     170236479\n",
      "2014.csv.gz     168607573\n",
      "2015.csv.gz     171192339\n",
      "2016.csv.gz     172621542\n",
      "2017.csv.gz     172257484\n",
      "2018.csv.gz     172359764\n",
      "2019.csv.gz     171131311\n",
      "2020.csv.gz     172132828\n",
      "2021.csv.gz     175050475\n",
      "2022.csv.gz     175205910\n",
      "2023.csv.gz     174726829\n",
      "2024.csv.gz     168485088\n",
      "2025.csv.gz     17061071\n"
     ]
    }
   ],
   "source": [
    "#! hdfs dfs -du -h  {prefix}\n",
    "#! hdfs dfs -ls     {prefix} #structure \n",
    "#! hdfs dfs -ls     {prefixDaily} #structure \n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/daily/\n",
    "\n",
    "print(\"--- PART 1\")\n",
    "\n",
    "lines_ls = !hdfs dfs -ls {prefix}\n",
    "parsed_ls = []\n",
    "\n",
    "for line in lines_ls:\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(\"INFO\") or line.startswith(\"WARN\") or \"Found\" in line: # noise\n",
    "        continue                                                                          # noise \n",
    "\n",
    "    parts = line.split()                   \n",
    "    if len(parts) >= 2:\n",
    "        perms     = parts[0]\n",
    "        full_path = parts[-1]\n",
    "\n",
    "        if perms.startswith(\"-\") or perms.startswith(\"d\"): # file or directory \n",
    "            rel_path = full_path.replace(prefix, '')\n",
    "            parsed_ls.append((perms, rel_path))\n",
    "\n",
    "# Print result\n",
    "for perms, name in parsed_ls:\n",
    "    print(f\"{perms:<12} {name}\")  #columns are aligned\n",
    "\n",
    "print(\"--- PART 2\")\n",
    "# --- PART 2:  -du -h (size info) ---\n",
    "lines     = !hdfs dfs -du -h  {prefix}\n",
    "parsed_du = []\n",
    "for line in lines:\n",
    "    if line.startswith(\"INFO\") or line.startswith(\"WARN\"): # some noise\n",
    "        continue                                           # some noise\n",
    "\n",
    "    parts = line.split()                                  \n",
    "    if len(parts) >= 5:\n",
    "        size1    = f\"{parts[0]} {parts[1]}\"\n",
    "        size2    = f\"{parts[2]} {parts[3]}\"\n",
    "        full_path = parts[4]\n",
    "    elif len(parts) >= 3:\n",
    "        size1, size2, full_path = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    rel_path = full_path.replace(prefix, '')\n",
    "    parsed_du.append((rel_path, size1, size2))\n",
    "\n",
    "# Print parsed_du\n",
    "for name, size1, size2 in parsed_du:\n",
    "    print(f\"{name:<25} {size1:<7} {size2:<7}\")\n",
    "\n",
    "\n",
    "print(\"--- PART 3\")\n",
    "\n",
    "!hdfs dfs -count  {prefixDaily}\n",
    "\n",
    "print(\"--- PART 4\")\n",
    "\n",
    "lines_daily = !hdfs dfs -ls {prefixDaily}\n",
    "parsed_daily = []\n",
    "\n",
    "for line in lines_daily:\n",
    "    line = line.strip()\n",
    "\n",
    "    \n",
    "    if not line or \"INFO\" in line or \"WARN\" in line or \"Found\" in line: # Skip noise and non-data lines\n",
    "        continue                                                        # Skip noise and non-data lines\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) == 6:  # Exact match \n",
    "        size      = parts[2]\n",
    "        full_path = parts[5]\n",
    "        file_name = full_path.rsplit('/', 1)[-1]\n",
    "        parsed_daily.append((size, file_name))\n",
    "    else:\n",
    "        print(f\"(wrong format): {line}\")\n",
    "\n",
    "# \n",
    "if parsed_daily:\n",
    "    for size, name in parsed_daily:\n",
    "        print(f\"{name:<15} {size}\")\n",
    "else:\n",
    "    print(\"none found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744cf47",
   "metadata": {},
   "source": [
    "### Q2 (a)\n",
    "Schema for `daily` based on  GHCN Daily README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14167028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: integer (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBS_TIME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MFLAG</th>\n",
       "      <th>QFLAG</th>\n",
       "      <th>SFLAG</th>\n",
       "      <th>OBS_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00037091</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00037098</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00037104</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASN00037105</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASN00037106</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      DATE ELEMENT  VALUE MFLAG QFLAG SFLAG OBS_TIME\n",
       "0  ASN00037091  20220101    PRCP      0  None  None     a     None\n",
       "1  ASN00037098  20220101    PRCP      0  None  None     a     None\n",
       "2  ASN00037104  20220101    PRCP      0  None  None     a     None\n",
       "3  ASN00037105  20220101    PRCP      0  None  None     a     None\n",
       "4  ASN00037106  20220101    PRCP      0  None  None     a     None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: integer (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBS_TIME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MFLAG</th>\n",
       "      <th>QFLAG</th>\n",
       "      <th>SFLAG</th>\n",
       "      <th>OBS_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00037091</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00037098</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00037104</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASN00037105</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASN00037106</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID        DATE ELEMENT  VALUE MFLAG QFLAG SFLAG OBS_TIME\n",
       "0  ASN00037091  2022-01-01    PRCP      0  None  None     a     None\n",
       "1  ASN00037098  2022-01-01    PRCP      0  None  None     a     None\n",
       "2  ASN00037104  2022-01-01    PRCP      0  None  None     a     None\n",
       "3  ASN00037105  2022-01-01    PRCP      0  None  None     a     None\n",
       "4  ASN00037106  2022-01-01    PRCP      0  None  None     a     None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "dailyPath = f\"{prefixDaily}2022.csv.gz\"\n",
    "dailySchema = StructType([\n",
    "    StructField(\"ID\",       StringType(),  False), # Station ID\n",
    "    StructField(\"DATE\",     StringType(),  False), # YYYYMMDD\n",
    "    StructField(\"ELEMENT\",  StringType(),  True),  # Measurement type (e.g., TMAX)\n",
    "    StructField(\"VALUE\",    IntegerType(), True),  # Observation value\n",
    "    StructField(\"MFLAG\",    StringType(),  True),  # Measurement flag\n",
    "    StructField(\"QFLAG\",    StringType(),  True),  # Quality flag\n",
    "    StructField(\"SFLAG\",    StringType(),  True),  # Source flag\n",
    "    StructField(\"OBS_TIME\", StringType(),  True)   # Time of observation\n",
    "])\n",
    "\n",
    "\n",
    "dailydf   = spark.read.csv(dailyPath, schema=dailySchema, header=False)\n",
    "dailydf.printSchema()\n",
    "show_as_html(dailydf.limit(5))\n",
    "# Parse DATE \n",
    "dailydf = dailydf.withColumn(\"DATE\", F.to_date(F.col(\"DATE\"), \"yyyyMMdd\"))\n",
    "\n",
    "# Parse OBS_TIME \n",
    "dailydf = dailydf.withColumn(\"OBS_TIME\", \n",
    "    F.to_timestamp(F.concat(F.lit(\"1970-01-01 \"), F.col(\"OBS_TIME\")), \"yyyy-MM-dd HHmm\")) # fake full date\n",
    "dailydf = dailydf.withColumn(\"OBS_TIME\", \n",
    "    F.date_format(F.col(\"OBS_TIME\"), \"HH:mm\")) # just HH:mm\n",
    "\n",
    "# show the sample of data and print the schema\n",
    "dailydf.printSchema()\n",
    "show_as_html(dailydf.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf0bfd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- GSNFLAG: string (nullable = true)\n",
      " |-- HCNFLAG: string (nullable = true)\n",
      " |-- WMOID: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSNFLAG</th>\n",
       "      <th>HCNFLAG</th>\n",
       "      <th>WMOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>19.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td></td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GS</td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.364</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.0</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0  ACW00011604   17.1167    -61.783       10.0         \n",
       "1  ACW00011647   17.1333    -61.783       19.0         \n",
       "2  AE000041196   25.3330     55.517       34.0         \n",
       "3  AEM00041194   25.2550     55.364       10.0         \n",
       "4  AEM00041217   24.4330     54.651       26.0         \n",
       "\n",
       "                             NAME GSNFLAG HCNFLAG  WMOID  \n",
       "0   ST JOHNS COOLIDGE FLD                                 \n",
       "1   ST JOHNS                                              \n",
       "2   SHARJAH INTER. AIRP                GS           4119  \n",
       "3   DUBAI INTL                                      4119  \n",
       "4   ABU DHABI INTL                                  4121  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Example: Load stations\n",
    "stationsPath = f\"{prefix}ghcnd-stations.txt\"\n",
    "stationsdf = spark.read.text(stationsPath)\n",
    "\n",
    "# Parse columns by fixed positions\n",
    "stationsdf = stationsdf.select(\n",
    "    substring(\"value\", 1, 11).alias(\"ID\"),\n",
    "    substring(\"value\", 13, 8).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    substring(\"value\", 22, 8).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    substring(\"value\", 31, 6).cast(\"double\").alias(\"ELEVATION\"),\n",
    "    substring(\"value\", 38, 2).alias(\"STATE\"),\n",
    "    substring(\"value\", 41, 30).alias(\"NAME\"),\n",
    "    substring(\"value\", 72, 3).alias(\"GSNFLAG\"),\n",
    "    substring(\"value\", 76, 3).alias(\"HCNFLAG\"),\n",
    "    substring(\"value\", 80, 5).alias(\"WMOID\")\n",
    ")\n",
    "stationsdf.printSchema()\n",
    "show_as_html(stationsdf.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34036a7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: integer (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBS_TIME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MFLAG</th>\n",
       "      <th>QFLAG</th>\n",
       "      <th>SFLAG</th>\n",
       "      <th>OBS_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00023000</td>\n",
       "      <td>18400101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SWE00139148</td>\n",
       "      <td>18400101</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>-97</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SWE00139148</td>\n",
       "      <td>18400101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EZE00100082</td>\n",
       "      <td>18400101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EZE00100082</td>\n",
       "      <td>18400101</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>-22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      DATE ELEMENT  VALUE MFLAG QFLAG SFLAG OBS_TIME\n",
       "0  ASN00023000  18400101    PRCP      0  None  None     a     None\n",
       "1  SWE00139148  18400101    TMIN    -97  None  None     E     None\n",
       "2  SWE00139148  18400101    PRCP      0  None  None     E     None\n",
       "3  EZE00100082  18400101    TMAX     -4  None  None     E     None\n",
       "4  EZE00100082  18400101    TMIN    -22  None  None     E     None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a small subset of daily (e.g. 2022)\n",
    "dailyPath = f\"{prefixDaily}1840.csv.gz\"\n",
    "dailydf   = spark.read.csv(dailyPath, schema=dailySchema, header=False)\n",
    "dailydf.printSchema()\n",
    "show_as_html(dailydf.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd62cc13-023e-4242-9603-e6878496829a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AC</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AG</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AJ</td>\n",
       "      <td>Azerbaijan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CODE                   NAME\n",
       "0   AC   Antigua and Barbuda \n",
       "1   AE  United Arab Emirates \n",
       "2   AF            Afghanistan\n",
       "3   AG               Algeria \n",
       "4   AJ            Azerbaijan "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Example: Load countries\n",
    "pathCountries = f\"{prefix}ghcnd-countries.txt\"\n",
    "rawCountries  = spark.read.text(pathCountries)\n",
    "\n",
    "# Parse columns by fixed positions\n",
    "countriesdf = rawCountries.select(\n",
    "    substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "    substring(\"value\", 4, 50).alias(\"NAME\")\n",
    ")\n",
    "show_as_html(countriesdf.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb441fa9-ac16-4326-a431-f6414f2eaeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AB</td>\n",
       "      <td>ALBERTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AS</td>\n",
       "      <td>AMERICAN SAMOA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CODE                                             NAME\n",
       "0   AB                                          ALBERTA\n",
       "1   AK                                           ALASKA\n",
       "2   AL  ALABAMA                                        \n",
       "3   AR                                         ARKANSAS\n",
       "4   AS                                   AMERICAN SAMOA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Example: Load states\n",
    "pathStates = f\"{prefix}ghcnd-states.txt\"\n",
    "rawStates  = spark.read.text(pathStates)\n",
    "\n",
    "# Parse columns by fixed positions\n",
    "statesdf = rawStates.select(\n",
    "    substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "    substring(\"value\", 4, 50).alias(\"NAME\")\n",
    ")\n",
    "show_as_html(statesdf.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27b50e6f-d443-4db6-bd74-c3c14b2fef7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>FIRSTYEAR</th>\n",
       "      <th>LASTYEAR</th>\n",
       "      <th>ELEMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  FIRSTYEAR  LASTYEAR ELEMENT\n",
       "0  ACW00011604   17.1167    -61.783        NaN       194     194\n",
       "1  ACW00011604   17.1167    -61.783        NaN       194     194\n",
       "2  ACW00011604   17.1167    -61.783        NaN       194     194\n",
       "3  ACW00011604   17.1167    -61.783        NaN       194     194\n",
       "4  ACW00011604   17.1167    -61.783        NaN       194     194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Example: Load inventory\n",
    "pathInventory = f\"{prefix}ghcnd-inventory.txt\"\n",
    "rawInventory  = spark.read.text(pathInventory)\n",
    "\n",
    "# Parse columns by fixed positions\n",
    "inventorydf = rawInventory.select(\n",
    "    substring(\"value\", 1, 11).alias(\"ID\"),\n",
    "    substring(\"value\", 13, 8).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    substring(\"value\", 22, 8).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    substring(\"value\", 31, 4).cast(\"int\").alias(\"FIRSTYEAR\"),\n",
    "    substring(\"value\", 36, 4).cast(\"int\").alias(\"LASTYEAR\"),\n",
    "    substring(\"value\", 41, 4).alias(\"ELEMENT\")\n",
    ")\n",
    "show_as_html(inventorydf.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf473e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "stations: 129657 rows\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "states: 74 rows\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "countries: 219 rows\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "inventory: 765615 rows\n"
     ]
    }
   ],
   "source": [
    "# Load and count each\n",
    "paths = {\n",
    "    \"stations\":  f\"{prefix}ghcnd-stations.txt\",\n",
    "    \"states\":    f\"{prefix}ghcnd-states.txt\",\n",
    "    \"countries\": f\"{prefix}ghcnd-countries.txt\",\n",
    "    \"inventory\": f\"{prefix}ghcnd-inventory.txt\"\n",
    "}\n",
    "\n",
    "for name, path in paths.items():\n",
    "    df = spark.read.text(path)\n",
    "    df.printSchema()\n",
    "    print(f\"{name}: {df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5cc5b43-8693-4edb-97d8-bb850213808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66eb3a60-1919-4cad-b5d1-84f55c33cd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSNFLAG</th>\n",
       "      <th>HCNFLAG</th>\n",
       "      <th>WMOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>19.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td></td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GS</td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.364</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.0</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0  ACW00011604   17.1167    -61.783       10.0         \n",
       "1  ACW00011647   17.1333    -61.783       19.0         \n",
       "2  AE000041196   25.3330     55.517       34.0         \n",
       "3  AEM00041194   25.2550     55.364       10.0         \n",
       "4  AEM00041217   24.4330     54.651       26.0         \n",
       "\n",
       "                             NAME GSNFLAG HCNFLAG  WMOID  \n",
       "0   ST JOHNS COOLIDGE FLD                                 \n",
       "1   ST JOHNS                                              \n",
       "2   SHARJAH INTER. AIRP                GS           4119  \n",
       "3   DUBAI INTL                                      4119  \n",
       "4   ABU DHABI INTL                                  4121  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSNFLAG</th>\n",
       "      <th>HCNFLAG</th>\n",
       "      <th>WMOID</th>\n",
       "      <th>COUNTRY_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>19.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td></td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GS</td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.364</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.0</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4121</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0  ACW00011604   17.1167    -61.783       10.0         \n",
       "1  ACW00011647   17.1333    -61.783       19.0         \n",
       "2  AE000041196   25.3330     55.517       34.0         \n",
       "3  AEM00041194   25.2550     55.364       10.0         \n",
       "4  AEM00041217   24.4330     54.651       26.0         \n",
       "\n",
       "                             NAME GSNFLAG HCNFLAG  WMOID COUNTRY_CODE  \n",
       "0   ST JOHNS COOLIDGE FLD                                          AC  \n",
       "1   ST JOHNS                                                       AC  \n",
       "2   SHARJAH INTER. AIRP                GS           4119           AE  \n",
       "3   DUBAI INTL                                      4119           AE  \n",
       "4   ABU DHABI INTL                                  4121           AE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSNFLAG</th>\n",
       "      <th>HCNFLAG</th>\n",
       "      <th>WMOID</th>\n",
       "      <th>COUNTRY_CODE</th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AC</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>19.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AC</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td></td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GS</td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.364</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.0</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4121</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0  ACW00011604   17.1167    -61.783       10.0         \n",
       "1  ACW00011647   17.1333    -61.783       19.0         \n",
       "2  AE000041196   25.3330     55.517       34.0         \n",
       "3  AEM00041194   25.2550     55.364       10.0         \n",
       "4  AEM00041217   24.4330     54.651       26.0         \n",
       "\n",
       "                             NAME GSNFLAG HCNFLAG  WMOID COUNTRY_CODE  \\\n",
       "0   ST JOHNS COOLIDGE FLD                                          AC   \n",
       "1   ST JOHNS                                                       AC   \n",
       "2   SHARJAH INTER. AIRP                GS           4119           AE   \n",
       "3   DUBAI INTL                                      4119           AE   \n",
       "4   ABU DHABI INTL                                  4121           AE   \n",
       "\n",
       "            COUNTRY_NAME  \n",
       "0   Antigua and Barbuda   \n",
       "1   Antigua and Barbuda   \n",
       "2  United Arab Emirates   \n",
       "3  United Arab Emirates   \n",
       "4  United Arab Emirates   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COUNTRY_CODE</th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>AC</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>AC</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID COUNTRY_CODE           COUNTRY_NAME\n",
       "0  ACW00011604           AC   Antigua and Barbuda \n",
       "1  ACW00011647           AC   Antigua and Barbuda \n",
       "2  AE000041196           AE  United Arab Emirates \n",
       "3  AEM00041194           AE  United Arab Emirates \n",
       "4  AEM00041217           AE  United Arab Emirates "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "show_as_html(stationsdf.limit(5))\n",
    "\n",
    "# Extract 2-character country code from station ID\n",
    "stationCodedf = stationsdf.withColumn(\"COUNTRY_CODE\", substring(\"ID\", 1, 2))\n",
    "show_as_html(stationCodedf.limit(5))\n",
    "\n",
    "# Rename NAME in countriesdf to avoid conflict\n",
    "countriesRenamed = countriesdf.withColumnRenamed(\"NAME\", \"COUNTRY_NAME\")\n",
    "\n",
    "# LEFT JOIN with countries on country code\n",
    "stationJoined = stationCodedf.join(\n",
    "    countriesRenamed,\n",
    "    stationCodedf[\"COUNTRY_CODE\"] == countriesRenamed[\"CODE\"],\n",
    "    how=\"left\"\n",
    ").drop(countriesRenamed[\"CODE\"])\n",
    "\n",
    "# Preview result\n",
    "show_as_html(stationJoined.limit(5))\n",
    "\n",
    "# Final preview of selected columns\n",
    "stationsSelect = stationJoined.select(\"ID\", \"COUNTRY_CODE\", \"COUNTRY_NAME\")\n",
    "\n",
    "show_as_html(stationsSelect.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7d1e6cd-4d09-4e74-bcc0-72dc7c0ed3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COUNTRY_CODE</th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US009052008</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>SD</td>\n",
       "      <td>SOUTH DAKOTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US10RMHS145</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>CO</td>\n",
       "      <td>COLORADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US10adam001</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>NE</td>\n",
       "      <td>NEBRASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US10adam002</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>NE</td>\n",
       "      <td>NEBRASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US10adam003</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>NE</td>\n",
       "      <td>NEBRASKA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID COUNTRY_CODE   COUNTRY_NAME STATE    STATE_NAME\n",
       "0  US009052008           US  United States    SD  SOUTH DAKOTA\n",
       "1  US10RMHS145           US  United States    CO      COLORADO\n",
       "2  US10adam001           US  United States    NE      NEBRASKA\n",
       "3  US10adam002           US  United States    NE      NEBRASKA\n",
       "4  US10adam003           US  United States    NE      NEBRASKA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring, lit, trim, upper\n",
    "\n",
    "# Reference paths from GHCN\n",
    "statesPath = f\"{prefix}ghcnd-states.txt\"\n",
    "countriesPath = f\"{prefix}ghcnd-countries.txt\"\n",
    "stationsPath = f\"{prefix}ghcnd-stations.txt\"\n",
    "\n",
    "# Load metadata\n",
    "statesdf = spark.read.text(statesPath)\n",
    "countriesdf = spark.read.text(countriesPath)\n",
    "stationsdf = spark.read.text(stationsPath)\n",
    "\n",
    "# Parse states: fixed width (02 = CODE, 3+ = NAME)\n",
    "statesdf = statesdf.select(\n",
    "    trim(substring(\"value\", 1, 2)).alias(\"CODE\"),\n",
    "    trim(substring(\"value\", 4, 50)).alias(\"STATE_NAME\")\n",
    ")\n",
    "\n",
    "# Parse countries: fixed width (02 = CODE, 3+ = NAME)\n",
    "countriesdf = countriesdf.select(\n",
    "    trim(substring(\"value\", 1, 2)).alias(\"CODE\"),\n",
    "    trim(substring(\"value\", 4, 50)).alias(\"COUNTRY_NAME\")\n",
    ")\n",
    "\n",
    "# Parse stations file (already loaded or reuse if exists)\n",
    "stationsdf = spark.read.text(stationsPath)\n",
    "\n",
    "# Extract structured fields from fixed-width station file\n",
    "stationsdf = stationsdf.select(\n",
    "    trim(substring(\"value\", 1, 11)).alias(\"ID\"),\n",
    "    trim(substring(\"value\", 13, 8)).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    trim(substring(\"value\", 22, 8)).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    trim(substring(\"value\", 31, 6)).cast(\"double\").alias(\"ELEVATION\"),\n",
    "    trim(substring(\"value\", 39, 2)).alias(\"STATE\"),\n",
    "    trim(substring(\"value\", 42, 30)).alias(\"NAME\"),\n",
    "    trim(substring(\"value\", 72, 3)).alias(\"GSNFLAG\"),\n",
    "    trim(substring(\"value\", 75, 3)).alias(\"HCNFLAG\"),\n",
    "    trim(substring(\"value\", 78, 5)).alias(\"WMOID\")\n",
    ")\n",
    "\n",
    "# Add country code from station ID\n",
    "stations_with_code = stationsdf.withColumn(\"COUNTRY_CODE\", substring(\"ID\", 1, 2))\n",
    "\n",
    "# Join with countries\n",
    "station_country_joined = stations_with_code.join(\n",
    "    countriesdf,\n",
    "    stations_with_code[\"COUNTRY_CODE\"] == countriesdf[\"CODE\"],\n",
    "    how=\"left\"\n",
    ").drop(countriesdf[\"CODE\"])\n",
    "\n",
    "# Separate US and non-US\n",
    "us_stations = station_country_joined.filter(col(\"COUNTRY_CODE\") == \"US\")\n",
    "non_us_stations = station_country_joined.filter(col(\"COUNTRY_CODE\") != \"US\") \\\n",
    "    .withColumn(\"STATE_NAME\", lit(None).cast(\"string\"))\n",
    "\n",
    "# Join US stations with states (clean key)\n",
    "us_stations_clean = us_stations.withColumn(\"STATE_CLEAN\", upper(trim(col(\"STATE\"))))\n",
    "\n",
    "us_stations_joined = us_stations_clean.join(\n",
    "    statesdf,\n",
    "    us_stations_clean[\"STATE_CLEAN\"] == statesdf[\"CODE\"],\n",
    "    how=\"left\"\n",
    ").drop(\"STATE_CLEAN\", statesdf[\"CODE\"])\n",
    "\n",
    "# Merge US and non-US stations\n",
    "stations_full = us_stations_joined.unionByName(non_us_stations)\n",
    "\n",
    "# Final select and preview\n",
    "stations_select = stations_full.select(\"ID\", \"COUNTRY_CODE\", \"COUNTRY_NAME\", \"STATE\", \"STATE_NAME\")\n",
    "show_as_html(stations_select.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5326477a-8ec3-4781-a873-f51b0635d46d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/meta-daata.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Load and clean states metadata from Excel\u001b[39;00m\n\u001b[1;32m      5\u001b[0m meta_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/meta-daata.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m states_meta \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m states_cleaned \u001b[38;5;241m=\u001b[39m states_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?P<CODE>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+(?P<STATE_NAME>.+)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 2: Convert cleaned metadata to Spark DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/excel/_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    481\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/excel/_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1652\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1659\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/excel/_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1523\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1528\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1529\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/meta-daata.xlsx'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring, lit, trim, upper\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load and clean states metadata from Excel\n",
    "meta_path = \"/mnt/data/meta-daata.xlsx\"\n",
    "states_meta = pd.read_excel(meta_path, sheet_name=\"states\")\n",
    "states_cleaned = states_meta[\"value\"].str.extract(r\"(?P<CODE>\\w+)\\s+(?P<STATE_NAME>.+)\")\n",
    "\n",
    "# Step 2: Convert cleaned metadata to Spark DataFrame\n",
    "statesdf = spark.createDataFrame(states_cleaned)\n",
    "\n",
    "# Step 3: Preview original station data\n",
    "show_as_html(stationsdf.limit(5))\n",
    "\n",
    "# Step 4: Extract country code from station ID\n",
    "stations_with_code = stationsdf.withColumn(\"COUNTRY_CODE\", substring(\"ID\", 1, 2))\n",
    "show_as_html(stations_with_code.limit(5))\n",
    "\n",
    "# Step 5: Join with countries\n",
    "countries_renamed = countriesdf.withColumnRenamed(\"NAME\", \"COUNTRY_NAME\")\n",
    "\n",
    "stations_country_joined = stations_with_code.join(\n",
    "    countries_renamed,\n",
    "    stations_with_code[\"COUNTRY_CODE\"] == countries_renamed[\"CODE\"],\n",
    "    how=\"left\"\n",
    ").drop(countries_renamed[\"CODE\"])\n",
    "\n",
    "# Step 6: Filter US stations and clean STATE\n",
    "us_stations = stations_country_joined.filter(col(\"COUNTRY_CODE\") == \"US\")\n",
    "us_stations_clean = us_stations.withColumn(\"STATE_CLEAN\", upper(trim(col(\"STATE\"))))\n",
    "\n",
    "# Step 7: Join with cleaned statesdf from Excel\n",
    "states_renamed = statesdf.withColumnRenamed(\"STATE_NAME\", \"STATE_NAME\")\n",
    "us_stations_joined = us_stations_clean.join(\n",
    "    states_renamed,\n",
    "    us_stations_clean[\"STATE_CLEAN\"] == states_renamed[\"CODE\"],\n",
    "    how=\"left\"\n",
    ").drop(\"STATE_CLEAN\", states_renamed[\"CODE\"])\n",
    "\n",
    "# Step 8: Add null STATE_NAME to non-US stations\n",
    "non_us_stations = stations_country_joined.filter(col(\"COUNTRY_CODE\") != \"US\") \\\n",
    "    .withColumn(\"STATE_NAME\", lit(None).cast(\"string\"))\n",
    "\n",
    "# Step 9: Merge US and non-US stations\n",
    "stations_full = us_stations_joined.unionByName(non_us_stations)\n",
    "\n",
    "# Step 10: Final selection and preview\n",
    "stations_select = stations_full.select(\"ID\", \"COUNTRY_CODE\", \"COUNTRY_NAME\", \"STATE\", \"STATE_NAME\")\n",
    "show_as_html(stations_select.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7fbbf-2963-48b7-bd6c-a58c316a0962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
