{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c21ce5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/03/31 21:46:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4040\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743410789039</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-4972b2a967a2458b983b0c8294ac0bda</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-48c21395eb617dd0</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743410788869</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4040\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/home/dew59/ghcn-spark-analysis/notebooks/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743410789039</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-4972b2a967a2458b983b0c8294ac0bda</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-48c21395eb617dd0</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743410788869</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./00_ghcn_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d72776",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578aa0",
   "metadata": {},
   "source": [
    "**Q1** First you will investigate the `daily`, `stations`, `states`, `countries`, and `inventory` data provided  in cloud storage in:\n",
    " `wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`  \n",
    "using the `hdfs` command.\n",
    "\n",
    "**(a)** How is the data structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af9cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a3b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5915f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n",
      "--- PART 1\n",
      "drwxrwxrwx   daily\n",
      "-rwxrwxrwx   ghcnd-countries.txt\n",
      "-rwxrwxrwx   ghcnd-inventory.txt\n",
      "-rwxrwxrwx   ghcnd-states.txt\n",
      "-rwxrwxrwx   ghcnd-stations.txt\n",
      "--- PART 2\n",
      "Unable                    2025-03-31 21:47:12,076 WARN util.NativeCodeLoader:\n",
      "Loaded                    2025-03-31 21:47:12,345 INFO impl.MetricsConfig:\n",
      "Scheduled                 2025-03-31 21:47:12,394 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-03-31 21:47:12,395 INFO impl.MetricsSystemImpl:\n",
      "ghcnd-countries.txt       3.6 K   3.6 K  \n",
      "ghcnd-states.txt          1.1 K   1.1 K  \n",
      "ghcnd-stations.txt        10.6 M  10.6 M \n",
      "daily                     13.0 G  13.0 G \n",
      "ghcnd-inventory.txt       33.6 M  33.6 M \n",
      "Stopping                  2025-03-31 21:47:13,075 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-03-31 21:47:13,075 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-03-31 21:47:13,075 INFO impl.MetricsSystemImpl:\n",
      "--- PART 3\n",
      "2025-03-31 21:47:14,193 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-03-31 21:47:14,462 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-03-31 21:47:14,510 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-03-31 21:47:14,510 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "           1          264        13975887693 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily\n",
      "2025-03-31 21:47:15,266 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n",
      "2025-03-31 21:47:15,266 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n",
      "2025-03-31 21:47:15,266 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n",
      "--- PART 4\n",
      "1750.csv.gz     1385743\n",
      "1763.csv.gz     3358\n",
      "1764.csv.gz     3327\n",
      "1765.csv.gz     3335\n",
      "1766.csv.gz     3344\n",
      "1767.csv.gz     3356\n",
      "1768.csv.gz     3325\n",
      "1769.csv.gz     3418\n",
      "1770.csv.gz     3357\n",
      "1771.csv.gz     3373\n",
      "1772.csv.gz     3419\n",
      "1773.csv.gz     3368\n",
      "1774.csv.gz     3393\n",
      "1775.csv.gz     6365\n",
      "1776.csv.gz     6425\n",
      "1777.csv.gz     6424\n",
      "1778.csv.gz     6240\n",
      "1779.csv.gz     6144\n",
      "1780.csv.gz     6245\n",
      "1781.csv.gz     7799\n",
      "1782.csv.gz     7808\n",
      "1783.csv.gz     7920\n",
      "1784.csv.gz     7946\n",
      "1785.csv.gz     7804\n",
      "1786.csv.gz     7891\n",
      "1787.csv.gz     6336\n",
      "1788.csv.gz     6394\n",
      "1789.csv.gz     7773\n",
      "1790.csv.gz     7786\n",
      "1791.csv.gz     7734\n",
      "1792.csv.gz     7806\n",
      "1793.csv.gz     6342\n",
      "1794.csv.gz     7770\n",
      "1795.csv.gz     7843\n",
      "1796.csv.gz     7821\n",
      "1797.csv.gz     9239\n",
      "1798.csv.gz     9290\n",
      "1799.csv.gz     6351\n",
      "1800.csv.gz     7783\n",
      "1801.csv.gz     7801\n",
      "1802.csv.gz     9195\n",
      "1803.csv.gz     7892\n",
      "1804.csv.gz     8727\n",
      "1805.csv.gz     8901\n",
      "1806.csv.gz     8598\n",
      "1807.csv.gz     8725\n",
      "1808.csv.gz     8928\n",
      "1809.csv.gz     8778\n",
      "1810.csv.gz     8829\n",
      "1811.csv.gz     8932\n",
      "1812.csv.gz     8993\n",
      "1813.csv.gz     9264\n",
      "1814.csv.gz     10833\n",
      "1815.csv.gz     13884\n",
      "1816.csv.gz     13865\n",
      "1817.csv.gz     13758\n",
      "1818.csv.gz     13758\n",
      "1819.csv.gz     13690\n",
      "1820.csv.gz     13997\n",
      "1821.csv.gz     13792\n",
      "1822.csv.gz     14049\n",
      "1823.csv.gz     14745\n",
      "1824.csv.gz     18035\n",
      "1825.csv.gz     18101\n",
      "1826.csv.gz     18403\n",
      "1827.csv.gz     20855\n",
      "1828.csv.gz     20981\n",
      "1829.csv.gz     21141\n",
      "1830.csv.gz     21306\n",
      "1831.csv.gz     21319\n",
      "1832.csv.gz     22435\n",
      "1833.csv.gz     27262\n",
      "1834.csv.gz     27164\n",
      "1835.csv.gz     27546\n",
      "1836.csv.gz     29795\n",
      "1837.csv.gz     29334\n",
      "1838.csv.gz     31459\n",
      "1839.csv.gz     29361\n",
      "1840.csv.gz     35913\n",
      "1841.csv.gz     36864\n",
      "1842.csv.gz     39570\n",
      "1843.csv.gz     39535\n",
      "1844.csv.gz     43325\n",
      "1845.csv.gz     52256\n",
      "1846.csv.gz     49535\n",
      "1847.csv.gz     50847\n",
      "1848.csv.gz     50033\n",
      "1849.csv.gz     51977\n",
      "1850.csv.gz     51871\n",
      "1851.csv.gz     59192\n",
      "1852.csv.gz     63805\n",
      "1853.csv.gz     64356\n",
      "1854.csv.gz     64004\n",
      "1855.csv.gz     70710\n",
      "1856.csv.gz     81212\n",
      "1857.csv.gz     88910\n",
      "1858.csv.gz     122842\n",
      "1859.csv.gz     136813\n",
      "1860.csv.gz     146995\n",
      "1861.csv.gz     150606\n",
      "1862.csv.gz     145625\n",
      "1863.csv.gz     164239\n",
      "1864.csv.gz     164146\n",
      "1865.csv.gz     163927\n",
      "1866.csv.gz     212821\n",
      "1867.csv.gz     256701\n",
      "1868.csv.gz     269545\n",
      "1869.csv.gz     314469\n",
      "1870.csv.gz     360140\n",
      "1871.csv.gz     477098\n",
      "1872.csv.gz     650129\n",
      "1873.csv.gz     728688\n",
      "1874.csv.gz     815760\n",
      "1875.csv.gz     886014\n",
      "1876.csv.gz     965160\n",
      "1877.csv.gz     1077970\n",
      "1878.csv.gz     1274702\n",
      "1879.csv.gz     1461379\n",
      "1880.csv.gz     1847043\n",
      "1881.csv.gz     2208841\n",
      "1882.csv.gz     2512758\n",
      "1883.csv.gz     2775527\n",
      "1884.csv.gz     3303464\n",
      "1885.csv.gz     3745027\n",
      "1886.csv.gz     4102788\n",
      "1887.csv.gz     4605113\n",
      "1888.csv.gz     4958378\n",
      "1889.csv.gz     5388271\n",
      "1890.csv.gz     5878742\n",
      "1891.csv.gz     6170065\n",
      "1892.csv.gz     7118004\n",
      "1893.csv.gz     13052983\n",
      "1894.csv.gz     13929624\n",
      "1895.csv.gz     15036340\n",
      "1896.csv.gz     16204018\n",
      "1897.csv.gz     17474772\n",
      "1898.csv.gz     18225580\n",
      "1899.csv.gz     18953308\n",
      "1900.csv.gz     20251196\n",
      "1901.csv.gz     25767142\n",
      "1902.csv.gz     26736821\n",
      "1903.csv.gz     27302649\n",
      "1904.csv.gz     28201320\n",
      "1905.csv.gz     29632148\n",
      "1906.csv.gz     30289765\n",
      "1907.csv.gz     31144951\n",
      "1908.csv.gz     31867868\n",
      "1909.csv.gz     33519209\n",
      "1910.csv.gz     34622865\n",
      "1911.csv.gz     35904462\n",
      "1912.csv.gz     37263226\n",
      "1913.csv.gz     38255770\n",
      "1914.csv.gz     39485528\n",
      "1915.csv.gz     40658560\n",
      "1916.csv.gz     42029487\n",
      "1917.csv.gz     42252301\n",
      "1918.csv.gz     41304535\n",
      "1919.csv.gz     40799997\n",
      "1920.csv.gz     41033429\n",
      "1921.csv.gz     41324111\n",
      "1922.csv.gz     42113624\n",
      "1923.csv.gz     42862524\n",
      "1924.csv.gz     43710214\n",
      "1925.csv.gz     44015966\n",
      "1926.csv.gz     45226774\n",
      "1927.csv.gz     46033600\n",
      "1928.csv.gz     46580018\n",
      "1929.csv.gz     47573559\n",
      "1930.csv.gz     48987101\n",
      "1931.csv.gz     50624633\n",
      "1932.csv.gz     51705273\n",
      "1933.csv.gz     52178613\n",
      "1934.csv.gz     52521149\n",
      "1935.csv.gz     53488183\n",
      "1936.csv.gz     56768695\n",
      "1937.csv.gz     58175289\n",
      "1938.csv.gz     59344873\n",
      "1939.csv.gz     61148235\n",
      "1940.csv.gz     63463421\n",
      "1941.csv.gz     65375485\n",
      "1942.csv.gz     67537465\n",
      "1943.csv.gz     68405592\n",
      "1944.csv.gz     70192973\n",
      "1945.csv.gz     72659632\n",
      "1946.csv.gz     73148444\n",
      "1947.csv.gz     74970819\n",
      "1948.csv.gz     89145605\n",
      "1949.csv.gz     101758958\n",
      "1950.csv.gz     104856670\n",
      "1951.csv.gz     108201081\n",
      "1952.csv.gz     109667528\n",
      "1953.csv.gz     111171866\n",
      "1954.csv.gz     113330494\n",
      "1955.csv.gz     115757071\n",
      "1956.csv.gz     117984088\n",
      "1957.csv.gz     120673205\n",
      "1958.csv.gz     121914900\n",
      "1959.csv.gz     124376950\n",
      "1960.csv.gz     126849252\n",
      "1961.csv.gz     130750539\n",
      "1962.csv.gz     133559230\n",
      "1963.csv.gz     136585486\n",
      "1964.csv.gz     137581099\n",
      "1965.csv.gz     142018261\n",
      "1966.csv.gz     143937982\n",
      "1967.csv.gz     145306876\n",
      "1968.csv.gz     144896387\n",
      "1969.csv.gz     146762160\n",
      "1970.csv.gz     147692050\n",
      "1971.csv.gz     142221845\n",
      "1972.csv.gz     141305619\n",
      "1973.csv.gz     148100923\n",
      "1974.csv.gz     149378829\n",
      "1975.csv.gz     148905604\n",
      "1976.csv.gz     148764292\n",
      "1977.csv.gz     148575501\n",
      "1978.csv.gz     148815187\n",
      "1979.csv.gz     149117704\n",
      "1980.csv.gz     149579207\n",
      "1981.csv.gz     152656825\n",
      "1982.csv.gz     154508476\n",
      "1983.csv.gz     155940158\n",
      "1984.csv.gz     154313147\n",
      "1985.csv.gz     152811879\n",
      "1986.csv.gz     151770851\n",
      "1987.csv.gz     151809884\n",
      "1988.csv.gz     152626966\n",
      "1989.csv.gz     153053088\n",
      "1990.csv.gz     153157679\n",
      "1991.csv.gz     153852159\n",
      "1992.csv.gz     154026142\n",
      "1993.csv.gz     152865318\n",
      "1994.csv.gz     151844762\n",
      "1995.csv.gz     151469358\n",
      "1996.csv.gz     151756945\n",
      "1997.csv.gz     150454010\n",
      "1998.csv.gz     153494337\n",
      "1999.csv.gz     156271670\n",
      "2000.csv.gz     158226488\n",
      "2001.csv.gz     160670036\n",
      "2002.csv.gz     162275809\n",
      "2003.csv.gz     165928854\n",
      "2004.csv.gz     168459055\n",
      "2005.csv.gz     165289423\n",
      "2006.csv.gz     171894892\n",
      "2007.csv.gz     174934788\n",
      "2008.csv.gz     182624806\n",
      "2009.csv.gz     185641000\n",
      "2010.csv.gz     187293865\n",
      "2011.csv.gz     177949864\n",
      "2012.csv.gz     175262367\n",
      "2013.csv.gz     170236479\n",
      "2014.csv.gz     168607573\n",
      "2015.csv.gz     171192339\n",
      "2016.csv.gz     172621542\n",
      "2017.csv.gz     172257484\n",
      "2018.csv.gz     172359764\n",
      "2019.csv.gz     171131311\n",
      "2020.csv.gz     172132828\n",
      "2021.csv.gz     175050475\n",
      "2022.csv.gz     175205910\n",
      "2023.csv.gz     174726829\n",
      "2024.csv.gz     168485088\n",
      "2025.csv.gz     17061071\n"
     ]
    }
   ],
   "source": [
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "aDaily         = f'/2025.csv.gz'\n",
    "prefix         = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/'\n",
    "prefixDaily    = f'{prefix}/daily/'\n",
    "\n",
    "print(prefix)\n",
    "#! hdfs dfs -du -h  {prefix}\n",
    "#! hdfs dfs -ls     {prefix} #structure \n",
    "#! hdfs dfs -ls     {prefixDaily} #structure \n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/daily/\n",
    "\n",
    "print(\"--- PART 1\")\n",
    "\n",
    "lines_ls = !hdfs dfs -ls {prefix}\n",
    "parsed_ls = []\n",
    "\n",
    "for line in lines_ls:\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(\"INFO\") or line.startswith(\"WARN\") or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 2:\n",
    "        perms     = parts[0]\n",
    "        full_path = parts[-1]\n",
    "\n",
    "        if perms.startswith(\"-\") or perms.startswith(\"d\"):\n",
    "            rel_path = full_path.replace(prefix, '')\n",
    "            parsed_ls.append((perms, rel_path))\n",
    "\n",
    "# Print result\n",
    "for perms, name in parsed_ls:\n",
    "    print(f\"{perms:<12} {name}\")\n",
    "\n",
    "\n",
    "print(\"--- PART 2\")\n",
    "# --- PART 2:  -du -h (size info) ---\n",
    "lines     = !hdfs dfs -du -h  {prefix}\n",
    "parsed_du = []\n",
    "for line in lines:\n",
    "    if line.startswith(\"INFO\") or line.startswith(\"WARN\"):\n",
    "        continue  # skip log lines\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 5:\n",
    "        size1    = f\"{parts[0]} {parts[1]}\"\n",
    "        size2    = f\"{parts[2]} {parts[3]}\"\n",
    "        full_path = parts[4]\n",
    "    elif len(parts) >= 3:\n",
    "        size1, size2, full_path = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    rel_path = full_path.replace(prefix, '')\n",
    "    parsed_du.append((rel_path, size1, size2))\n",
    "\n",
    "# Print parsed_du\n",
    "for name, size1, size2 in parsed_du:\n",
    "    print(f\"{name:<25} {size1:<7} {size2:<7}\")\n",
    "\n",
    "\n",
    "print(\"--- PART 3\")\n",
    "\n",
    "!hdfs dfs -count  {prefixDaily}\n",
    "\n",
    "print(\"--- PART 4\")\n",
    "\n",
    "lines_daily = !hdfs dfs -ls {prefixDaily}\n",
    "parsed_daily = []\n",
    "\n",
    "for line in lines_daily:\n",
    "    line = line.strip()\n",
    "\n",
    "    # Skip noise and non-data lines\n",
    "    if not line or \"INFO\" in line or \"WARN\" in line or \"Found\" in line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) == 6:  # ✅ Exact match from your image\n",
    "        size      = parts[2]\n",
    "        full_path = parts[5]\n",
    "        file_name = full_path.rsplit('/', 1)[-1]\n",
    "        parsed_daily.append((size, file_name))\n",
    "    else:\n",
    "        print(f\"Skipped line (unexpected format): {line}\")\n",
    "\n",
    "# Output\n",
    "if parsed_daily:\n",
    "    for size, name in parsed_daily:\n",
    "        print(f\"{name:<15} {size}\")\n",
    "else:\n",
    "    print(\"⚠️ No valid entries found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24737373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2025.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Define the input path for the last year in daily\n",
    "\n",
    "daily_relative_path = f'ghcnd/daily/2025.csv.gz'\n",
    "daily_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{daily_relative_path}'\n",
    "\n",
    "print(daily_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of the last year in daily into Spark from Azure Blob Storage using spark.read.csv\n",
    "\n",
    "daily = spark.read.csv(daily_path).limit(1000)\n",
    "\n",
    "print(type(daily))\n",
    "daily.printSchema()\n",
    "print(daily)\n",
    "daily.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46600172",
   "metadata": {},
   "source": [
    "**(b)** How many years are contained in `daily`, and how does the size of the data change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23e3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/mnt/m': No such file or directory\n",
      "ls: cannot access '/mnt/p': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls /mnt/m\n",
    "!ls /mnt/p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576a535",
   "metadata": {},
   "source": [
    "**(c)** What is the total size of all of the data, and how much of that is `daily`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a946d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
