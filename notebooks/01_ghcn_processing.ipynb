{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ce5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./00_notebook_with_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d72776",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578aa0",
   "metadata": {},
   "source": [
    "**Q1** First you will investigate the `daily`, `stations`, `states`, `countries`, and `inventory` data provided  in cloud storage in:\n",
    " `wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`  \n",
    "using the `hdfs` command.\n",
    "\n",
    "**(a)** How is the data structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "aDaily      = f'/2025.csv.gz'\n",
    "remotePath  = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/\n",
    "remoteDaily = f'{remotePath}/daily/\"\n",
    "print(remoteDaily)\n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/daily/\n",
    "! hdfs dhs -ls    $remotepath\n",
    "! hdfs dhs -du -h $remotepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24737373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input path for the last year in daily\n",
    "\n",
    "daily_relative_path = f'ghcnd/daily/2025.csv.gz'\n",
    "daily_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{daily_relative_path}'\n",
    "\n",
    "print(daily_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of the last year in daily into Spark from Azure Blob Storage using spark.read.csv\n",
    "\n",
    "daily = spark.read.csv(daily_path).limit(1000)\n",
    "\n",
    "print(type(daily))\n",
    "daily.printSchema()\n",
    "print(daily)\n",
    "daily.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46600172",
   "metadata": {},
   "source": [
    "**(b)** How many years are contained in `daily`, and how does the size of the data change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576a535",
   "metadata": {},
   "source": [
    "**(c)** What is the total size of all of the data, and how much of that is `daily`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a946d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
