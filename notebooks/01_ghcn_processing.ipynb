{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d72776",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578aa0",
   "metadata": {},
   "source": [
    "**Q1** First you will investigate the `daily`, `stations`, `states`, `countries`, and `inventory` data provided  in cloud storage in:\n",
    " `wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`  \n",
    "using the `hdfs` command.\n",
    "\n",
    "**(a)** How is the data structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af9cd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/04/01 13:35:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4048\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743467718171</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-0e68997e8b91472faf3dccc7229670a4</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743467718296</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-dc63e295eec629d8</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4048\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/home/dew59/ghcn-spark-analysis/notebooks/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743467718171</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-0e68997e8b91472faf3dccc7229670a4</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743467718296</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-dc63e295eec629d8</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./00_ghcn_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a3b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d6483f-a00f-4e25-83b4-4cd4af8ab743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n"
     ]
    }
   ],
   "source": [
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "aDaily         = f'/2025.csv.gz'\n",
    "prefix         = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/'\n",
    "prefixDaily    = f'{prefix}/daily/'\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d988e31f-04e9-4ab2-9e8d-018996389e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PART 1\n",
      "drwxrwxrwx   daily\n",
      "-rwxrwxrwx   ghcnd-countries.txt\n",
      "-rwxrwxrwx   ghcnd-inventory.txt\n",
      "-rwxrwxrwx   ghcnd-states.txt\n",
      "-rwxrwxrwx   ghcnd-stations.txt\n",
      "--- PART 2\n",
      "Unable                    2025-04-01 14:15:26,310 WARN util.NativeCodeLoader:\n",
      "Loaded                    2025-04-01 14:15:26,589 INFO impl.MetricsConfig:\n",
      "Scheduled                 2025-04-01 14:15:26,637 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-01 14:15:26,637 INFO impl.MetricsSystemImpl:\n",
      "ghcnd-countries.txt       3.6 K   3.6 K  \n",
      "ghcnd-states.txt          1.1 K   1.1 K  \n",
      "ghcnd-stations.txt        10.6 M  10.6 M \n",
      "daily                     13.0 G  13.0 G \n",
      "ghcnd-inventory.txt       33.6 M  33.6 M \n",
      "Stopping                  2025-04-01 14:15:27,357 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-01 14:15:27,357 INFO impl.MetricsSystemImpl:\n",
      "azure-file-system         2025-04-01 14:15:27,357 INFO impl.MetricsSystemImpl:\n",
      "--- PART 3\n",
      "2025-04-01 14:15:28,500 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-04-01 14:15:28,780 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-04-01 14:15:28,828 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-04-01 14:15:28,828 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "           1          264        13975887693 wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily\n",
      "2025-04-01 14:15:29,451 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n",
      "2025-04-01 14:15:29,451 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n",
      "2025-04-01 14:15:29,451 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n",
      "--- PART 4\n",
      "1750.csv.gz     1385743\n",
      "1763.csv.gz     3358\n",
      "1764.csv.gz     3327\n",
      "1765.csv.gz     3335\n",
      "1766.csv.gz     3344\n",
      "1767.csv.gz     3356\n",
      "1768.csv.gz     3325\n",
      "1769.csv.gz     3418\n",
      "1770.csv.gz     3357\n",
      "1771.csv.gz     3373\n",
      "1772.csv.gz     3419\n",
      "1773.csv.gz     3368\n",
      "1774.csv.gz     3393\n",
      "1775.csv.gz     6365\n",
      "1776.csv.gz     6425\n",
      "1777.csv.gz     6424\n",
      "1778.csv.gz     6240\n",
      "1779.csv.gz     6144\n",
      "1780.csv.gz     6245\n",
      "1781.csv.gz     7799\n",
      "1782.csv.gz     7808\n",
      "1783.csv.gz     7920\n",
      "1784.csv.gz     7946\n",
      "1785.csv.gz     7804\n",
      "1786.csv.gz     7891\n",
      "1787.csv.gz     6336\n",
      "1788.csv.gz     6394\n",
      "1789.csv.gz     7773\n",
      "1790.csv.gz     7786\n",
      "1791.csv.gz     7734\n",
      "1792.csv.gz     7806\n",
      "1793.csv.gz     6342\n",
      "1794.csv.gz     7770\n",
      "1795.csv.gz     7843\n",
      "1796.csv.gz     7821\n",
      "1797.csv.gz     9239\n",
      "1798.csv.gz     9290\n",
      "1799.csv.gz     6351\n",
      "1800.csv.gz     7783\n",
      "1801.csv.gz     7801\n",
      "1802.csv.gz     9195\n",
      "1803.csv.gz     7892\n",
      "1804.csv.gz     8727\n",
      "1805.csv.gz     8901\n",
      "1806.csv.gz     8598\n",
      "1807.csv.gz     8725\n",
      "1808.csv.gz     8928\n",
      "1809.csv.gz     8778\n",
      "1810.csv.gz     8829\n",
      "1811.csv.gz     8932\n",
      "1812.csv.gz     8993\n",
      "1813.csv.gz     9264\n",
      "1814.csv.gz     10833\n",
      "1815.csv.gz     13884\n",
      "1816.csv.gz     13865\n",
      "1817.csv.gz     13758\n",
      "1818.csv.gz     13758\n",
      "1819.csv.gz     13690\n",
      "1820.csv.gz     13997\n",
      "1821.csv.gz     13792\n",
      "1822.csv.gz     14049\n",
      "1823.csv.gz     14745\n",
      "1824.csv.gz     18035\n",
      "1825.csv.gz     18101\n",
      "1826.csv.gz     18403\n",
      "1827.csv.gz     20855\n",
      "1828.csv.gz     20981\n",
      "1829.csv.gz     21141\n",
      "1830.csv.gz     21306\n",
      "1831.csv.gz     21319\n",
      "1832.csv.gz     22435\n",
      "1833.csv.gz     27262\n",
      "1834.csv.gz     27164\n",
      "1835.csv.gz     27546\n",
      "1836.csv.gz     29795\n",
      "1837.csv.gz     29334\n",
      "1838.csv.gz     31459\n",
      "1839.csv.gz     29361\n",
      "1840.csv.gz     35913\n",
      "1841.csv.gz     36864\n",
      "1842.csv.gz     39570\n",
      "1843.csv.gz     39535\n",
      "1844.csv.gz     43325\n",
      "1845.csv.gz     52256\n",
      "1846.csv.gz     49535\n",
      "1847.csv.gz     50847\n",
      "1848.csv.gz     50033\n",
      "1849.csv.gz     51977\n",
      "1850.csv.gz     51871\n",
      "1851.csv.gz     59192\n",
      "1852.csv.gz     63805\n",
      "1853.csv.gz     64356\n",
      "1854.csv.gz     64004\n",
      "1855.csv.gz     70710\n",
      "1856.csv.gz     81212\n",
      "1857.csv.gz     88910\n",
      "1858.csv.gz     122842\n",
      "1859.csv.gz     136813\n",
      "1860.csv.gz     146995\n",
      "1861.csv.gz     150606\n",
      "1862.csv.gz     145625\n",
      "1863.csv.gz     164239\n",
      "1864.csv.gz     164146\n",
      "1865.csv.gz     163927\n",
      "1866.csv.gz     212821\n",
      "1867.csv.gz     256701\n",
      "1868.csv.gz     269545\n",
      "1869.csv.gz     314469\n",
      "1870.csv.gz     360140\n",
      "1871.csv.gz     477098\n",
      "1872.csv.gz     650129\n",
      "1873.csv.gz     728688\n",
      "1874.csv.gz     815760\n",
      "1875.csv.gz     886014\n",
      "1876.csv.gz     965160\n",
      "1877.csv.gz     1077970\n",
      "1878.csv.gz     1274702\n",
      "1879.csv.gz     1461379\n",
      "1880.csv.gz     1847043\n",
      "1881.csv.gz     2208841\n",
      "1882.csv.gz     2512758\n",
      "1883.csv.gz     2775527\n",
      "1884.csv.gz     3303464\n",
      "1885.csv.gz     3745027\n",
      "1886.csv.gz     4102788\n",
      "1887.csv.gz     4605113\n",
      "1888.csv.gz     4958378\n",
      "1889.csv.gz     5388271\n",
      "1890.csv.gz     5878742\n",
      "1891.csv.gz     6170065\n",
      "1892.csv.gz     7118004\n",
      "1893.csv.gz     13052983\n",
      "1894.csv.gz     13929624\n",
      "1895.csv.gz     15036340\n",
      "1896.csv.gz     16204018\n",
      "1897.csv.gz     17474772\n",
      "1898.csv.gz     18225580\n",
      "1899.csv.gz     18953308\n",
      "1900.csv.gz     20251196\n",
      "1901.csv.gz     25767142\n",
      "1902.csv.gz     26736821\n",
      "1903.csv.gz     27302649\n",
      "1904.csv.gz     28201320\n",
      "1905.csv.gz     29632148\n",
      "1906.csv.gz     30289765\n",
      "1907.csv.gz     31144951\n",
      "1908.csv.gz     31867868\n",
      "1909.csv.gz     33519209\n",
      "1910.csv.gz     34622865\n",
      "1911.csv.gz     35904462\n",
      "1912.csv.gz     37263226\n",
      "1913.csv.gz     38255770\n",
      "1914.csv.gz     39485528\n",
      "1915.csv.gz     40658560\n",
      "1916.csv.gz     42029487\n",
      "1917.csv.gz     42252301\n",
      "1918.csv.gz     41304535\n",
      "1919.csv.gz     40799997\n",
      "1920.csv.gz     41033429\n",
      "1921.csv.gz     41324111\n",
      "1922.csv.gz     42113624\n",
      "1923.csv.gz     42862524\n",
      "1924.csv.gz     43710214\n",
      "1925.csv.gz     44015966\n",
      "1926.csv.gz     45226774\n",
      "1927.csv.gz     46033600\n",
      "1928.csv.gz     46580018\n",
      "1929.csv.gz     47573559\n",
      "1930.csv.gz     48987101\n",
      "1931.csv.gz     50624633\n",
      "1932.csv.gz     51705273\n",
      "1933.csv.gz     52178613\n",
      "1934.csv.gz     52521149\n",
      "1935.csv.gz     53488183\n",
      "1936.csv.gz     56768695\n",
      "1937.csv.gz     58175289\n",
      "1938.csv.gz     59344873\n",
      "1939.csv.gz     61148235\n",
      "1940.csv.gz     63463421\n",
      "1941.csv.gz     65375485\n",
      "1942.csv.gz     67537465\n",
      "1943.csv.gz     68405592\n",
      "1944.csv.gz     70192973\n",
      "1945.csv.gz     72659632\n",
      "1946.csv.gz     73148444\n",
      "1947.csv.gz     74970819\n",
      "1948.csv.gz     89145605\n",
      "1949.csv.gz     101758958\n",
      "1950.csv.gz     104856670\n",
      "1951.csv.gz     108201081\n",
      "1952.csv.gz     109667528\n",
      "1953.csv.gz     111171866\n",
      "1954.csv.gz     113330494\n",
      "1955.csv.gz     115757071\n",
      "1956.csv.gz     117984088\n",
      "1957.csv.gz     120673205\n",
      "1958.csv.gz     121914900\n",
      "1959.csv.gz     124376950\n",
      "1960.csv.gz     126849252\n",
      "1961.csv.gz     130750539\n",
      "1962.csv.gz     133559230\n",
      "1963.csv.gz     136585486\n",
      "1964.csv.gz     137581099\n",
      "1965.csv.gz     142018261\n",
      "1966.csv.gz     143937982\n",
      "1967.csv.gz     145306876\n",
      "1968.csv.gz     144896387\n",
      "1969.csv.gz     146762160\n",
      "1970.csv.gz     147692050\n",
      "1971.csv.gz     142221845\n",
      "1972.csv.gz     141305619\n",
      "1973.csv.gz     148100923\n",
      "1974.csv.gz     149378829\n",
      "1975.csv.gz     148905604\n",
      "1976.csv.gz     148764292\n",
      "1977.csv.gz     148575501\n",
      "1978.csv.gz     148815187\n",
      "1979.csv.gz     149117704\n",
      "1980.csv.gz     149579207\n",
      "1981.csv.gz     152656825\n",
      "1982.csv.gz     154508476\n",
      "1983.csv.gz     155940158\n",
      "1984.csv.gz     154313147\n",
      "1985.csv.gz     152811879\n",
      "1986.csv.gz     151770851\n",
      "1987.csv.gz     151809884\n",
      "1988.csv.gz     152626966\n",
      "1989.csv.gz     153053088\n",
      "1990.csv.gz     153157679\n",
      "1991.csv.gz     153852159\n",
      "1992.csv.gz     154026142\n",
      "1993.csv.gz     152865318\n",
      "1994.csv.gz     151844762\n",
      "1995.csv.gz     151469358\n",
      "1996.csv.gz     151756945\n",
      "1997.csv.gz     150454010\n",
      "1998.csv.gz     153494337\n",
      "1999.csv.gz     156271670\n",
      "2000.csv.gz     158226488\n",
      "2001.csv.gz     160670036\n",
      "2002.csv.gz     162275809\n",
      "2003.csv.gz     165928854\n",
      "2004.csv.gz     168459055\n",
      "2005.csv.gz     165289423\n",
      "2006.csv.gz     171894892\n",
      "2007.csv.gz     174934788\n",
      "2008.csv.gz     182624806\n",
      "2009.csv.gz     185641000\n",
      "2010.csv.gz     187293865\n",
      "2011.csv.gz     177949864\n",
      "2012.csv.gz     175262367\n",
      "2013.csv.gz     170236479\n",
      "2014.csv.gz     168607573\n",
      "2015.csv.gz     171192339\n",
      "2016.csv.gz     172621542\n",
      "2017.csv.gz     172257484\n",
      "2018.csv.gz     172359764\n",
      "2019.csv.gz     171131311\n",
      "2020.csv.gz     172132828\n",
      "2021.csv.gz     175050475\n",
      "2022.csv.gz     175205910\n",
      "2023.csv.gz     174726829\n",
      "2024.csv.gz     168485088\n",
      "2025.csv.gz     17061071\n"
     ]
    }
   ],
   "source": [
    "#! hdfs dfs -du -h  {prefix}\n",
    "#! hdfs dfs -ls     {prefix} #structure \n",
    "#! hdfs dfs -ls     {prefixDaily} #structure \n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/daily/\n",
    "\n",
    "print(\"--- PART 1\")\n",
    "\n",
    "lines_ls = !hdfs dfs -ls {prefix}\n",
    "parsed_ls = []\n",
    "\n",
    "for line in lines_ls:\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(\"INFO\") or line.startswith(\"WARN\") or \"Found\" in line: # noise\n",
    "        continue                                                                          # noise \n",
    "\n",
    "    parts = line.split()                   \n",
    "    if len(parts) >= 2:\n",
    "        perms     = parts[0]\n",
    "        full_path = parts[-1]\n",
    "\n",
    "        if perms.startswith(\"-\") or perms.startswith(\"d\"): # file or directory \n",
    "            rel_path = full_path.replace(prefix, '')\n",
    "            parsed_ls.append((perms, rel_path))\n",
    "\n",
    "# Print result\n",
    "for perms, name in parsed_ls:\n",
    "    print(f\"{perms:<12} {name}\")  #columns are aligned\n",
    "\n",
    "print(\"--- PART 2\")\n",
    "# --- PART 2:  -du -h (size info) ---\n",
    "lines     = !hdfs dfs -du -h  {prefix}\n",
    "parsed_du = []\n",
    "for line in lines:\n",
    "    if line.startswith(\"INFO\") or line.startswith(\"WARN\"): # some noise\n",
    "        continue                                           # some noise\n",
    "\n",
    "    parts = line.split()                                  \n",
    "    if len(parts) >= 5:\n",
    "        size1    = f\"{parts[0]} {parts[1]}\"\n",
    "        size2    = f\"{parts[2]} {parts[3]}\"\n",
    "        full_path = parts[4]\n",
    "    elif len(parts) >= 3:\n",
    "        size1, size2, full_path = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    rel_path = full_path.replace(prefix, '')\n",
    "    parsed_du.append((rel_path, size1, size2))\n",
    "\n",
    "# Print parsed_du\n",
    "for name, size1, size2 in parsed_du:\n",
    "    print(f\"{name:<25} {size1:<7} {size2:<7}\")\n",
    "\n",
    "\n",
    "print(\"--- PART 3\")\n",
    "\n",
    "!hdfs dfs -count  {prefixDaily}\n",
    "\n",
    "print(\"--- PART 4\")\n",
    "\n",
    "lines_daily = !hdfs dfs -ls {prefixDaily}\n",
    "parsed_daily = []\n",
    "\n",
    "for line in lines_daily:\n",
    "    line = line.strip()\n",
    "\n",
    "    \n",
    "    if not line or \"INFO\" in line or \"WARN\" in line or \"Found\" in line: # Skip noise and non-data lines\n",
    "        continue                                                        # Skip noise and non-data lines\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) == 6:  # Exact match \n",
    "        size      = parts[2]\n",
    "        full_path = parts[5]\n",
    "        file_name = full_path.rsplit('/', 1)[-1]\n",
    "        parsed_daily.append((size, file_name))\n",
    "    else:\n",
    "        print(f\"(wrong format): {line}\")\n",
    "\n",
    "# \n",
    "if parsed_daily:\n",
    "    for size, name in parsed_daily:\n",
    "        print(f\"{name:<15} {size}\")\n",
    "else:\n",
    "    print(\"none found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5432c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\64276\\appdata\\local\\temp\\ipykernel_286612\\2449182363.py\u001b[0m(1)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2744cf47",
   "metadata": {},
   "source": [
    "### Q2 (a)\n",
    "Schema for `daily` based on  GHCN Daily README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14167028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: integer (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBS_TIME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MFLAG</th>\n",
       "      <th>QFLAG</th>\n",
       "      <th>SFLAG</th>\n",
       "      <th>OBS_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00037091</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00037098</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00037104</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASN00037105</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASN00037106</td>\n",
       "      <td>20220101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      DATE ELEMENT  VALUE MFLAG QFLAG SFLAG OBS_TIME\n",
       "0  ASN00037091  20220101    PRCP      0  None  None     a     None\n",
       "1  ASN00037098  20220101    PRCP      0  None  None     a     None\n",
       "2  ASN00037104  20220101    PRCP      0  None  None     a     None\n",
       "3  ASN00037105  20220101    PRCP      0  None  None     a     None\n",
       "4  ASN00037106  20220101    PRCP      0  None  None     a     None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: integer (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBS_TIME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MFLAG</th>\n",
       "      <th>QFLAG</th>\n",
       "      <th>SFLAG</th>\n",
       "      <th>OBS_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00037091</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00037098</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00037104</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASN00037105</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASN00037106</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID        DATE ELEMENT  VALUE MFLAG QFLAG SFLAG OBS_TIME\n",
       "0  ASN00037091  2022-01-01    PRCP      0  None  None     a     None\n",
       "1  ASN00037098  2022-01-01    PRCP      0  None  None     a     None\n",
       "2  ASN00037104  2022-01-01    PRCP      0  None  None     a     None\n",
       "3  ASN00037105  2022-01-01    PRCP      0  None  None     a     None\n",
       "4  ASN00037106  2022-01-01    PRCP      0  None  None     a     None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "dailyPath = f\"{prefixDaily}2022.csv.gz\"\n",
    "dailySchema = StructType([\n",
    "    StructField(\"ID\",       StringType(),  False), # Station ID\n",
    "    StructField(\"DATE\",     StringType(),  False), # YYYYMMDD\n",
    "    StructField(\"ELEMENT\",  StringType(),  True),  # Measurement type (e.g., TMAX)\n",
    "    StructField(\"VALUE\",    IntegerType(), True),  # Observation value\n",
    "    StructField(\"MFLAG\",    StringType(),  True),  # Measurement flag\n",
    "    StructField(\"QFLAG\",    StringType(),  True),  # Quality flag\n",
    "    StructField(\"SFLAG\",    StringType(),  True),  # Source flag\n",
    "    StructField(\"OBS_TIME\", StringType(),  True)   # Time of observation\n",
    "])\n",
    "\n",
    "\n",
    "dailydf   = spark.read.csv(dailyPath, schema=dailySchema, header=False)\n",
    "dailydf.printSchema()\n",
    "show_as_html(dailydf.limit(5))\n",
    "# Parse DATE \n",
    "dailydf = dailydf.withColumn(\"DATE\", F.to_date(F.col(\"DATE\"), \"yyyyMMdd\"))\n",
    "\n",
    "# Parse OBS_TIME \n",
    "dailydf = dailydf.withColumn(\"OBS_TIME\", \n",
    "    F.to_timestamp(F.concat(F.lit(\"1970-01-01 \"), F.col(\"OBS_TIME\")), \"yyyy-MM-dd HHmm\")) # fake full date\n",
    "dailydf = dailydf.withColumn(\"OBS_TIME\", \n",
    "    F.date_format(F.col(\"OBS_TIME\"), \"HH:mm\")) # just HH:mm\n",
    "\n",
    "# show the sample of data and print the schema\n",
    "dailydf.printSchema()\n",
    "show_as_html(dailydf.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c647bb-14f6-4057-acbc-475b137c15ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa174aa2-4767-4272-95d4-0ad00130a2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf0bfd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- GSNFLAG: string (nullable = true)\n",
      " |-- HCNFLAG: string (nullable = true)\n",
      " |-- WMOID: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSNFLAG</th>\n",
       "      <th>HCNFLAG</th>\n",
       "      <th>WMOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS COOLIDGE FLD</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011647</td>\n",
       "      <td>17.1333</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>19.0</td>\n",
       "      <td></td>\n",
       "      <td>ST JOHNS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>25.3330</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td></td>\n",
       "      <td>SHARJAH INTER. AIRP</td>\n",
       "      <td>GS</td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041194</td>\n",
       "      <td>25.2550</td>\n",
       "      <td>55.364</td>\n",
       "      <td>10.0</td>\n",
       "      <td></td>\n",
       "      <td>DUBAI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.4330</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.0</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0  ACW00011604   17.1167    -61.783       10.0         \n",
       "1  ACW00011647   17.1333    -61.783       19.0         \n",
       "2  AE000041196   25.3330     55.517       34.0         \n",
       "3  AEM00041194   25.2550     55.364       10.0         \n",
       "4  AEM00041217   24.4330     54.651       26.0         \n",
       "\n",
       "                             NAME GSNFLAG HCNFLAG  WMOID  \n",
       "0   ST JOHNS COOLIDGE FLD                                 \n",
       "1   ST JOHNS                                              \n",
       "2   SHARJAH INTER. AIRP                GS           4119  \n",
       "3   DUBAI INTL                                      4119  \n",
       "4   ABU DHABI INTL                                  4121  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Example: Load stations\n",
    "stationsPath = f\"{prefix}ghcnd-stations.txt\"\n",
    "stationsdf = spark.read.text(stationsPath)\n",
    "\n",
    "# Parse columns by fixed positions\n",
    "stationsdf = stationsdf.select(\n",
    "    substring(\"value\", 1, 11).alias(\"ID\"),\n",
    "    substring(\"value\", 13, 8).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    substring(\"value\", 22, 8).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    substring(\"value\", 31, 6).cast(\"double\").alias(\"ELEVATION\"),\n",
    "    substring(\"value\", 38, 2).alias(\"STATE\"),\n",
    "    substring(\"value\", 41, 30).alias(\"NAME\"),\n",
    "    substring(\"value\", 72, 3).alias(\"GSNFLAG\"),\n",
    "    substring(\"value\", 76, 3).alias(\"HCNFLAG\"),\n",
    "    substring(\"value\", 80, 5).alias(\"WMOID\")\n",
    ")\n",
    "stationsdf.printSchema()\n",
    "show_as_html(stationsdf.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ae17e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34036a7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- VALUE: integer (nullable = true)\n",
      " |-- MFLAG: string (nullable = true)\n",
      " |-- QFLAG: string (nullable = true)\n",
      " |-- SFLAG: string (nullable = true)\n",
      " |-- OBS_TIME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MFLAG</th>\n",
       "      <th>QFLAG</th>\n",
       "      <th>SFLAG</th>\n",
       "      <th>OBS_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00023000</td>\n",
       "      <td>18400101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SWE00139148</td>\n",
       "      <td>18400101</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>-97</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SWE00139148</td>\n",
       "      <td>18400101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EZE00100082</td>\n",
       "      <td>18400101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EZE00100082</td>\n",
       "      <td>18400101</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>-22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      DATE ELEMENT  VALUE MFLAG QFLAG SFLAG OBS_TIME\n",
       "0  ASN00023000  18400101    PRCP      0  None  None     a     None\n",
       "1  SWE00139148  18400101    TMIN    -97  None  None     E     None\n",
       "2  SWE00139148  18400101    PRCP      0  None  None     E     None\n",
       "3  EZE00100082  18400101    TMAX     -4  None  None     E     None\n",
       "4  EZE00100082  18400101    TMIN    -22  None  None     E     None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a small subset of daily (e.g. 2022)\n",
    "dailyPath = f\"{prefixDaily}1840.csv.gz\"\n",
    "dailydf   = spark.read.csv(dailyPath, schema=dailySchema, header=False)\n",
    "dailydf.printSchema()\n",
    "show_as_html(dailydf.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd62cc13-023e-4242-9603-e6878496829a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AC</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AG</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AJ</td>\n",
       "      <td>Azerbaijan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CODE                   NAME\n",
       "0   AC   Antigua and Barbuda \n",
       "1   AE  United Arab Emirates \n",
       "2   AF            Afghanistan\n",
       "3   AG               Algeria \n",
       "4   AJ            Azerbaijan "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Example: Load countries\n",
    "pathCountries = f\"{prefix}ghcnd-countries.txt\"\n",
    "rawCountries  = spark.read.text(pathCountries)\n",
    "\n",
    "# Parse columns by fixed positions\n",
    "countriesdf = rawCountries.select(\n",
    "    substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "    substring(\"value\", 4, 50).alias(\"NAME\")\n",
    ")\n",
    "show_as_html(countriesdf.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb441fa9-ac16-4326-a431-f6414f2eaeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AB</td>\n",
       "      <td>ALBERTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AS</td>\n",
       "      <td>AMERICAN SAMOA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CODE                                             NAME\n",
       "0   AB                                          ALBERTA\n",
       "1   AK                                           ALASKA\n",
       "2   AL  ALABAMA                                        \n",
       "3   AR                                         ARKANSAS\n",
       "4   AS                                   AMERICAN SAMOA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Example: Load states\n",
    "pathStates = f\"{prefix}ghcnd-states.txt\"\n",
    "rawStates  = spark.read.text(pathStates)\n",
    "\n",
    "# Parse columns by fixed positions\n",
    "statesdf = rawStates.select(\n",
    "    substring(\"value\", 1, 2).alias(\"CODE\"),\n",
    "    substring(\"value\", 4, 50).alias(\"NAME\")\n",
    ")\n",
    "show_as_html(statesdf.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27b50e6f-d443-4db6-bd74-c3c14b2fef7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>FIRSTYEAR</th>\n",
       "      <th>LASTYEAR</th>\n",
       "      <th>ELEMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACW00011604</td>\n",
       "      <td>17.1167</td>\n",
       "      <td>-61.783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  FIRSTYEAR  LASTYEAR ELEMENT\n",
       "0  ACW00011604   17.1167    -61.783        NaN       194     194\n",
       "1  ACW00011604   17.1167    -61.783        NaN       194     194\n",
       "2  ACW00011604   17.1167    -61.783        NaN       194     194\n",
       "3  ACW00011604   17.1167    -61.783        NaN       194     194\n",
       "4  ACW00011604   17.1167    -61.783        NaN       194     194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Example: Load inventory\n",
    "pathInventory = f\"{prefix}ghcnd-inventory.txt\"\n",
    "rawInventory  = spark.read.text(pathInventory)\n",
    "\n",
    "# Parse columns by fixed positions\n",
    "inventorydf = rawInventory.select(\n",
    "    substring(\"value\", 1, 11).alias(\"ID\"),\n",
    "    substring(\"value\", 13, 8).cast(\"double\").alias(\"LATITUDE\"),\n",
    "    substring(\"value\", 22, 8).cast(\"double\").alias(\"LONGITUDE\"),\n",
    "    substring(\"value\", 31, 4).cast(\"int\").alias(\"FIRSTYEAR\"),\n",
    "    substring(\"value\", 36, 4).cast(\"int\").alias(\"LASTYEAR\"),\n",
    "    substring(\"value\", 41, 4).alias(\"ELEMENT\")\n",
    ")\n",
    "show_as_html(inventorydf.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf473e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "stations: 129657 rows\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "states: 74 rows\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "countries: 219 rows\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "inventory: 765615 rows\n"
     ]
    }
   ],
   "source": [
    "# Load and count each\n",
    "paths = {\n",
    "    \"stations\":  f\"{prefix}ghcnd-stations.txt\",\n",
    "    \"states\":    f\"{prefix}ghcnd-states.txt\",\n",
    "    \"countries\": f\"{prefix}ghcnd-countries.txt\",\n",
    "    \"inventory\": f\"{prefix}ghcnd-inventory.txt\"\n",
    "}\n",
    "\n",
    "for name, path in paths.items():\n",
    "    df = spark.read.text(path)\n",
    "    df.printSchema()\n",
    "    print(f\"{name}: {df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24583df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb5737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
