{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "017705b0-281d-4565-abe1-852ebc5ca888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    " \n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e2f6d-f8f0-4150-8eec-e5bb2b0471e4",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06343f5-a638-4f7f-ade5-5f564a068bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/03/29 19:26:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4041\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743229568824</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-2eeb8d95e0944ac7</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-f313a277fd0c425385771c0770ea99f6</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743229569000</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00a161b-0750-4616-8ecd-7b908e37aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f631901-c8d1-4458-97c0-fc6f4bed1fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remotepath: wasbs://campus-data@madsstorage002.blob.core.windows.net/\n",
      "remotedaily: wasbs://campus-data@madsstorage002.blob.core.windows.net//ghcnd/\n",
      "2025-03-29 20:20:09,173 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-03-29 20:20:09,440 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-03-29 20:20:09,487 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-03-29 20:20:09,487 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "13.1 G   13.1 G   wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd\n",
      "1.9 K    1.9 K    wasbs://campus-data@madsstorage002.blob.core.windows.net/helloworld\n",
      "279.9 M  279.9 M  wasbs://campus-data@madsstorage002.blob.core.windows.net/crime\n",
      "3.7 M    3.7 M    wasbs://campus-data@madsstorage002.blob.core.windows.net/openflights\n",
      "12.9 G   12.9 G   wasbs://campus-data@madsstorage002.blob.core.windows.net/msd\n",
      "1.2 G    1.2 G    wasbs://campus-data@madsstorage002.blob.core.windows.net/ml\n",
      "143.8 M  143.8 M  wasbs://campus-data@madsstorage002.blob.core.windows.net/fraud\n",
      "2025-03-29 20:20:11,517 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n",
      "2025-03-29 20:20:11,517 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n",
      "2025-03-29 20:20:11,517 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n",
      "13.1 G   13.1 G   wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd\n",
      "1.9 K    1.9 K    wasbs://campus-data@madsstorage002.blob.core.windows.net/helloworld\n",
      "279.9 M  279.9 M  wasbs://campus-data@madsstorage002.blob.core.windows.net/crime\n",
      "3.7 M    3.7 M    wasbs://campus-data@madsstorage002.blob.core.windows.net/openflights\n",
      "12.9 G   12.9 G   wasbs://campus-data@madsstorage002.blob.core.windows.net/msd\n",
      "1.2 G    1.2 G    wasbs://campus-data@madsstorage002.blob.core.windows.net/ml\n",
      "143.8 M  143.8 M  wasbs://campus-data@madsstorage002.blob.core.windows.net/fraud\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "# http://localhost:4041/jobs/\n",
    "import subprocess\n",
    "\n",
    "remotepath = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/'\n",
    "print(f'remotepath: {remotepath}')\n",
    "remotedaily = f'{remotepath}/ghcnd/'\n",
    "print(f'remotedaily: {remotedaily}')\n",
    "!hdfs dfs -du -h {remotepath}             #file sizes\n",
    "\n",
    "\n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/\n",
    "#! HADOOP_ROOT_LOGGER=\"WARNING\" hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/daily/\n",
    "\n",
    "command = f'hdfs dfs -du -h {remotepath}'\n",
    "try:\n",
    "    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout.decode())  # Output the results of the command\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d0ebf38-760d-450e-be5f-850625050ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_relative_path: /ghcnd/daily/2025.csv.gz\n",
      "remotepath: wasbs://campus-data@madsstorage002.blob.core.windows.net/\n",
      "remote2025csv: wasbs://campus-data@madsstorage002.blob.core.windows.net//ghcnd/daily/2025.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Load a subset of the last year in daily into Spark from Azure Blob Storage using spark.read.csv\n",
    "# Define the input path for the last year in daily\n",
    "\n",
    "daily_relative_path = f'/ghcnd/daily/2025.csv.gz'\n",
    "print(f'daily_relative_path: {daily_relative_path}')\n",
    "print(f'remotepath: {remotepath}')\n",
    "remote2025csv = f'{remotepath}{daily_relative_path}'\n",
    "print(f'remote2025csv: {remote2025csv}')\n",
    "\n",
    "#daily_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{daily_relative_path}'\n",
    "#print(daily_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82aa580c-9e8d-4385-8c68-3084a886e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(type(weekly))\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dailydf.printSchema()\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n",
      "print(dailydf)\n",
      "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]\n",
      "dailydf.show(20, False):\n",
      "+-----------+--------+----+---+----+----+---+----+\n",
      "|_c0        |_c1     |_c2 |_c3|_c4 |_c5 |_c6|_c7 |\n",
      "+-----------+--------+----+---+----+----+---+----+\n",
      "|ASN00037106|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "|ASN00037115|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "|ASN00037118|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "|ASN00037120|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "|ASN00038010|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "|ASN00038026|20250101|TMAX|419|NULL|NULL|a  |NULL|\n",
      "|ASN00038026|20250101|TMIN|226|NULL|NULL|a  |NULL|\n",
      "|ASN00038026|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "|ASN00038026|20250101|TAVG|346|H   |NULL|S  |NULL|\n",
      "|ASN00004028|20250101|DAPR|2  |NULL|NULL|a  |NULL|\n",
      "|ASN00004028|20250101|DATN|2  |NULL|NULL|a  |NULL|\n",
      "|ASN00004028|20250101|MDPR|250|NULL|NULL|a  |NULL|\n",
      "|ASN00004028|20250101|MDTN|228|NULL|NULL|a  |NULL|\n",
      "|ASN00004032|20250101|TMAX|352|NULL|NULL|a  |NULL|\n",
      "|ASN00004032|20250101|TMIN|227|NULL|NULL|a  |NULL|\n",
      "|ASN00004032|20250101|TAVG|308|H   |NULL|S  |NULL|\n",
      "|ASN00004038|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "|ASN00004039|20250101|PRCP|240|NULL|NULL|a  |NULL|\n",
      "|ASN00044038|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "|ASN00044045|20250101|PRCP|0  |NULL|NULL|a  |NULL|\n",
      "+-----------+--------+----+---+----+----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#daily  = spark.read.csv(daily_path).limit(1000)\n",
    "dailydf = spark.read.csv(remote2025csv).limit(1000)\n",
    "\n",
    "#print(type(daily))\n",
    "print(\"print(type(weekly))\")\n",
    "print(type(dailydf))\n",
    "\n",
    "#daily.printSchema()\n",
    "print(\"dailydf.printSchema()\")\n",
    "dailydf.printSchema()\n",
    "\n",
    "#print(daily)\n",
    "print(\"print(dailydf)\")\n",
    "print(dailydf)\n",
    "\n",
    "print(\"dailydf.show(20, False):\")\n",
    "dailydf.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "731e5a3f-67e4-4ef9-a704-f4ed240302e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt\n",
      "remotestation: wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-stations.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the input path for stations\n",
    "\n",
    "stations_relative_path = f'ghcnd/ghcnd-stations.txt'\n",
    "stations_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{stations_relative_path}'\n",
    "print(stations_path)\n",
    "remotestation = f'{remotepath}{stations_relative_path}'\n",
    "print(f'remotestation: {remotestation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2b835f6-3780-4e52-a313-1b517b88c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "DataFrame[value: string]\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|value                                                                                |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       |\n",
      "|ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    |\n",
      "|AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196|\n",
      "|AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194|\n",
      "|AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217|\n",
      "|AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218|\n",
      "|AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930|\n",
      "|AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938|\n",
      "|AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948|\n",
      "|AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990|\n",
      "|AG000060390  36.7167    3.2500   24.0    ALGER-DAR EL BEIDA             GSN     60390|\n",
      "|AG000060590  30.5667    2.8667  397.0    EL-GOLEA                       GSN     60590|\n",
      "|AG000060611  28.0500    9.6331  561.0    IN-AMENAS                      GSN     60611|\n",
      "|AG000060680  22.8000    5.4331 1362.0    TAMANRASSET                    GSN     60680|\n",
      "|AGE00135039  35.7297    0.6500   50.0    ORAN-HOPITAL MILITAIRE                      |\n",
      "|AGE00147704  36.9700    7.7900  161.0    ANNABA-CAP DE GARDE                         |\n",
      "|AGE00147705  36.7800    3.0700   59.0    ALGIERS-VILLE/UNIVERSITE                    |\n",
      "|AGE00147706  36.8000    3.0300  344.0    ALGIERS-BOUZAREAH                           |\n",
      "|AGE00147707  36.8000    3.0400   38.0    ALGIERS-CAP CAXINE                          |\n",
      "|AGE00147708  36.7200    4.0500  222.0    TIZI OUZOU                             60395|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the stations metadata into Spark from Azure Blob Storage using spark.read.text without any other processing\n",
    "\n",
    "stations = spark.read.text(stations_path).limit(1000)\n",
    "print(type(stations))\n",
    "stations.printSchema()\n",
    "print(stations)\n",
    "stations.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d15503e4-3b5b-496d-84fe-c87786c32e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(stationdf):<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "stationdf.printSchema()\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "print(stationdf)\n",
      "DataFrame[value: string]\n",
      "stationdf.show(20, False)\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|value                                                                                |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       |\n",
      "|ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    |\n",
      "|AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196|\n",
      "|AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194|\n",
      "|AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217|\n",
      "|AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218|\n",
      "|AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930|\n",
      "|AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938|\n",
      "|AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948|\n",
      "|AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990|\n",
      "|AG000060390  36.7167    3.2500   24.0    ALGER-DAR EL BEIDA             GSN     60390|\n",
      "|AG000060590  30.5667    2.8667  397.0    EL-GOLEA                       GSN     60590|\n",
      "|AG000060611  28.0500    9.6331  561.0    IN-AMENAS                      GSN     60611|\n",
      "|AG000060680  22.8000    5.4331 1362.0    TAMANRASSET                    GSN     60680|\n",
      "|AGE00135039  35.7297    0.6500   50.0    ORAN-HOPITAL MILITAIRE                      |\n",
      "|AGE00147704  36.9700    7.7900  161.0    ANNABA-CAP DE GARDE                         |\n",
      "|AGE00147705  36.7800    3.0700   59.0    ALGIERS-VILLE/UNIVERSITE                    |\n",
      "|AGE00147706  36.8000    3.0300  344.0    ALGIERS-BOUZAREAH                           |\n",
      "|AGE00147707  36.8000    3.0400   38.0    ALGIERS-CAP CAXINE                          |\n",
      "|AGE00147708  36.7200    4.0500  222.0    TIZI OUZOU                             60395|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the stations metadata into Spark from Azure Blob Storage using spark.read.text without any other processing\n",
    "\n",
    "stationdf = spark.read.text(remotestation).limit(1000)\n",
    "print(f'type(stationdf):{type(stationdf)}')\n",
    "print(\"stationdf.printSchema()\")\n",
    "stationdf.printSchema()\n",
    "print(\"print(stationdf)\")\n",
    "print(stationdf)\n",
    "print(\"stationdf.show(20, False)\")\n",
    "stationdf.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "feed52b3-dcb6-492c-a589-f3a461f8dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations\n",
      "remotestations: wasbs://campus-data@madsstorage002.blob.core.windows.net/dew59/stations\n"
     ]
    }
   ],
   "source": [
    "# Define an output path as an exmaple\n",
    "\n",
    "output_relative_path = f'{username}/stations'\n",
    "output_path = f'wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{output_relative_path}'\n",
    "remotestations = f'{remotepath}{output_relative_path}'\n",
    "print(output_path)\n",
    "print(f'remotestations: {remotestations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b24beb8d-4488-4657-b0af-15f2468f92f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 20:49:54 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n",
      "25/03/29 20:49:55 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n"
     ]
    }
   ],
   "source": [
    "# Save the stations metadata to Azure Blob Storage from Spark\n",
    "\n",
    "stations.write.mode(\"overwrite\").text(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93c062b7-db91-4e7f-8435-b205ea54708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-26 16:58:51,456 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-03-26 16:58:51,725 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-03-26 16:58:51,770 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-03-26 16:58:51,770 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "Found 2 items\n",
      "-rw-r--r--   1 dew59 supergroup          0 2025-03-26 08:51 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations/_SUCCESS\n",
      "-rw-r--r--   1 dew59 supergroup      86000 2025-03-26 08:51 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations/part-00000-d05f6d0c-3adc-4399-ac77-2b0fb22e760b-c000.txt\n",
      "2025-03-26 16:58:52,146 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n",
      "2025-03-26 16:58:52,146 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n",
      "2025-03-26 16:58:52,146 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "\n",
    "!hdfs dfs -ls wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/stations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea2318c1-9ef3-4db9-a5cb-7d028114864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 16:59:45 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>dew59 (notebook)</code> is under the completed applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
