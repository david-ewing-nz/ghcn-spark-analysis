{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0458a4-22bc-4295-8f3e-c74f8dd31182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 0: Reset Kernel State\n",
    "# CELL 0: \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - clear all existing variables and definitions \n",
    "# - annoying to deebug without it\n",
    "\n",
    "#  \n",
    "%reset -f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017705b0-281d-4565-abe1-852ebc5ca888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e2f6d-f8f0-4150-8eec-e5bb2b0471e4",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46817cd2-d6d0-49b4-a97a-21881d5bb3e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CELL 1: my common definitions  \n",
    "# CELL 1: \n",
    "# ------------------------------------------------\n",
    "import traceback \n",
    "\n",
    "DEEBUG = True  #override at any point in the cells\n",
    "\n",
    "def dprintf(b: bool, s: str):\n",
    "    \"\"\"Conditional print of str\"\"\"\n",
    "    if b:\n",
    "        print(s)\n",
    "\n",
    "def hprintf(b: bool, df, n=5):\n",
    "    \"\"\"Conditional print of top rows in Spark DataFrame (safe for large datasets)\"\"\"\n",
    "    dprintf(1,\"hprintf start\")\n",
    "    if b:            \n",
    "        print(\"bool = 1\")\n",
    "        dprintf(1, f\"hprintf: show top {n} rows\")\n",
    "        df.printSchema()\n",
    "        #df.limit(n).show(truncate=False)        \n",
    "        count =  df.count()\n",
    "        dprintf(1, f\"result.count() = {count}\")\n",
    "        display(df.limit(n).toPandas())\n",
    "    else:\n",
    "        print(\"bool = 0\")\n",
    "    dprintf(1,\"hprintf end\")\n",
    "\n",
    "\n",
    "def dReadTEXT(b: bool, path: str, show: int = 5):\n",
    "    try:\n",
    "        \n",
    "        if b:\n",
    "            dprintf(1, f\"bool = 1, read file from: {path}\")\n",
    "            start = time.time()\n",
    "            result = spark.read.text(path)\n",
    "            stop = time.time()\n",
    "            hprintf(1, result, show)\n",
    "            dprintf(1, f\"completed in {stop - start:.2f} seconds\")\n",
    "            return result\n",
    "        else:\n",
    "            dprintf(1, f\"bool = 0, read NO file from: {path}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error caught:\", type(e), e)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "def dReadCSV(b: bool = 1, path: str = \"\", bHeader: bool = False, schema: StructType = None, show: int = 5):\n",
    "    try:\n",
    "  \n",
    "       \n",
    "        if schema:\n",
    "            dprintf(1, f\"schema: {schema.simpleString()}\")\n",
    "        else:\n",
    "            dprintf(1, \"no schema\")\n",
    "\n",
    "        if b:\n",
    "            dprintf(1, f\"bool = 1, read file from: {path}\")\n",
    "            start = time.time()\n",
    "            result = spark.read \\\n",
    "                .option(\"header\", bool(bHeader)) \\\n",
    "                .schema(schema) \\\n",
    "                .csv(path) \\\n",
    "                .repartition(8)  # leave this for now\n",
    "\n",
    "            stop = time.time()\n",
    "            dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "            hprintf(1, result, show)\n",
    "            count = result.count()\n",
    "            dprintf(1, f\"result.count() = {count}\")\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            dprintf(1, f\"bool = 1, read NO file from: {path}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error caught:\", type(e), e)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda12fa7-d929-468c-bccd-26118ef34429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: my common definitions  \n",
    "# CELL 1: \n",
    "# ------------------------------------------------\n",
    "import traceback\n",
    "import time\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "DEEBUG = True  # override at any point in the cells\n",
    "\n",
    "def dprintf(b: bool, s: str):\n",
    "    \"\"\"Conditional print of str\"\"\"\n",
    "    if b:\n",
    "        print(s)\n",
    "\n",
    "def hprintf(b: bool, df, n=5):\n",
    "    \"\"\"Conditional print of top rows in Spark DataFrame (safe for large datasets)\"\"\"\n",
    "    try:\n",
    "        dprintf(1, \"hprintf start\")\n",
    "        if not df:\n",
    "            raise ValueError(\"❌ DataFrame is None\")\n",
    "        if b:\n",
    "            dprintf(1, f\"hprintf: show top {n} rows\")\n",
    "            df.printSchema()\n",
    "            count = df.count()\n",
    "            dprintf(1, f\"result.count() = {count}\")\n",
    "            display(df.limit(n).toPandas())\n",
    "        else:\n",
    "            print(\"bool = 0\")\n",
    "        dprintf(1, \"hprintf end\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error caught in hprintf:\", type(e), e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def dReadTEXT(b: bool, path: str, show: int = 5):\n",
    "    try:\n",
    "        if not b:\n",
    "            dprintf(1, f\"bool = 0, read NO file from: {path}\")\n",
    "            return None\n",
    "\n",
    "        dprintf(1, f\"filename: {path}\")\n",
    "        start = time.time()\n",
    "        result = spark.read.text(path)\n",
    "        stop = time.time()\n",
    "\n",
    "        if result is None:\n",
    "            raise Exception(\"❌ spark.read.text() returned None\")\n",
    "\n",
    "        hprintf(1, result, show)\n",
    "        dprintf(1, f\"completed in {stop - start:.2f} seconds\")\n",
    "        dprintf(1, f\"result.count() = {result.count()}\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error caught:\", type(e), e)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def dReadCSV(b: bool = 1, path: str = \"\", bHeader: bool = False, schema: StructType = None, show: int = 5):\n",
    "    try:\n",
    "        dprintf(1, f\"path: {path}\")\n",
    "\n",
    "        if schema:\n",
    "            dprintf(1, f\"schema: {schema.simpleString()}\")\n",
    "        else:\n",
    "            dprintf(1, \"no schema\")\n",
    "\n",
    "        if b:\n",
    "            start = time.time()\n",
    "            result = spark.read \\\n",
    "                .option(\"header\", bool(bHeader)) \\\n",
    "                .schema(schema) \\\n",
    "                .csv(path) \\\n",
    "                .repartition(8)  # leave this for now\n",
    "\n",
    "            stop = time.time()\n",
    "            hprintf(1, result, show)\n",
    "            dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "            count = result.count()\n",
    "            dprintf(1, f\"result.count() = {count}\")\n",
    "            return result\n",
    "        else:\n",
    "            dprintf(1, f\"bool = 0, read NO file from: {path}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error caught:\", type(e), e)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def dWritePQ(b: bool = 1, path: str = \"\", df: DataFrame = None, show: int = 5):\n",
    "    try:\n",
    "        dprintf(1, f\"dWritePQ: path = {path}\")\n",
    "        \n",
    "        if df is None:\n",
    "            raise ValueError(\"❌ DataFrame is None — cannot write to Parquet.\")\n",
    "\n",
    "        if b:\n",
    "            dprintf(1, \"Preview before write:\")\n",
    "            hprintf(1, df, show)\n",
    "\n",
    "            start = time.time()\n",
    "            df.write.mode(\"overwrite\").parquet(path)            \n",
    "            stop = time.time()\n",
    "\n",
    "            dprintf(1, f\"✅ Write complete in {stop - start:.2f} seconds\")\n",
    "            dprintf(1, f\"📦 Rows written: {df.count()}\")\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            dprintf(1, f\"⚠️  b = 0 — write to {path} skipped\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error caught during dWritePQ:\", type(e), e)\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06343f5-a638-4f7f-ade5-5f564a068bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "#start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)\n",
    "#this file is treated as a headerfile and included in otehr jbooks call start after the file is included"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
