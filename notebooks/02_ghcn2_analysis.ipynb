{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f8b8c7-fd01-4c25-9bc2-b223d3e0dbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/04/07 02:29:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4040\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-4d16a2960b81cc2c</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-d484385e0d32460d899e8473c028a037</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743949777106</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743949777231</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 0: Start Spark Session  \n",
    "# CELL 0: \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Start Spark session using shared notebook 00_ghcn2_setup.ipynb \n",
    "# - Use lightweight config: 2 executors, 1 core each, 1GB memory\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "%run ./00_ghcn2_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "\n",
    "start_spark(\n",
    "    executor_instances=2,\n",
    "    executor_cores=1,\n",
    "    worker_memory=1,\n",
    "    master_memory=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d72776",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578aa0",
   "metadata": {},
   "source": [
    "**Q1** First you will investigate the `daily`, `stations`, `states`, `countries`, and `inventory` data provided  in cloud storage in:\n",
    " `wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`  \n",
    "using the `hdfs` command.\n",
    "\n",
    "**(a)** How is the data structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a3b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql           import functions as F \n",
    "from pyspark.sql.types     import *\n",
    "from pyspark.sql.functions import col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d6483f-a00f-4e25-83b4-4cd4af8ab743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n",
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/\n",
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd//daily/\n",
      "DEEBUG = TRUE\n"
     ]
    }
   ],
   "source": [
    "aDaily         = f'/2025.csv.gz'\n",
    "prefix         = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/'\n",
    "prefixWrite    = f'wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/'\n",
    "#prefixWrite    = \"./\"\n",
    "prefixDaily    = f'{prefix}/daily/'\n",
    "dprintf(DEEBUG,prefix)\n",
    "dprintf(DEEBUG,prefixWrite)\n",
    "dprintf(DEEBUG,prefixDaily)\n",
    "dprintf(DEEBUG,\"DEEBUG = TRUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6af364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59//stations_enriched.parquet\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations_enriched.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# read enriched stations metadata from user parquet\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m stationsEnricheddf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpqtEnrichedStations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m stop \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     22\u001b[0m dprintf(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstop\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/stations_enriched.parquet."
     ]
    }
   ],
   "source": [
    "# CELL 1 : Read Metadata for Analysis\n",
    "# CELL 1 \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - write  stationsEnricheddf from processing metadata previously developed\n",
    "# - build  local stationsEnricheddf from Parquet file \n",
    "\n",
    "#  \n",
    "\n",
    "pqtEnrichedStations = f\"{prefixWrite}/stations_enriched.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"Read from : {pqtEnrichedStations}\")\n",
    "\n",
    "if not 0:    \n",
    "    start = time.time()\n",
    "\n",
    "    # read enriched stations metadata from user parquet\n",
    "    stationsEnricheddf = spark.read.parquet(pqtEnrichedStations)\n",
    "\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"stationsEnricheddf.count() = {stationsEnricheddf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no read from : {pqtEnrichedStations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b229c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 2 Q1(a.1): – Total number of stations\n",
    "# CELL 2 Q1(a.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - count distinct station IDs in stationsEnricheddf\n",
    "# - result answers Q1(a.1) in Analysis section\n",
    "\n",
    "#  \n",
    "\n",
    "nTotalStations = stationsEnricheddf.select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Total Stations: {nTotalStations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03294c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 Q1(a.2): – Stations active in 2025\n",
    "# CELL 3 Q1(a.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter stationsEnricheddf where LASTYEAR >= 2025\n",
    "# - count distinct station IDs in this subset\n",
    "\n",
    "#  \n",
    "\n",
    "nStations2025 = stationsEnricheddf.filter(\"LASTYEAR >= 2025\").select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations active in 2025: {nStations2025}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab79446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 Q1(a.3): – Stations in GSN, HCN, and CRN\n",
    "# CELL 4 Q1(a.3): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter stationsEnricheddf where each network flag is not null\n",
    "# - count distinct station IDs for each of GSN, HCN, and CRN\n",
    "\n",
    "#  \n",
    "\n",
    "nGSN = stationsEnricheddf.filter(\"GSN_FLAG IS NOT NULL AND GSN_FLAG != ''\").select(\"ID\").distinct().count()\n",
    "nHCN = stationsEnricheddf.filter(\"HCN_CRN_FLAG == 'HCN'\").select(\"ID\").distinct().count()\n",
    "nCRN = stationsEnricheddf.filter(\"HCN_CRN_FLAG == 'CRN'\").select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations in GSN: {nGSN}\")\n",
    "dprintf(1, f\"Stations in HCN: {nHCN}\")\n",
    "dprintf(1, f\"Stations in CRN: {nCRN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3510f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 Q1(a.4): – Stations in more than one network\n",
    "# CELL 5 Q1(a.4): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - define boolean flags for membership in GSN, HCN, CRN\n",
    "# - count rows where station belongs to two or more networks\n",
    "\n",
    "#  \n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "stationsFlaggeddf = stationsEnricheddf.withColumn(\"inGSN\", when((col(\"GSN_FLAG\").isNotNull()) & (col(\"GSN_FLAG\") != \"\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"inHCN\", when(col(\"HCN_CRN_FLAG\") == \"HCN\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"inCRN\", when(col(\"HCN_CRN_FLAG\") == \"CRN\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"nMemberships\", col(\"inGSN\") + col(\"inHCN\") + col(\"inCRN\"))\n",
    "\n",
    "nMultiNetwork = stationsFlaggeddf.filter(\"nMemberships >= 2\").select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations in more than one network: {nMultiNetwork}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 Q1(b.1): – Stations in the Southern Hemisphere\n",
    "# CELL 6 Q1(b.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter stationsEnricheddf where LATITUDE < 0\n",
    "# - count distinct station IDs\n",
    "\n",
    "#  \n",
    "\n",
    "nSouthernHemisphere = stationsEnricheddf.filter(\"LATITUDE < 0\").select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations in Southern Hemisphere: {nSouthernHemisphere}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974763d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 Q1(b.2): – Stations in US Territories (excluding United States)\n",
    "# CELL 7 Q1(b.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter by COUNTRY_NAME containing \"United States\" but not equal to \"United States\"\n",
    "# - count distinct station IDs\n",
    "\n",
    "#  \n",
    "\n",
    "territorydf = stationsEnricheddf.filter(\n",
    "    (col(\"COUNTRY_NAME\").like(\"%United States%\")) & (col(\"COUNTRY_NAME\") != \"United States\")\n",
    ")\n",
    "\n",
    "nUSTerritories = territorydf.select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations in US Territories (excluding United States): {nUSTerritories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ad40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 Q1(b.2): – Stations in US Territories (excluding United States)\n",
    "# CELL 7 Q1(b.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - read countries.txt into countriesdf with fixed-width logic\n",
    "# - join with stationsEnricheddf to build stationsCountriesdf\n",
    "# - count distinct stations where COUNTRY_NAME includes \"United States\" but is not \"United States\"\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q1(b.2): Stations in US Territories (excluding United States)\")\n",
    "\n",
    "# read and trim countries.txt\n",
    "countriesdf = spark.read.text(f\"{prefix}/ghcnd-countries.txt\") \\\n",
    "    .withColumn(\"CODE\", substring(\"value\", 1, 2)) \\\n",
    "    .withColumn(\"COUNTRY_NAME\", trim(substring(\"value\", 4, 61))) \\\n",
    "    .select(\"CODE\", \"COUNTRY_NAME\")\n",
    "\n",
    "dprintf(2, \"countriesdf created\")\n",
    "\n",
    "# join with enriched stations metadata\n",
    "stationsCountriesdf = stationsEnricheddf.join(\n",
    "    countriesdf,\n",
    "    stationsEnricheddf.COUNTRY == countriesdf.CODE,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dprintf(2, \"stationsCountriesdf joined\")\n",
    "\n",
    "# filter US territories (contains 'United States' but not exactly equal)\n",
    "territoryStationsdf = stationsCountriesdf.filter(\n",
    "    (col(\"COUNTRY_NAME\").like(\"%United States%\")) & (col(\"COUNTRY_NAME\") != \"United States\")\n",
    ")\n",
    "\n",
    "nUSTerritories = territoryStationsdf.select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations in US Territories (excluding United States): {nUSTerritories}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: stationsCountriesdf to Parquet\n",
    "\n",
    "pqtStationsCountries = f\"{prefixWrite}/stations_countries.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtStationsCountries}\")\n",
    "\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    stationsCountriesdf.write.mode(\"overwrite\").parquet(pqtStationsCountries)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"stationsCountriesdf = {stationsCountriesdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtStationsCountries}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d706b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 Q1(c.1): – Station counts per country\n",
    "# CELL 8 Q1(c.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - group stationsCountriesdf by COUNTRY\n",
    "# - join with countriesdf to attach COUNTRY_NAME\n",
    "# - output result as countryStationCountsdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q1(c.1): Station counts per country\")\n",
    "\n",
    "# group by country code and count distinct station IDs\n",
    "countryCountsdf = stationsCountriesdf.groupBy(\"COUNTRY\").agg(countDistinct(\"ID\").alias(\"nStations\"))\n",
    "\n",
    "# join with countriesdf to get readable country names\n",
    "countryStationCountsdf = countryCountsdf.join(\n",
    "    countriesdf,\n",
    "    countryCountsdf.COUNTRY == countriesdf.CODE,\n",
    "    how=\"left\"\n",
    ").select(\"CODE\", \"COUNTRY_NAME\", \"nStations\").orderBy(\"nStations\", ascending=False)\n",
    "\n",
    "dprintf(1, f\"Total countries with stations: {countryStationCountsdf.count()}\")\n",
    "countryStationCountsdf.show(10, truncate=False)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: countryStationCountsdf to Parquet\n",
    "\n",
    "pqtCountryCounts = f\"{prefixWrite}/country_station_counts.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtCountryCounts}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    countryStationCountsdf.write.mode(\"overwrite\").parquet(pqtCountryCounts)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"countryStationCountsdf.count() = {countryStationCountsdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtCountryCounts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 Q1(c.2): – Station counts per US state\n",
    "# CELL 9 Q1(c.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - read ghcnd-states.txt into statesdf\n",
    "# - join with stationsEnricheddf on STATE\n",
    "# - group by state code and count distinct station IDs\n",
    "# - output result as stateStationCountsdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q1(c.2): Station counts per US state\")\n",
    "\n",
    "# read and trim states.txt\n",
    "statesdf = spark.read.text(f\"{prefix}/ghcnd-states.txt\") \\\n",
    "    .withColumn(\"CODE\", substring(\"value\", 1, 2)) \\\n",
    "    .withColumn(\"STATE_NAME\", trim(substring(\"value\", 4, 47))) \\\n",
    "    .select(\"CODE\", \"STATE_NAME\")\n",
    "\n",
    "dprintf(2, \"statesdf created\")\n",
    "\n",
    "# join with stationsEnricheddf\n",
    "stateCountsdf = stationsEnricheddf.join(\n",
    "    statesdf,\n",
    "    stationsEnricheddf.STATE == statesdf.CODE,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# group by state and count distinct station IDs\n",
    "stateStationCountsdf = stateCountsdf.groupBy(\"STATE\", \"STATE_NAME\") \\\n",
    "    .agg(countDistinct(\"ID\").alias(\"nStations\")) \\\n",
    "    .orderBy(\"nStations\", ascending=False)\n",
    "\n",
    "dprintf(1, f\"Total US states with stations: {stateStationCountsdf.count()}\")\n",
    "stateStationCountsdf.show(10, truncate=False)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: stateStationCountsdf to Parquet\n",
    "\n",
    "pqtStateCounts = f\"{prefixWrite}/state_station_counts.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtStateCounts}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    stateStationCountsdf.write.mode(\"overwrite\").parquet(pqtStateCounts)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"stateStationCountsdf.count() = {stateStationCountsdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtStateCounts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10 Q2(a.1): – Define geographic distance function\n",
    "# CELL 10 Q2(a.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - define haversine distance function to compute great-circle distance\n",
    "# - register as Spark UDF: geoDistanceUDF(lat1, lon1, lat2, lon2)\n",
    "# - result in kilometres\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q2(a.1): Define geographic distance function\")\n",
    "\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def geoDistance(lat1, lon1, lat2, lon2):\n",
    "    # convert to radians\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    # earth radius in km\n",
    "    R = 6371.0\n",
    "    return R * c\n",
    "\n",
    "# register UDF\n",
    "geoDistanceUDF = udf(geoDistance, DoubleType())\n",
    "\n",
    "dprintf(1, \"geoDistanceUDF registered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab48ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11 Q2(a.2): – Test geoDistanceUDF on sample stations\n",
    "# CELL 11 Q2(a.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - take 3 sample stations from stationsEnricheddf\n",
    "# - perform CROSS JOIN to compute pairwise distances\n",
    "# - apply geoDistanceUDF\n",
    "# - exclude self-pairs (ID1 == ID2)\n",
    "# - output result as stationPairsdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q2(a.2): Test geoDistanceUDF on CROSS JOIN\")\n",
    "\n",
    "# take 3 sample stations\n",
    "sampleStationsdf = stationsEnricheddf.select(\"ID\", \"LATITUDE\", \"LONGITUDE\").distinct().limit(3)\n",
    "\n",
    "# create aliases for self-join\n",
    "left = sampleStationsdf.alias(\"left\")\n",
    "right = sampleStationsdf.alias(\"right\")\n",
    "\n",
    "# perform CROSS JOIN\n",
    "stationPairsdf = left.crossJoin(right) \\\n",
    "    .filter(col(\"left.ID\") != col(\"right.ID\")) \\\n",
    "    .withColumn(\n",
    "        \"DistanceKM\",\n",
    "        geoDistanceUDF(\n",
    "            col(\"left.LATITUDE\"), col(\"left.LONGITUDE\"),\n",
    "            col(\"right.LATITUDE\"), col(\"right.LONGITUDE\")\n",
    "        )\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"left.ID\").alias(\"ID1\"),\n",
    "        col(\"right.ID\").alias(\"ID2\"),\n",
    "        col(\"DistanceKM\")\n",
    "    )\n",
    "\n",
    "dprintf(1, f\"Pairwise distances computed: {stationPairsdf.count()} rows\")\n",
    "stationPairsdf.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181df76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12 Q2(b.1): – Pairwise distances between NZ stations\n",
    "# CELL 12 Q2(b.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter stationsCountriesdf to COUNTRY == 'NZ'\n",
    "# - CROSS JOIN to compute distances between all station pairs\n",
    "# - apply geoDistanceUDF and exclude self-pairs\n",
    "# - output as nzStationDistancesdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q2(b.1): Pairwise distances between stations in New Zealand\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# filter NZ stations\n",
    "nzStationsdf = stationsCountriesdf.filter(\"COUNTRY == 'NZ'\") \\\n",
    "    .select(\"ID\", \"NAME\", \"LATITUDE\", \"LONGITUDE\").distinct()\n",
    "\n",
    "dprintf(2, f\"nzStationsdf count = {nzStationsdf.count()}\")\n",
    "\n",
    "# alias for cross join\n",
    "left = nzStationsdf.alias(\"left\")\n",
    "right = nzStationsdf.alias(\"right\")\n",
    "\n",
    "# compute pairwise distances\n",
    "nzStationDistancesdf = left.crossJoin(right) \\\n",
    "    .filter(col(\"left.ID\") != col(\"right.ID\")) \\\n",
    "    .withColumn(\n",
    "        \"DistanceKM\",\n",
    "        geoDistanceUDF(\n",
    "            col(\"left.LATITUDE\"), col(\"left.LONGITUDE\"),\n",
    "            col(\"right.LATITUDE\"), col(\"right.LONGITUDE\")\n",
    "        )\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"left.ID\").alias(\"ID1\"),\n",
    "        col(\"left.NAME\").alias(\"Name1\"),\n",
    "        col(\"right.ID\").alias(\"ID2\"),\n",
    "        col(\"right.NAME\").alias(\"Name2\"),\n",
    "        col(\"DistanceKM\")\n",
    "    )\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "dprintf(1, f\"NZ station pairs: {nzStationDistancesdf.count()}\")\n",
    "dprintf(1, f\"Q2(b.1) complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: nzStationDistancesdf to Parquet\n",
    "\n",
    "pqtNZDistances = f\"{prefixWrite}/nz_station_distances.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtNZDistances}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    nzStationDistancesdf.write.mode(\"overwrite\").parquet(pqtNZDistances)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"nzStationDistancesdf.count() = {nzStationDistancesdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtNZDistances}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cf41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13 Q2(b.2): – Closest pair of NZ stations\n",
    "# CELL 13 Q2(b.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - sort nzStationDistancesdf by DistanceKM\n",
    "# - retrieve top 1 closest pair\n",
    "# - output station names, IDs, and distance\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q2(b.2): Closest pair of stations in New Zealand\")\n",
    "\n",
    "closestPair = nzStationDistancesdf.orderBy(\"DistanceKM\").limit(1)\n",
    "\n",
    "row = closestPair.collect()[0]\n",
    "dprintf(1, f\"Closest pair: {row['Name1']} ({row['ID1']}) ↔ {row['Name2']} ({row['ID2']})\")\n",
    "dprintf(1, f\"Distance = {row['DistanceKM']:.2f} km\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aeb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14A: – Load full daily dataset from Azure\n",
    "# CELL 14A: \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - read daily data from prefixDaily path\n",
    "# - apply dailySchema as used in 01_explore\n",
    "# - output DataFrame: dailydf\n",
    "\n",
    "#  \n",
    "dailySchema = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"DATE\", DateType(), True),\n",
    "    StructField(\"ELEMENT\", StringType(), True),\n",
    "    StructField(\"VALUE\", FloatType(), True),\n",
    "    StructField(\"MFLAG\", StringType(), True),\n",
    "    StructField(\"QFLAG\", StringType(), True),\n",
    "    StructField(\"SFLAG\", StringType(), True),\n",
    "    StructField(\"OBSTIME\", StringType(), True)\n",
    "])\n",
    "\n",
    "hprintf(\"Q3(a.1): Load full daily dataset\")\n",
    "\n",
    "# path to daily .csv files in blob storage\n",
    "dailyPath = f\"{prefixDaily}/*\"\n",
    "\n",
    "dprintf(2, f\"Reading daily data from: {dailyPath}\")\n",
    "\n",
    "# read data with schema (headerless CSV)\n",
    "dailydf = spark.read.csv(dailyPath, schema=dailySchema, header=False)\n",
    "\n",
    "dprintf(1, f\"dailydf.count() = {dailydf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 15 Q3(b.1): – Core element observation counts\n",
    "# CELL 15 Q3(b.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter dailydf for core elements: PRCP, SNOW, SNWD, TMAX, TMIN\n",
    "# - group by ELEMENT and count rows\n",
    "# - output as coreElementCountsdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q3(b.1): Count of observations for core elements\")\n",
    "\n",
    "coreElements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "coreElementCountsdf = dailydf.filter(col(\"ELEMENT\").isin(coreElements)) \\\n",
    "    .groupBy(\"ELEMENT\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "dprintf(DEEBUG, \"Core element counts:\")\n",
    "hprintf(DEEBUG, coreElementCountsdf.show(5))\n",
    "dprintf(1, f\"coreElementCountsdf.count() = {coreElementCountsdf.count()}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: coreElementCountsdf to Parquet\n",
    "\n",
    "pqtCoreCounts = f\"{prefixWrite}/core_element_counts.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtCoreCounts}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    coreElementCountsdf.write.mode(\"overwrite\").parquet(pqtCoreCounts)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"coreElementCountsdf.count() = {coreElementCountsdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtCoreCounts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 16 Q3(b.2): – Core element with most observations\n",
    "# CELL 16 Q3(b.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - use coreElementCountsdf from CELL 15\n",
    "# - display preview and row count\n",
    "# - extract top row using .orderBy().limit().collect()\n",
    "\n",
    "#  \n",
    "\n",
    "topCoreElement = coreElementCountsdf.orderBy(col(\"count\").desc()).limit(1).collect()[0]\n",
    "\n",
    "dprintf(DEEBUG, f\"topCoreElement = {topCoreElement}\")\n",
    "hprintf(DEEBUG, coreElementCountsdf)\n",
    "dprintf(1, f\"Most observed element: {topCoreElement['ELEMENT']} ({topCoreElement['count']:,} observations)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 17 Q3(c.1): – TMAX without corresponding TMIN\n",
    "# CELL 17 Q3(c.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter dailydf for TMAX and TMIN\n",
    "# - extract (ID, DATE) pairs for each\n",
    "# - anti-join to find TMAX records not matched to TMIN\n",
    "# - display and count results\n",
    "\n",
    "#  \n",
    "\n",
    "dprintf(DEEBUG, \"Q3(c.1): TMAX observations without corresponding TMIN\")\n",
    "\n",
    "tmaxdf = dailydf.filter(col(\"ELEMENT\") == \"TMAX\").select(\"ID\", \"DATE\")\n",
    "tmindf = dailydf.filter(col(\"ELEMENT\") == \"TMIN\").select(\"ID\", \"DATE\")\n",
    "\n",
    "# anti-join: TMAX records that don't match TMIN by ID and DATE\n",
    "tmaxNoTmindf = tmaxdf.join(tmindf, on=[\"ID\", \"DATE\"], how=\"left_anti\")\n",
    "\n",
    "dprintf(DEEBUG, f\"tmaxNoTmindf.count() = {tmaxNoTmindf.count()}\")\n",
    "hprintf(DEEBUG, tmaxNoTmindf)\n",
    "dprintf(1, f\"TMAX observations without matching TMIN = {tmaxNoTmindf.count()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
