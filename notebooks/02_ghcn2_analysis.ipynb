{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f8b8c7-fd01-4c25-9bc2-b223d3e0dbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/04/07 16:39:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4049\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1744000785186</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-ce69e86f44194b659d75068998c3ba2a</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1744000785356</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-e386dc960e8c1ecc</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 0: Start Spark Session  \n",
    "# CELL 0: \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - Start Spark session using shared notebook 00_ghcn2_setup.ipynb \n",
    "# - Use config: 2 executors, 1 core each, 1GB memory\n",
    "#  \n",
    "\n",
    "#  \n",
    "#  \n",
    "%run ./00_ghcn2_setup.ipynb\n",
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "\n",
    "start_spark(\n",
    "    executor_instances=2,\n",
    "    executor_cores=1,\n",
    "    worker_memory=1,\n",
    "    master_memory=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d72776",
   "metadata": {},
   "source": [
    "### Assignment 1 ###\n",
    "\n",
    "The code below demonstrates how to explore and load the data provided for the assignment from Azure Blob Storage and how to save any outputs that you generate to a separate user container.\n",
    "\n",
    "**Key points**\n",
    "\n",
    "- The data provided for the assignment is stored in Azure Blob Storage and outputs that you generate will be stored in Azure Blob Storage as well. Hadoop and Spark can both interact with Azure Blob Storage similar to how they interact with HDFS, but where the replication and distribution is handled by Azure instead. This makes it possible to read or write data in Azure over HTTPS where the path is prefixed by `wasbs://`.\n",
    "- There are two containers, one for the data which is read only and one for any outputs that you generate,\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/`\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n",
    "- You can use variable interpolation to insert your global username variable into paths automatically.\n",
    "  - This works for bash commands as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90578aa0",
   "metadata": {},
   "source": [
    "**Q1** First you will investigate the `daily`, `stations`, `states`, `countries`, and `inventory` data provided  in cloud storage in:\n",
    " `wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/`  \n",
    "using the `hdfs` command.\n",
    "\n",
    "**(a)** How is the data structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a3b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql           import functions as F \n",
    "from pyspark.sql.types     import *\n",
    "from pyspark.sql.functions import col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477a0f9-d234-438a-a7e8-d2a73c32e98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbdfd821-cfe1-4f67-b431-2ca249f36ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/\n",
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/\n",
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/\n",
      "DEEBUG = {DEEBUG}\n",
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/2025.csv.gz\n",
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-countries.txt\n",
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-inventory.txt\n",
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/ghcnd-states.txt\n",
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/COUNTRY-metadata.parquet\n",
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/INVENTORY-metadata.parquet\n",
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/STATE-metadata.parquet\n",
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/CLASSIFY-metadata.parquet\n",
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/PIVOT-metadata.parquet\n"
     ]
    }
   ],
   "source": [
    "aDaily         = f'/2025.csv.gz'\n",
    "prefix         = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/ghcnd/'\n",
    "prefixData     = f'wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/'\n",
    "#prefixData    = \"./\"\n",
    "prefixDaily    = f'{prefix}daily/'\n",
    "dprintf(DEEBUG,prefix)\n",
    "dprintf(DEEBUG,prefixData)\n",
    "dprintf(DEEBUG,prefixDaily)\n",
    "dprintf(DEEBUG,\"DEEBUG = {DEEBUG}\")\n",
    "\n",
    "# 2023.csv.gz     174726829\n",
    "# 2024.csv.gz     168485088\n",
    "# 2025.csv.gz     17061071 \n",
    "# drwxrwxrwx   daily\n",
    "# -rwxrwxrwx   ghcnd-countries.txt\n",
    "# -rwxrwxrwx   ghcnd-inventory.txt\n",
    "# -rwxrwxrwx   ghcnd-states.txt\n",
    "# -rwxrwxrwx   ghcnd-stations.txt\n",
    "\n",
    "\n",
    "CSVPathADaily       = f\"{prefixDaily}2025.csv.gz\"\n",
    "TXTPathRawCountries = f\"{prefix}ghcnd-countries.txt\"\n",
    "TXTPathRawInventory = f\"{prefix}ghcnd-inventory.txt\"\n",
    "TXTPathRawStates    = f\"{prefix}ghcnd-states.txt\"\n",
    "TXTPathRawStations  = f\"{prefix}ghcnd-stations.txt\"\n",
    "\n",
    "\n",
    "dprintf(DEEBUG,CSVPathADaily)\n",
    "dprintf(DEEBUG,TXTPathRawCountries)\n",
    "dprintf(DEEBUG,TXTPathRawInventory)\n",
    "dprintf(DEEBUG,TXTPathRawStates)\n",
    "\n",
    "\n",
    "PQPathCountriesdf          = f\"{prefixData}COUNTRY-metadata.parquet\"\n",
    "PQPathinventorydf          = f\"{prefixData}INVENTORY-metadata.parquet\"\n",
    "PQPathstatesdf             = f\"{prefixData}STATE-metadata.parquet\"\n",
    "PQPathstationsdf           = f\"{prefixData}STATIONS-metadata.parquet\"\n",
    "PQPathClassifiedStationsdf = f\"{prefixData}CLASSIFY-metadata.parquet\"\n",
    "PQPaththeMetadata      = f\"{prefixData}PIVOT-metadata.parquet\"\n",
    "\n",
    "\n",
    "dprintf(DEEBUG,PQPathCountriesdf)\n",
    "dprintf(DEEBUG,PQPathinventorydf)\n",
    "dprintf(DEEBUG,PQPathstatesdf)\n",
    "dprintf(DEEBUG,PQPathClassifiedStationsdf)\n",
    "dprintf(DEEBUG,PQPaththeMetadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6af364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theMetadata\n",
      "dReadPQ: path = wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/CLASSIFY-metadata.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Read complete in 3.98 seconds\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- STATIONNAME: string (nullable = true)\n",
      " |-- GSNFLAG: string (nullable = true)\n",
      " |-- HCNFLAG: string (nullable = true)\n",
      " |-- WMOID: string (nullable = true)\n",
      " |-- COUNTRYCODE: string (nullable = true)\n",
      " |-- COUNTRYNAME: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- FIRSTYEAR: integer (nullable = true)\n",
      " |-- LASTYEAR: integer (nullable = true)\n",
      " |-- STATENAME: string (nullable = true)\n",
      " |-- CLASSIFICATION: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 8221223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATIONNAME</th>\n",
       "      <th>GSNFLAG</th>\n",
       "      <th>HCNFLAG</th>\n",
       "      <th>WMOID</th>\n",
       "      <th>COUNTRYCODE</th>\n",
       "      <th>COUNTRYNAME</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>FIRSTYEAR</th>\n",
       "      <th>LASTYEAR</th>\n",
       "      <th>STATENAME</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.433</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.8</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>41217</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1983</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>PARTIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.433</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.8</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>41217</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1983</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>PARTIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.433</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.8</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>41217</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1983</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>PARTIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.433</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.8</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>41217</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1983</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>PARTIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEM00041217</td>\n",
       "      <td>24.433</td>\n",
       "      <td>54.651</td>\n",
       "      <td>26.8</td>\n",
       "      <td></td>\n",
       "      <td>ABU DHABI INTL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>41217</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>1983</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>PARTIAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  LATITUDE  LONGITUDE  ELEVATION STATE  \\\n",
       "0  AEM00041217    24.433     54.651       26.8         \n",
       "1  AEM00041217    24.433     54.651       26.8         \n",
       "2  AEM00041217    24.433     54.651       26.8         \n",
       "3  AEM00041217    24.433     54.651       26.8         \n",
       "4  AEM00041217    24.433     54.651       26.8         \n",
       "\n",
       "                      STATIONNAME GSNFLAG HCNFLAG  WMOID COUNTRYCODE  \\\n",
       "0  ABU DHABI INTL                                  41217          AE   \n",
       "1  ABU DHABI INTL                                  41217          AE   \n",
       "2  ABU DHABI INTL                                  41217          AE   \n",
       "3  ABU DHABI INTL                                  41217          AE   \n",
       "4  ABU DHABI INTL                                  41217          AE   \n",
       "\n",
       "             COUNTRYNAME ELEMENT  FIRSTYEAR  LASTYEAR STATENAME CLASSIFICATION  \n",
       "0  United Arab Emirates     TMAX       1983      2025      None        PARTIAL  \n",
       "1  United Arab Emirates     TMAX       1983      2025      None        PARTIAL  \n",
       "2  United Arab Emirates     TMAX       1983      2025      None        PARTIAL  \n",
       "3  United Arab Emirates     TMAX       1983      2025      None        PARTIAL  \n",
       "4  United Arab Emirates     TMIN       1983      2025      None        PARTIAL  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rows loaded: 8221223\n",
      "✅ Rows loaded: 8221223\n",
      "dReadPQ: path = wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/INVENTORY-metadata.parquet\n",
      "⚠️ b = 0: SKIPPED READ FROM: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/INVENTORY-metadata.parquet \n",
      "⚠️ b = 0: SKIPPED READ FROM: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/INVENTORY-metadata.parquet \n"
     ]
    }
   ],
   "source": [
    "# CELL 1 : Read Metadata for Analysis\n",
    "# CELL 1 \n",
    "# ------------------------------------------------\n",
    "\n",
    "# Summary:\n",
    "# - write  theMetadata from processing metadata previously developed\n",
    "# - build  local theMetadata from Parquet file \n",
    "\n",
    "#  \n",
    "\n",
    "dprintf(DEEBUG, \"theMetadata\")\n",
    "theMetadata = dReadPQ(1, PQPathClassifiedStationsdf)\n",
    "inventordf  = dReadPQ(0, PQPathinventorydf)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b229c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stations: 129657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# CELL 2 Q1(a.1): – Total number of stations\n",
    "# CELL 2 Q1(a.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - count distinct station IDs in theMetadata\n",
    "# - result answers Q1(a.1) in Analysis section\n",
    "\n",
    "#  \n",
    "\n",
    "nTotalStations = theMetadata.select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Total Stations: {nTotalStations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03294c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations active in 2025: 30735\n"
     ]
    }
   ],
   "source": [
    "# CELL 3 Q1(a.2): – Stations active in 2025\n",
    "# CELL 3 Q1(a.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter theMetadata where LASTYEAR >= 2025\n",
    "# - count distinct station IDs in this subset\n",
    "\n",
    "#  \n",
    "\n",
    "nStations2025 = theMetadata.filter(\"LASTYEAR >= 2025\").select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations active in 2025: {nStations2025}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab79446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations in GSN: 129656\n",
      "Stations in HCN: 1218\n",
      "Stations in CRN: 234\n"
     ]
    }
   ],
   "source": [
    "# CELL 4 Q1(a.3): – Stations in GSN, HCN, and CRN\n",
    "# CELL 4 Q1(a.3): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter theMetadata where each network flag is not null\n",
    "# - count distinct station IDs for each of GSN, HCN, and CRN\n",
    "\n",
    "#  \n",
    "\n",
    "\n",
    "\n",
    "nGSN = theMetadata.filter((col(\"GSNFLAG\").isNotNull()) & (col(\"GSNFLAG\") != \"\")).select(\"ID\").distinct().count()\n",
    "nHCN = theMetadata.filter(col(\"HCNFLAG\") == \"HCN\").select(\"ID\").distinct().count()\n",
    "nCRN = theMetadata.filter(col(\"HCNFLAG\") == \"CRN\").select(\"ID\").distinct().count()\n",
    "\n",
    "\n",
    "dprintf(1, f\"Stations in GSN: {nGSN}\")\n",
    "dprintf(1, f\"Stations in HCN: {nHCN}\")\n",
    "dprintf(1, f\"Stations in CRN: {nCRN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f3510f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations in more than one network: 1452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# CELL 5 Q1(a.4): – Stations in more than one network\n",
    "# CELL 5 Q1(a.4): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - define boolean flags for membership in GSN, HCN, CRN\n",
    "# - count rows where station belongs to two or more networks\n",
    "\n",
    "#  \n",
    "\n",
    "\n",
    "stationsFlagged = theMetadata \\\n",
    "    .withColumn(\"inGSN\", when((col(\"GSNFLAG\").isNotNull()) & (col(\"GSNFLAG\") != \"\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"inHCN\", when(col(\"HCNFLAG\") == \"HCN\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"inCRN\", when(col(\"HCNFLAG\") == \"CRN\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"nMemberships\", col(\"inGSN\") + col(\"inHCN\") + col(\"inCRN\"))\n",
    "\n",
    "nMultiNetwork = stationsFlagged \\\n",
    "    .filter(col(\"nMemberships\") >= 2) \\\n",
    "    .select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations in more than one network: {nMultiNetwork}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd7d60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations in Southern Hemisphere: 25357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# CELL 6 Q1(b.1): – Stations in the Southern Hemisphere\n",
    "# CELL 6 Q1(b.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter theMetadata where LATITUDE < 0\n",
    "# - count distinct station IDs\n",
    "\n",
    "#  \n",
    "\n",
    "nSouthernHemisphere = theMetadata.filter(\"LATITUDE < 0\").select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations in Southern Hemisphere: {nSouthernHemisphere}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "974763d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "territorydf created (US Territories excluding mainland US)\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- STATIONNAME: string (nullable = true)\n",
      " |-- GSNFLAG: string (nullable = true)\n",
      " |-- HCNFLAG: string (nullable = true)\n",
      " |-- WMOID: string (nullable = true)\n",
      " |-- COUNTRYCODE: string (nullable = true)\n",
      " |-- COUNTRYNAME: string (nullable = true)\n",
      " |-- ELEMENT: string (nullable = true)\n",
      " |-- FIRSTYEAR: integer (nullable = true)\n",
      " |-- LASTYEAR: integer (nullable = true)\n",
      " |-- STATENAME: string (nullable = true)\n",
      " |-- CLASSIFICATION: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/07 17:23:45 WARN TaskSetManager: Lost task 1.0 in stage 59.0 (TID 44) (10.244.20.34 executor 1): org.apache.spark.SparkFileNotFoundException: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/CLASSIFY-metadata.parquet/part-00001-0d75c243-6c11-497d-91a8-9c14a6ad596f-c000.snappy.parquet: No such file or directory.\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/07 17:23:45 WARN TaskSetManager: Lost task 0.0 in stage 59.0 (TID 43) (10.244.31.12 executor 2): org.apache.spark.SparkFileNotFoundException: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/CLASSIFY-metadata.parquet/part-00000-0d75c243-6c11-497d-91a8-9c14a6ad596f-c000.snappy.parquet: No such file or directory.\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/07 17:23:45 ERROR TaskSetManager: Task 0 in stage 59.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dprintf() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_8249/3258072300.py:38\u001b[0m, in \u001b[0;36mhprintf\u001b[0;34m(b, df, n, bSkema, bTable)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bTable:    \n\u001b[0;32m---> 38\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     dprintf(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o382.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 4 times, most recent failure: Lost task 0.3 in stage 59.0 (TID 50) (10.244.31.12 executor 2): org.apache.spark.SparkFileNotFoundException: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/CLASSIFY-metadata.parquet/part-00000-0d75c243-6c11-497d-91a8-9c14a6ad596f-c000.snappy.parquet: No such file or directory.\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59/CLASSIFY-metadata.parquet/part-00000-0d75c243-6c11-497d-91a8-9c14a6ad596f-c000.snappy.parquet: No such file or directory.\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m territorydf \u001b[38;5;241m=\u001b[39m theMetadata\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     15\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOUNTRYNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlike(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mUnited States\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m&\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOUNTRYNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnited States\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m dprintf(DEEBUG, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterritorydf created (US Territories excluding mainland US)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mhprintf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEEBUG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterritorydf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     23\u001b[0m nUSTerritories \u001b[38;5;241m=\u001b[39m territorydf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/tmp/ipykernel_8249/3258072300.py:44\u001b[0m, in \u001b[0;36mhprintf\u001b[0;34m(b, df, n, bSkema, bTable)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mdprintf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m❌ Error caught in hprintf:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n",
      "\u001b[0;31mTypeError\u001b[0m: dprintf() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# CELL 7 Q1(b.2): – Stations in US Territories (excluding United States)\n",
    "# CELL 7 Q1(b.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter theMetadata for stations with COUNTRYNAME including \"United States\"\n",
    "# - exclude stations where COUNTRYNAME is exactly \"United States\"\n",
    "# - count distinct station IDs in those US territories\n",
    "\n",
    "#\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "territorydf = theMetadata.filter(\n",
    "    (col(\"COUNTRYNAME\").like(\"%United States%\")) & (col(\"COUNTRYNAME\") != \"United States\")\n",
    ")\n",
    "\n",
    "dprintf(DEEBUG, \"territorydf created (US Territories excluding mainland US)\")\n",
    "hprintf(DEEBUG, territorydf)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "nUSTerritories = territorydf.select(\"ID\").distinct().count()\n",
    "dprintf(1, f\"Stations in US Territories (excluding United States): {nUSTerritories}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "516ad40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1(b.2): Stations in US Territories (excluding United States)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'substring' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m dprintf(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ1(b.2): Stations in US Territories (excluding United States)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# read and trim countries.txt\u001b[39;00m\n\u001b[1;32m     14\u001b[0m countriesdf \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ghcnd-countries.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCODE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43msubstring\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOUNTRY_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m, trim(substring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m61\u001b[39m))) \\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCODE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOUNTRY_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m dprintf(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountriesdf created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# join with enriched stations metadata\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'substring' is not defined"
     ]
    }
   ],
   "source": [
    "# CELL 7 Q1(b.2): – Stations in US Territories (excluding United States)\n",
    "# CELL 7 Q1(b.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - read countries.txt into countriesdf with fixed-width logic\n",
    "# - join with theMetadata to build stationsCountriesdf\n",
    "# - count distinct stations where COUNTRY_NAME includes \"United States\" but is not \"United States\"\n",
    "\n",
    "#  \n",
    "\n",
    "dprintf(1, \"Q1(b.2): Stations in US Territories (excluding United States)\")\n",
    "\n",
    "# read and trim countries.txt\n",
    "countriesdf = spark.read.text(f\"{prefix}/ghcnd-countries.txt\") \\\n",
    "    .withColumn(\"CODE\", substring(\"value\", 1, 2)) \\\n",
    "    .withColumn(\"COUNTRY_NAME\", trim(substring(\"value\", 4, 61))) \\\n",
    "    .select(\"CODE\", \"COUNTRY_NAME\")\n",
    "\n",
    "dprintf(1, \"countriesdf created\")\n",
    "\n",
    "# join with enriched stations metadata\n",
    "stationsCountriesdf = theMetadata.join(\n",
    "    countriesdf,\n",
    "    theMetadata.COUNTRY == countriesdf.CODE,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dprintf(2, \"stationsCountriesdf joined\")\n",
    "\n",
    "# filter US territories (contains 'United States' but not exactly equal)\n",
    "territoryStationsdf = stationsCountriesdf.filter(\n",
    "    (col(\"COUNTRY_NAME\").like(\"%United States%\")) & (col(\"COUNTRY_NAME\") != \"United States\")\n",
    ")\n",
    "\n",
    "nUSTerritories = territoryStationsdf.select(\"ID\").distinct().count()\n",
    "\n",
    "dprintf(1, f\"Stations in US Territories (excluding United States): {nUSTerritories}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: stationsCountriesdf to Parquet\n",
    "\n",
    "pqtStationsCountries = f\"{prefixWrite}/stations_countries.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtStationsCountries}\")\n",
    "\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    stationsCountriesdf.write.mode(\"overwrite\").parquet(pqtStationsCountries)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"stationsCountriesdf = {stationsCountriesdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtStationsCountries}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d706b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 Q1(c.1): – Station counts per country\n",
    "# CELL 8 Q1(c.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - group stationsCountriesdf by COUNTRY\n",
    "# - join with countriesdf to attach COUNTRY_NAME\n",
    "# - output result as countryStationCountsdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q1(c.1): Station counts per country\")\n",
    "\n",
    "# group by country code and count distinct station IDs\n",
    "countryCountsdf = stationsCountriesdf.groupBy(\"COUNTRY\").agg(countDistinct(\"ID\").alias(\"nStations\"))\n",
    "\n",
    "# join with countriesdf to get readable country names\n",
    "countryStationCountsdf = countryCountsdf.join(\n",
    "    countriesdf,\n",
    "    countryCountsdf.COUNTRY == countriesdf.CODE,\n",
    "    how=\"left\"\n",
    ").select(\"CODE\", \"COUNTRY_NAME\", \"nStations\").orderBy(\"nStations\", ascending=False)\n",
    "\n",
    "dprintf(1, f\"Total countries with stations: {countryStationCountsdf.count()}\")\n",
    "countryStationCountsdf.show(10, truncate=False)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: countryStationCountsdf to Parquet\n",
    "\n",
    "pqtCountryCounts = f\"{prefixWrite}/country_station_counts.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtCountryCounts}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    countryStationCountsdf.write.mode(\"overwrite\").parquet(pqtCountryCounts)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"countryStationCountsdf.count() = {countryStationCountsdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtCountryCounts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 Q1(c.2): – Station counts per US state\n",
    "# CELL 9 Q1(c.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - read ghcnd-states.txt into statesdf\n",
    "# - join with theMetadata on STATE\n",
    "# - group by state code and count distinct station IDs\n",
    "# - output result as stateStationCountsdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q1(c.2): Station counts per US state\")\n",
    "\n",
    "# read and trim states.txt\n",
    "statesdf = spark.read.text(f\"{prefix}/ghcnd-states.txt\") \\\n",
    "    .withColumn(\"CODE\", substring(\"value\", 1, 2)) \\\n",
    "    .withColumn(\"STATE_NAME\", trim(substring(\"value\", 4, 47))) \\\n",
    "    .select(\"CODE\", \"STATE_NAME\")\n",
    "\n",
    "dprintf(2, \"statesdf created\")\n",
    "\n",
    "# join with theMetadata\n",
    "stateCountsdf = theMetadata.join(\n",
    "    statesdf,\n",
    "    theMetadata.STATE == statesdf.CODE,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# group by state and count distinct station IDs\n",
    "stateStationCountsdf = stateCountsdf.groupBy(\"STATE\", \"STATE_NAME\") \\\n",
    "    .agg(countDistinct(\"ID\").alias(\"nStations\")) \\\n",
    "    .orderBy(\"nStations\", ascending=False)\n",
    "\n",
    "dprintf(1, f\"Total US states with stations: {stateStationCountsdf.count()}\")\n",
    "stateStationCountsdf.show(10, truncate=False)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: stateStationCountsdf to Parquet\n",
    "\n",
    "pqtStateCounts = f\"{prefixWrite}/state_station_counts.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtStateCounts}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    stateStationCountsdf.write.mode(\"overwrite\").parquet(pqtStateCounts)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"stateStationCountsdf.count() = {stateStationCountsdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtStateCounts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10 Q2(a.1): – Define geographic distance function\n",
    "# CELL 10 Q2(a.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - define haversine distance function to compute great-circle distance\n",
    "# - register as Spark UDF: geoDistanceUDF(lat1, lon1, lat2, lon2)\n",
    "# - result in kilometres\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q2(a.1): Define geographic distance function\")\n",
    "\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def geoDistance(lat1, lon1, lat2, lon2):\n",
    "    # convert to radians\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    # earth radius in km\n",
    "    R = 6371.0\n",
    "    return R * c\n",
    "\n",
    "# register UDF\n",
    "geoDistanceUDF = udf(geoDistance, DoubleType())\n",
    "\n",
    "dprintf(1, \"geoDistanceUDF registered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab48ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11 Q2(a.2): – Test geoDistanceUDF on sample stations\n",
    "# CELL 11 Q2(a.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - take 3 sample stations from theMetadata\n",
    "# - perform CROSS JOIN to compute pairwise distances\n",
    "# - apply geoDistanceUDF\n",
    "# - exclude self-pairs (ID1 == ID2)\n",
    "# - output result as stationPairsdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q2(a.2): Test geoDistanceUDF on CROSS JOIN\")\n",
    "\n",
    "# take 3 sample stations\n",
    "sampleStationsdf = theMetadata.select(\"ID\", \"LATITUDE\", \"LONGITUDE\").distinct().limit(3)\n",
    "\n",
    "# create aliases for self-join\n",
    "left = sampleStationsdf.alias(\"left\")\n",
    "right = sampleStationsdf.alias(\"right\")\n",
    "\n",
    "# perform CROSS JOIN\n",
    "stationPairsdf = left.crossJoin(right) \\\n",
    "    .filter(col(\"left.ID\") != col(\"right.ID\")) \\\n",
    "    .withColumn(\n",
    "        \"DistanceKM\",\n",
    "        geoDistanceUDF(\n",
    "            col(\"left.LATITUDE\"), col(\"left.LONGITUDE\"),\n",
    "            col(\"right.LATITUDE\"), col(\"right.LONGITUDE\")\n",
    "        )\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"left.ID\").alias(\"ID1\"),\n",
    "        col(\"right.ID\").alias(\"ID2\"),\n",
    "        col(\"DistanceKM\")\n",
    "    )\n",
    "\n",
    "dprintf(1, f\"Pairwise distances computed: {stationPairsdf.count()} rows\")\n",
    "stationPairsdf.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181df76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12 Q2(b.1): – Pairwise distances between NZ stations\n",
    "# CELL 12 Q2(b.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter stationsCountriesdf to COUNTRY == 'NZ'\n",
    "# - CROSS JOIN to compute distances between all station pairs\n",
    "# - apply geoDistanceUDF and exclude self-pairs\n",
    "# - output as nzStationDistancesdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q2(b.1): Pairwise distances between stations in New Zealand\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# filter NZ stations\n",
    "nzStationsdf = stationsCountriesdf.filter(\"COUNTRY == 'NZ'\") \\\n",
    "    .select(\"ID\", \"NAME\", \"LATITUDE\", \"LONGITUDE\").distinct()\n",
    "\n",
    "dprintf(2, f\"nzStationsdf count = {nzStationsdf.count()}\")\n",
    "\n",
    "# alias for cross join\n",
    "left = nzStationsdf.alias(\"left\")\n",
    "right = nzStationsdf.alias(\"right\")\n",
    "\n",
    "# compute pairwise distances\n",
    "nzStationDistancesdf = left.crossJoin(right) \\\n",
    "    .filter(col(\"left.ID\") != col(\"right.ID\")) \\\n",
    "    .withColumn(\n",
    "        \"DistanceKM\",\n",
    "        geoDistanceUDF(\n",
    "            col(\"left.LATITUDE\"), col(\"left.LONGITUDE\"),\n",
    "            col(\"right.LATITUDE\"), col(\"right.LONGITUDE\")\n",
    "        )\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"left.ID\").alias(\"ID1\"),\n",
    "        col(\"left.NAME\").alias(\"Name1\"),\n",
    "        col(\"right.ID\").alias(\"ID2\"),\n",
    "        col(\"right.NAME\").alias(\"Name2\"),\n",
    "        col(\"DistanceKM\")\n",
    "    )\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "dprintf(1, f\"NZ station pairs: {nzStationDistancesdf.count()}\")\n",
    "dprintf(1, f\"Q2(b.1) complete in {stop - start:.2f} seconds\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: nzStationDistancesdf to Parquet\n",
    "\n",
    "pqtNZDistances = f\"{prefixWrite}/nz_station_distances.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtNZDistances}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    nzStationDistancesdf.write.mode(\"overwrite\").parquet(pqtNZDistances)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"nzStationDistancesdf.count() = {nzStationDistancesdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtNZDistances}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cf41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13 Q2(b.2): – Closest pair of NZ stations\n",
    "# CELL 13 Q2(b.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - sort nzStationDistancesdf by DistanceKM\n",
    "# - retrieve top 1 closest pair\n",
    "# - output station names, IDs, and distance\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q2(b.2): Closest pair of stations in New Zealand\")\n",
    "\n",
    "closestPair = nzStationDistancesdf.orderBy(\"DistanceKM\").limit(1)\n",
    "\n",
    "row = closestPair.collect()[0]\n",
    "dprintf(1, f\"Closest pair: {row['Name1']} ({row['ID1']}) ↔ {row['Name2']} ({row['ID2']})\")\n",
    "dprintf(1, f\"Distance = {row['DistanceKM']:.2f} km\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aeb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14A: – Load full daily dataset from Azure\n",
    "# CELL 14A: \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - read daily data from prefixDaily path\n",
    "# - apply dailySchema as used in 01_explore\n",
    "# - output DataFrame: dailydf\n",
    "\n",
    "#  \n",
    "dailySchema = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"DATE\", DateType(), True),\n",
    "    StructField(\"ELEMENT\", StringType(), True),\n",
    "    StructField(\"VALUE\", FloatType(), True),\n",
    "    StructField(\"MFLAG\", StringType(), True),\n",
    "    StructField(\"QFLAG\", StringType(), True),\n",
    "    StructField(\"SFLAG\", StringType(), True),\n",
    "    StructField(\"OBSTIME\", StringType(), True)\n",
    "])\n",
    "\n",
    "hprintf(\"Q3(a.1): Load full daily dataset\")\n",
    "\n",
    "# path to daily .csv files in blob storage\n",
    "dailyPath = f\"{prefixDaily}/*\"\n",
    "\n",
    "dprintf(2, f\"Reading daily data from: {dailyPath}\")\n",
    "\n",
    "# read data with schema (headerless CSV)\n",
    "dailydf = spark.read.csv(dailyPath, schema=dailySchema, header=False)\n",
    "\n",
    "dprintf(1, f\"dailydf.count() = {dailydf.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 15 Q3(b.1): – Core element observation counts\n",
    "# CELL 15 Q3(b.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter dailydf for core elements: PRCP, SNOW, SNWD, TMAX, TMIN\n",
    "# - group by ELEMENT and count rows\n",
    "# - output as coreElementCountsdf\n",
    "\n",
    "#  \n",
    "\n",
    "hprintf(\"Q3(b.1): Count of observations for core elements\")\n",
    "\n",
    "coreElements = [\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "coreElementCountsdf = dailydf.filter(col(\"ELEMENT\").isin(coreElements)) \\\n",
    "    .groupBy(\"ELEMENT\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "dprintf(DEEBUG, \"Core element counts:\")\n",
    "hprintf(DEEBUG, coreElementCountsdf.show(5))\n",
    "dprintf(1, f\"coreElementCountsdf.count() = {coreElementCountsdf.count()}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Save: coreElementCountsdf to Parquet\n",
    "\n",
    "pqtCoreCounts = f\"{prefixWrite}/core_element_counts.parquet\"\n",
    "\n",
    "dprintf(DEEBUG, f\"write to : {pqtCoreCounts}\")\n",
    "\n",
    "if not 0:\n",
    "    start = time.time()\n",
    "    coreElementCountsdf.write.mode(\"overwrite\").parquet(pqtCoreCounts)\n",
    "    stop = time.time()\n",
    "    dprintf(1, f\"complete in {stop - start:.2f} seconds\")\n",
    "    dprintf(1, f\"coreElementCountsdf.count() = {coreElementCountsdf.count()}\")\n",
    "else:\n",
    "    dprintf(1, f\"DEEBUG = 1, no write to : {pqtCoreCounts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 16 Q3(b.2): – Core element with most observations\n",
    "# CELL 16 Q3(b.2): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - use coreElementCountsdf from CELL 15\n",
    "# - display preview and row count\n",
    "# - extract top row using .orderBy().limit().collect()\n",
    "\n",
    "#  \n",
    "\n",
    "topCoreElement = coreElementCountsdf.orderBy(col(\"count\").desc()).limit(1).collect()[0]\n",
    "\n",
    "dprintf(DEEBUG, f\"topCoreElement = {topCoreElement}\")\n",
    "hprintf(DEEBUG, coreElementCountsdf)\n",
    "dprintf(1, f\"Most observed element: {topCoreElement['ELEMENT']} ({topCoreElement['count']:,} observations)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 17 Q3(c.1): – TMAX without corresponding TMIN\n",
    "# CELL 17 Q3(c.1): \n",
    "# ------------------------------------------------\n",
    "# Summary:\n",
    "# - filter dailydf for TMAX and TMIN\n",
    "# - extract (ID, DATE) pairs for each\n",
    "# - anti-join to find TMAX records not matched to TMIN\n",
    "# - display and count results\n",
    "\n",
    "#  \n",
    "\n",
    "dprintf(DEEBUG, \"Q3(c.1): TMAX observations without corresponding TMIN\")\n",
    "\n",
    "tmaxdf = dailydf.filter(col(\"ELEMENT\") == \"TMAX\").select(\"ID\", \"DATE\")\n",
    "tmindf = dailydf.filter(col(\"ELEMENT\") == \"TMIN\").select(\"ID\", \"DATE\")\n",
    "\n",
    "# anti-join: TMAX records that don't match TMIN by ID and DATE\n",
    "tmaxNoTmindf = tmaxdf.join(tmindf, on=[\"ID\", \"DATE\"], how=\"left_anti\")\n",
    "\n",
    "dprintf(DEEBUG, f\"tmaxNoTmindf.count() = {tmaxNoTmindf.count()}\")\n",
    "hprintf(DEEBUG, tmaxNoTmindf)\n",
    "dprintf(1, f\"TMAX observations without matching TMIN = {tmaxNoTmindf.count()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
